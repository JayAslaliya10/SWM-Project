{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fd521baf",
      "metadata": {
        "id": "fd521baf"
      },
      "source": [
        "## Importing Libraries and Setting up Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d1ec987d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1ec987d",
        "outputId": "92f8b35b-06df-4d1a-8042-b7731a5ea32d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ENVIRONMENT CHECK\n",
            "============================================================\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Transformers version: 4.57.1\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 39.56 GB\n",
            "Device selected: cuda\n",
            "============================================================\n",
            "\n",
            "FILE PATHS:\n",
            "------------------------------------------------------------\n",
            "Training data:      train_text2sql.jsonl\n",
            "Validation data:    val_text2sql.jsonl\n",
            "Test data:          test_hospital_1.jsonl\n",
            "SQLite DB:          hospital_1.sqlite\n",
            "Output directory:   finetuned_flant5\n",
            "Results CSV:        results_hospital_1_finetuned_flant5.csv\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE VERIFICATION:\n",
            "------------------------------------------------------------\n",
            "Training data       : ✓ EXISTS\n",
            "Validation data     : ✓ EXISTS\n",
            "Test data           : ✓ EXISTS\n",
            "SQLite DB           : ✓ EXISTS\n",
            "------------------------------------------------------------\n",
            "\n",
            "✅ All required files found!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "import time\n",
        "import sqlite3\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from datasets import Dataset\n",
        "import sqlglot\n",
        "from sqlglot import parse_one\n",
        "\n",
        "# Check environment\n",
        "print(\"=\" * 60)\n",
        "print(\"ENVIRONMENT CHECK\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {__import__('transformers').__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "    print(\"WARNING: No GPU detected! Training will be very slow.\")\n",
        "\n",
        "print(f\"Device selected: {DEVICE}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# File Paths\n",
        "# ============================================================\n",
        "\n",
        "# Data paths\n",
        "TRAIN_JSONL = Path(\"train_text2sql.jsonl\")\n",
        "VAL_JSONL = Path(\"val_text2sql.jsonl\")\n",
        "TEST_JSONL = Path(\"test_hospital_1.jsonl\")\n",
        "\n",
        "# Database path\n",
        "SQLITE_DB = Path(\"hospital_1.sqlite\")\n",
        "\n",
        "# Output directories\n",
        "OUTPUT_DIR = Path(\"finetuned_flant5\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RESULTS_CSV = Path(\"results_hospital_1_finetuned_flant5.csv\")\n",
        "\n",
        "print(\"\\nFILE PATHS:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Training data:      {TRAIN_JSONL}\")\n",
        "print(f\"Validation data:    {VAL_JSONL}\")\n",
        "print(f\"Test data:          {TEST_JSONL}\")\n",
        "print(f\"SQLite DB:          {SQLITE_DB}\")\n",
        "print(f\"Output directory:   {OUTPUT_DIR}\")\n",
        "print(f\"Results CSV:        {RESULTS_CSV}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Verify files exist\n",
        "print(\"\\nFILE VERIFICATION:\")\n",
        "print(\"-\" * 60)\n",
        "files_to_check = {\n",
        "    \"Training data\": TRAIN_JSONL,\n",
        "    \"Validation data\": VAL_JSONL,\n",
        "    \"Test data\": TEST_JSONL,\n",
        "    \"SQLite DB\": SQLITE_DB,\n",
        "}\n",
        "\n",
        "all_exist = True\n",
        "for name, path in files_to_check.items():\n",
        "    exists = path.exists()\n",
        "    status = \"✓ EXISTS\" if exists else \"✗ MISSING\"\n",
        "    print(f\"{name:20s}: {status}\")\n",
        "    if not exists:\n",
        "        all_exist = False\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if all_exist:\n",
        "    print(\"\\n✅ All required files found!\")\n",
        "else:\n",
        "    print(\"\\n⚠️  WARNING: Some files are missing. Please upload them before proceeding.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d802503",
      "metadata": {
        "id": "9d802503"
      },
      "source": [
        "## Setting up Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "79ab018a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79ab018a",
        "outputId": "de99bfcc-6d0f-4862-9fa1-6048b5600cd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CONFIGURATION PARAMETERS\n",
            "============================================================\n",
            "\n",
            "Model Selection:\n",
            "------------------------------------------------------------\n",
            "Base model: juierror/flan-t5-text2sql-with-schema-v2\n",
            "\n",
            "Training Hyperparameters:\n",
            "------------------------------------------------------------\n",
            "Epochs:              4\n",
            "Batch size:          8\n",
            "Learning rate:       2e-05\n",
            "Warmup steps:        500\n",
            "Weight decay:        0.01\n",
            "Logging steps:       50\n",
            "Save steps:          500\n",
            "Eval steps:          500\n",
            "\n",
            "Generation Parameters:\n",
            "------------------------------------------------------------\n",
            "Max input length:    512\n",
            "Max target length:   256\n",
            "Generation max len:  256\n",
            "Num beams:           4\n",
            "Temperature:         0.0\n",
            "\n",
            "Prompt Template:\n",
            "------------------------------------------------------------\n",
            "Question: <question here>\n",
            "\n",
            "Schema:\n",
            "<schema here>\n",
            "\n",
            "SQL:\n",
            "\n",
            "Other Settings:\n",
            "------------------------------------------------------------\n",
            "Random seed:         42\n",
            "FP16 (mixed prec):   True\n",
            "\n",
            "✅ Random seed set for reproducibility\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"CONFIGURATION PARAMETERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Model Selection\n",
        "# ============================================================\n",
        "\n",
        "# We'll use the base Flan-T5 model and fine-tune it on our data\n",
        "BASE_MODEL_NAME = \"juierror/flan-t5-text2sql-with-schema-v2\"  # ~250M parameters\n",
        "\n",
        "# Alternative options (uncomment if you want):\n",
        "# BASE_MODEL_NAME = \"google/flan-t5-small\"  # ~80M parameters (faster, less accurate)\n",
        "# BASE_MODEL_NAME = \"google/flan-t5-large\"  # ~780M parameters (slower, more accurate)\n",
        "\n",
        "print(\"\\nModel Selection:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Base model: {BASE_MODEL_NAME}\")\n",
        "\n",
        "# ============================================================\n",
        "# Training Hyperparameters\n",
        "# ============================================================\n",
        "\n",
        "# NUM_EPOCHS = 2\n",
        "BATCH_SIZE = 8  # Adjust based on your GPU memory\n",
        "# LEARNING_RATE = 1e-5\n",
        "NUM_EPOCHS = 4\n",
        "LEARNING_RATE = 2e-5\n",
        "WARMUP_STEPS = 500\n",
        "WEIGHT_DECAY = 0.01\n",
        "LOGGING_STEPS = 50\n",
        "SAVE_STEPS = 500\n",
        "EVAL_STEPS = 500\n",
        "\n",
        "print(\"\\nTraining Hyperparameters:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Epochs:              {NUM_EPOCHS}\")\n",
        "print(f\"Batch size:          {BATCH_SIZE}\")\n",
        "print(f\"Learning rate:       {LEARNING_RATE}\")\n",
        "print(f\"Warmup steps:        {WARMUP_STEPS}\")\n",
        "print(f\"Weight decay:        {WEIGHT_DECAY}\")\n",
        "print(f\"Logging steps:       {LOGGING_STEPS}\")\n",
        "print(f\"Save steps:          {SAVE_STEPS}\")\n",
        "print(f\"Eval steps:          {EVAL_STEPS}\")\n",
        "\n",
        "# ============================================================\n",
        "# Generation Parameters (for inference)\n",
        "# ============================================================\n",
        "\n",
        "MAX_INPUT_LENGTH = 512   # Maximum tokens for input (question + schema)\n",
        "MAX_TARGET_LENGTH = 256  # Maximum tokens for output (SQL)\n",
        "\n",
        "GEN_MAX_LENGTH = 256\n",
        "GEN_NUM_BEAMS = 4\n",
        "GEN_TEMPERATURE = 0.0  # 0 = greedy decoding\n",
        "\n",
        "print(\"\\nGeneration Parameters:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Max input length:    {MAX_INPUT_LENGTH}\")\n",
        "print(f\"Max target length:   {MAX_TARGET_LENGTH}\")\n",
        "print(f\"Generation max len:  {GEN_MAX_LENGTH}\")\n",
        "print(f\"Num beams:           {GEN_NUM_BEAMS}\")\n",
        "print(f\"Temperature:         {GEN_TEMPERATURE}\")\n",
        "\n",
        "# ============================================================\n",
        "# Prompt Template\n",
        "# ============================================================\n",
        "\n",
        "# This is how we'll format the input to the model\n",
        "PROMPT_TEMPLATE = \"\"\"Question: {question}\n",
        "\n",
        "Schema:\n",
        "{schema}\n",
        "\n",
        "SQL:\"\"\"\n",
        "\n",
        "print(\"\\nPrompt Template:\")\n",
        "print(\"-\" * 60)\n",
        "print(PROMPT_TEMPLATE.format(\n",
        "    question=\"<question here>\",\n",
        "    schema=\"<schema here>\"\n",
        "))\n",
        "\n",
        "# ============================================================\n",
        "# Other Settings\n",
        "# ============================================================\n",
        "\n",
        "SEED = 42\n",
        "FP16 = True if DEVICE == \"cuda\" else False  # Use mixed precision on GPU\n",
        "\n",
        "print(\"\\nOther Settings:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Random seed:         {SEED}\")\n",
        "print(f\"FP16 (mixed prec):   {FP16}\")\n",
        "\n",
        "# Set random seed\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"\\n✅ Random seed set for reproducibility\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "456e3ce3",
      "metadata": {
        "id": "456e3ce3"
      },
      "source": [
        "## Loading Base Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "68cdaa8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68cdaa8c",
        "outputId": "d643257b-5826-4699-a73c-5f086cb88d34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LOADING BASE MODEL & TOKENIZER\n",
            "============================================================\n",
            "\n",
            "Loading tokenizer...\n",
            "------------------------------------------------------------\n",
            "✅ Tokenizer loaded: juierror/flan-t5-text2sql-with-schema-v2\n",
            "   Vocab size: 32101\n",
            "   Model max length: 512\n",
            "\n",
            "Tokenization test:\n",
            "   Input: SELECT * FROM Department WHERE DepartmentID = 1 ;\n",
            "   Token IDs shape: torch.Size([1, 16])\n",
            "   Token IDs: [3, 23143, 14196, 1429, 21680, 1775, 549, 17444, 427, 1775, 4309, 3274, 209, 3, 117, 1]...\n",
            "\n",
            "------------------------------------------------------------\n",
            "Loading model...\n",
            "------------------------------------------------------------\n",
            "✅ Model loaded: juierror/flan-t5-text2sql-with-schema-v2\n",
            "\n",
            "Model Statistics:\n",
            "   Total parameters:      247,536,384\n",
            "   Trainable parameters:  247,536,384\n",
            "   Model size (approx):   944.28 MB\n",
            "\n",
            "------------------------------------------------------------\n",
            "Testing generation (before fine-tuning)...\n",
            "------------------------------------------------------------\n",
            "Test prompt:\n",
            "Question: How many physicians are there?\n",
            "\n",
            "Schema:\n",
            "Database: hospital_1\n",
            "Tables:\n",
            "- Physician(EmployeeID*, Name, Position, SSN)\n",
            "\n",
            "SQL:\n",
            "\n",
            "Generated SQL (baseline, before fine-tuning):\n",
            "   SELECT count(*) FROM physician\n",
            "\n",
            "✅ Model is ready for fine-tuning!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"LOADING BASE MODEL & TOKENIZER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Load Tokenizer\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "\n",
        "print(f\"✅ Tokenizer loaded: {BASE_MODEL_NAME}\")\n",
        "print(f\"   Vocab size: {len(tokenizer)}\")\n",
        "print(f\"   Model max length: {tokenizer.model_max_length}\")\n",
        "\n",
        "# Test tokenization\n",
        "test_text = \"SELECT * FROM Department WHERE DepartmentID = 1 ;\"\n",
        "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"\\nTokenization test:\")\n",
        "print(f\"   Input: {test_text}\")\n",
        "print(f\"   Token IDs shape: {test_tokens['input_ids'].shape}\")\n",
        "print(f\"   Token IDs: {test_tokens['input_ids'][0][:20].tolist()}...\")\n",
        "\n",
        "# ============================================================\n",
        "# Load Model\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Loading model...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME)\n",
        "\n",
        "# Move to device\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "print(f\"✅ Model loaded: {BASE_MODEL_NAME}\")\n",
        "\n",
        "# ============================================================\n",
        "# Model Statistics\n",
        "# ============================================================\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"   Total parameters:      {total_params:,}\")\n",
        "print(f\"   Trainable parameters:  {trainable_params:,}\")\n",
        "print(f\"   Model size (approx):   {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# ============================================================\n",
        "# Test Generation (before fine-tuning)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Testing generation (before fine-tuning)...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "test_prompt = PROMPT_TEMPLATE.format(\n",
        "    question=\"How many physicians are there?\",\n",
        "    schema=\"\"\"Database: hospital_1\n",
        "Tables:\n",
        "- Physician(EmployeeID*, Name, Position, SSN)\"\"\"\n",
        ")\n",
        "\n",
        "print(f\"Test prompt:\\n{test_prompt}\\n\")\n",
        "\n",
        "# Tokenize and generate\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=GEN_MAX_LENGTH,\n",
        "        num_beams=GEN_NUM_BEAMS,\n",
        "        temperature=GEN_TEMPERATURE if GEN_TEMPERATURE > 0 else 1.0,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Generated SQL (baseline, before fine-tuning):\")\n",
        "print(f\"   {generated_sql}\")\n",
        "\n",
        "print(\"\\n✅ Model is ready for fine-tuning!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab812057",
      "metadata": {
        "id": "ab812057"
      },
      "source": [
        "## Data Loading and Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6e8c8537",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831,
          "referenced_widgets": [
            "ee282ad93f39416795ba549f82447629",
            "f5c77c29849b47c1841abb5bb9fec068",
            "53b23a74a7ac47b898ae93252d43815d",
            "b9cca6eebfe34b8eb997ac54156b803f",
            "62c27373906d46a9a8c194f92503e249",
            "670407ed473843449aa9412a0eda735b",
            "d6d525ca5690426d8b9967b7f2c9f91e",
            "97037c5e0e8c4faea5321b78b21d3eea",
            "fb968e830f9b4a3f84264ab295f95165",
            "a2f3c9a7999840a88f5ce2a4a8a6d908",
            "2534a96dcae140dd83b97df389a761fe",
            "557bef4c130740f1a2cc3f4c6a380735",
            "cd89270fa6ee4b679560ea69ea8c95d3",
            "aeec6f163e6d4a139ff437a7d95fb167",
            "a7829f872d21477aad2d8537c9a0fcf9",
            "8edf1d737bf64e138de788f809061e93",
            "82261ff138dd46b4908533f9313cbffa",
            "ad97b0de4c304edaa5167a47c1c31611",
            "5e68d3b6df28400ca58300460e8bba3d",
            "695c5514abf94214abb6b9b25f0155b4",
            "79d68ea720ba4e42b9f5b01713f202c1",
            "bfa456ba358147448af76541857957e8"
          ]
        },
        "id": "6e8c8537",
        "outputId": "4f2642a7-c2ca-4a6e-a517-1622482795cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DATA LOADING AND PRE-PROCESSING\n",
            "============================================================\n",
            "\n",
            "Loading datasets...\n",
            "------------------------------------------------------------\n",
            "Training examples:   8559\n",
            "Validation examples: 1034\n",
            "Test examples:       100\n",
            "\n",
            "Sample training example:\n",
            "   Question: How many heads of the departments are older than 56 ?\n",
            "   Gold SQL: SELECT count(*) FROM head WHERE age  >  56;\n",
            "   Schema (first 100 chars): Database: department_management\n",
            "Tables:\n",
            "- department(Department_ID*, Name, Creation, Ranking, Budget...\n",
            "\n",
            "------------------------------------------------------------\n",
            "Converting to HuggingFace Dataset format...\n",
            "------------------------------------------------------------\n",
            "✅ Datasets converted\n",
            "   Train dataset: 8559 examples\n",
            "   Val dataset:   1034 examples\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tokenizing datasets (this may take a few minutes)...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee282ad93f39416795ba549f82447629",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing training data:   0%|          | 0/8559 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "557bef4c130740f1a2cc3f4c6a380735",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing validation data:   0%|          | 0/1034 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tokenization complete\n",
            "   Tokenized train: 8559 examples\n",
            "   Tokenized val:   1034 examples\n",
            "\n",
            "Tokenized example (first training sample):\n",
            "   Input IDs length:  134\n",
            "   Label IDs length:  17\n",
            "   Input IDs (first 20): [11860, 10, 571, 186, 7701, 13, 8, 10521, 33, 2749, 145, 11526, 3, 58, 10248, 51, 9, 10, 20230, 10]\n",
            "   Label IDs (first 20): [3, 23143, 14196, 3476, 599, 1935, 61, 21680, 819, 549, 17444, 427, 1246, 2490, 11526, 117, 1]\n",
            "\n",
            "------------------------------------------------------------\n",
            "Setting up data collator...\n",
            "------------------------------------------------------------\n",
            "✅ Data collator ready\n",
            "   Will pad batches dynamically during training\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"DATA LOADING AND PRE-PROCESSING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Load JSONL Files\n",
        "# ============================================================\n",
        "\n",
        "def load_jsonl(path):\n",
        "    \"\"\"Load JSONL file and return list of dictionaries.\"\"\"\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "print(\"\\nLoading datasets...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "train_data = load_jsonl(TRAIN_JSONL)\n",
        "val_data = load_jsonl(VAL_JSONL)\n",
        "test_data = load_jsonl(TEST_JSONL)\n",
        "\n",
        "print(f\"Training examples:   {len(train_data)}\")\n",
        "print(f\"Validation examples: {len(val_data)}\")\n",
        "print(f\"Test examples:       {len(test_data)}\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample training example:\")\n",
        "print(f\"   Question: {train_data[0]['question']}\")\n",
        "print(f\"   Gold SQL: {train_data[0]['gold_query']}\")\n",
        "print(f\"   Schema (first 100 chars): {train_data[0]['schema_serialized'][:100]}...\")\n",
        "\n",
        "# ============================================================\n",
        "# Preprocessing Function\n",
        "# ============================================================\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Preprocess examples for fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        examples: Dictionary with lists of questions, schemas, and SQL queries\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with tokenized inputs and labels\n",
        "    \"\"\"\n",
        "    # Build input prompts\n",
        "    inputs = []\n",
        "    for question, schema in zip(examples['question'], examples['schema_serialized']):\n",
        "        prompt = PROMPT_TEMPLATE.format(question=question, schema=schema)\n",
        "        inputs.append(prompt)\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=False  # We'll pad dynamically in the data collator\n",
        "    )\n",
        "\n",
        "    # Tokenize targets (SQL queries)\n",
        "    labels = tokenizer(\n",
        "        text_target=examples['gold_query'],\n",
        "        max_length=MAX_TARGET_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# ============================================================\n",
        "# Convert to HuggingFace Dataset Format\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Converting to HuggingFace Dataset format...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Convert lists of dicts to dict of lists (HuggingFace format)\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'question': [ex['question'] for ex in train_data],\n",
        "    'schema_serialized': [ex['schema_serialized'] for ex in train_data],\n",
        "    'gold_query': [ex['gold_query'] for ex in train_data]\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    'question': [ex['question'] for ex in val_data],\n",
        "    'schema_serialized': [ex['schema_serialized'] for ex in val_data],\n",
        "    'gold_query': [ex['gold_query'] for ex in val_data]\n",
        "})\n",
        "\n",
        "print(f\"✅ Datasets converted\")\n",
        "print(f\"   Train dataset: {len(train_dataset)} examples\")\n",
        "print(f\"   Val dataset:   {len(val_dataset)} examples\")\n",
        "\n",
        "# ============================================================\n",
        "# Tokenize Datasets\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Tokenizing datasets (this may take a few minutes)...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# # IN COLAB: Use multiprocessing for faster tokenization\n",
        "# tokenized_train = train_dataset.map(\n",
        "#     preprocess_function,\n",
        "#     batched=True,\n",
        "#     num_proc=4,  # Use 4 processes in Colab\n",
        "#     remove_columns=train_dataset.column_names,\n",
        "#     desc=\"Tokenizing training data\"\n",
        "# )\n",
        "\n",
        "# tokenized_val = val_dataset.map(\n",
        "#     preprocess_function,\n",
        "#     batched=True,\n",
        "#     num_proc=4,  # Use 4 processes in Colab\n",
        "#     remove_columns=val_dataset.column_names,\n",
        "#     desc=\"Tokenizing validation data\"\n",
        "# )\n",
        "\n",
        "# ON LAPTOP: Use single process (no multiprocessing on Mac sometimes causes issues)\n",
        "tokenized_train = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenizing training data\"\n",
        ")\n",
        "\n",
        "tokenized_val = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Tokenizing validation data\"\n",
        ")\n",
        "\n",
        "print(f\"✅ Tokenization complete\")\n",
        "print(f\"   Tokenized train: {len(tokenized_train)} examples\")\n",
        "print(f\"   Tokenized val:   {len(tokenized_val)} examples\")\n",
        "\n",
        "# Show tokenized example\n",
        "print(f\"\\nTokenized example (first training sample):\")\n",
        "print(f\"   Input IDs length:  {len(tokenized_train[0]['input_ids'])}\")\n",
        "print(f\"   Label IDs length:  {len(tokenized_train[0]['labels'])}\")\n",
        "print(f\"   Input IDs (first 20): {tokenized_train[0]['input_ids'][:20]}\")\n",
        "print(f\"   Label IDs (first 20): {tokenized_train[0]['labels'][:20]}\")\n",
        "\n",
        "# ============================================================\n",
        "# Data Collator\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Setting up data collator...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Data collator handles dynamic padding\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    max_length=MAX_INPUT_LENGTH\n",
        ")\n",
        "\n",
        "print(f\"✅ Data collator ready\")\n",
        "print(f\"   Will pad batches dynamically during training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "198d6be2",
      "metadata": {
        "id": "198d6be2"
      },
      "source": [
        "## Fine Tuning the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "75f1bcd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "75f1bcd9",
        "outputId": "e2747202-797c-4b24-9227-f2fdfe76eabc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FINE-TUNING SETUP\n",
            "============================================================\n",
            "\n",
            "Setting up training arguments...\n",
            "------------------------------------------------------------\n",
            "✅ Training arguments configured\n",
            "   Output directory: finetuned_flant5\n",
            "   Total epochs: 4\n",
            "   Batch size: 8\n",
            "   Learning rate: 2e-05\n",
            "\n",
            "------------------------------------------------------------\n",
            "Initializing Seq2SeqTrainer...\n",
            "------------------------------------------------------------\n",
            "✅ Trainer initialized\n",
            "   Training samples: 8559\n",
            "   Validation samples: 1034\n",
            "   Total training steps: 0\n",
            "\n",
            "============================================================\n",
            "STARTING FINE-TUNING\n",
            "============================================================\n",
            "\n",
            "This will take approximately:\n",
            "   - On GPU (Colab): ~15-30 minutes\n",
            "   - On CPU/MPS: ~3-5 hours\n",
            "\n",
            "Training progress will be displayed below...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3406138043.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4280' max='4280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4280/4280 23:25, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.215800</td>\n",
              "      <td>0.387498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.167600</td>\n",
              "      <td>0.399717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.143100</td>\n",
              "      <td>0.393052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.114800</td>\n",
              "      <td>0.405055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.126800</td>\n",
              "      <td>0.403906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.113800</td>\n",
              "      <td>0.413282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.113400</td>\n",
              "      <td>0.410019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.109900</td>\n",
              "      <td>0.410447</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Training Metrics:\n",
            "   Total runtime: 1405.97 seconds\n",
            "   Samples per second: 24.35\n",
            "   Final train loss: 0.1669\n",
            "\n",
            "------------------------------------------------------------\n",
            "Evaluating on validation set...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [130/130 00:07]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Evaluation complete\n",
            "\n",
            "Validation Metrics:\n",
            "   Eval loss: 0.3875\n",
            "   Eval runtime: 7.18 seconds\n",
            "   Samples per second: 143.92\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"FINE-TUNING SETUP\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Training Arguments\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nSetting up training arguments...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "\n",
        "    # Training hyperparameters\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "\n",
        "    # Evaluation and logging\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "\n",
        "    # Generation settings for evaluation\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=GEN_MAX_LENGTH,\n",
        "    generation_num_beams=GEN_NUM_BEAMS,\n",
        "\n",
        "    # Performance optimizations\n",
        "    # # IN COLAB: Use these settings for GPU\n",
        "    # fp16=FP16,\n",
        "    # dataloader_num_workers=2,\n",
        "\n",
        "    # # ON LAPTOP: Use these settings instead\n",
        "    fp16=False,\n",
        "    dataloader_num_workers=0,\n",
        "\n",
        "    # Model saving\n",
        "    save_total_limit=2,  # Only keep 2 best checkpoints\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    # Other settings\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard\n",
        "    seed=SEED,\n",
        ")\n",
        "\n",
        "print(f\"✅ Training arguments configured\")\n",
        "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"   Total epochs: {NUM_EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "\n",
        "# ============================================================\n",
        "# Initialize Trainer\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Initializing Seq2SeqTrainer...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(f\"✅ Trainer initialized\")\n",
        "print(f\"   Training samples: {len(tokenized_train)}\")\n",
        "print(f\"   Validation samples: {len(tokenized_val)}\")\n",
        "print(f\"   Total training steps: {trainer.state.max_steps if hasattr(trainer.state, 'max_steps') else 'calculating...'}\")\n",
        "\n",
        "# ============================================================\n",
        "# Start Fine-Tuning\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING FINE-TUNING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nThis will take approximately:\")\n",
        "# IN COLAB: ~15-30 minutes on GPU\n",
        "print(\"   - On GPU (Colab): ~15-30 minutes\")\n",
        "# ON LAPTOP: ~3-5 hours on CPU/MPS\n",
        "print(\"   - On CPU/MPS: ~3-5 hours\")\n",
        "print(\"\\nTraining progress will be displayed below...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Start training\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"\\nTraining Metrics:\")\n",
        "print(f\"   Total runtime: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"   Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
        "print(f\"   Final train loss: {train_result.metrics['train_loss']:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# Evaluate on Validation Set\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Evaluating on validation set...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "eval_result = trainer.evaluate()\n",
        "\n",
        "print(f\"✅ Evaluation complete\")\n",
        "print(f\"\\nValidation Metrics:\")\n",
        "print(f\"   Eval loss: {eval_result['eval_loss']:.4f}\")\n",
        "print(f\"   Eval runtime: {eval_result['eval_runtime']:.2f} seconds\")\n",
        "print(f\"   Samples per second: {eval_result['eval_samples_per_second']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ec8b777",
      "metadata": {
        "id": "9ec8b777"
      },
      "source": [
        "## Saving the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4c43d5e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c43d5e0",
        "outputId": "0690abc8-cd97-4957-e6df-7522fad19a54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SAVING FINE-TUNED MODEL\n",
            "============================================================\n",
            "\n",
            "Saving model and tokenizer...\n",
            "------------------------------------------------------------\n",
            "✅ Model saved to: finetuned_flant5/final_model\n",
            "✅ Tokenizer saved to: finetuned_flant5/final_model\n",
            "\n",
            "------------------------------------------------------------\n",
            "Saving training configuration...\n",
            "------------------------------------------------------------\n",
            "✅ Training config saved to: finetuned_flant5/training_config.json\n",
            "\n",
            "------------------------------------------------------------\n",
            "Saved Files Summary:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Final model directory: finetuned_flant5/final_model\n",
            "  Contains:\n",
            "  - pytorch_model.bin (model weights)\n",
            "  - config.json (model configuration)\n",
            "  - tokenizer files\n",
            "\n",
            "Checkpoints directory: finetuned_flant5\n",
            "  Contains intermediate checkpoints from training\n",
            "\n",
            "Training config: finetuned_flant5/training_config.json\n",
            "\n",
            "✅ All files saved successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"SAVING FINE-TUNED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Save Model and Tokenizer\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nSaving model and tokenizer...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# The trainer already saved checkpoints during training\n",
        "# Now we'll save the final best model explicitly\n",
        "\n",
        "FINAL_MODEL_DIR = OUTPUT_DIR / \"final_model\"\n",
        "FINAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "trainer.save_model(str(FINAL_MODEL_DIR))\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(str(FINAL_MODEL_DIR))\n",
        "\n",
        "print(f\"✅ Model saved to: {FINAL_MODEL_DIR}\")\n",
        "print(f\"✅ Tokenizer saved to: {FINAL_MODEL_DIR}\")\n",
        "\n",
        "# ============================================================\n",
        "# Save Training Configuration\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Saving training configuration...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "training_config = {\n",
        "    \"base_model\": BASE_MODEL_NAME,\n",
        "    \"num_epochs\": NUM_EPOCHS,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"warmup_steps\": WARMUP_STEPS,\n",
        "    \"weight_decay\": WEIGHT_DECAY,\n",
        "    \"max_input_length\": MAX_INPUT_LENGTH,\n",
        "    \"max_target_length\": MAX_TARGET_LENGTH,\n",
        "    \"gen_max_length\": GEN_MAX_LENGTH,\n",
        "    \"gen_num_beams\": GEN_NUM_BEAMS,\n",
        "    \"seed\": SEED,\n",
        "    \"final_train_loss\": train_result.metrics.get('train_loss', 'N/A'),\n",
        "    \"final_eval_loss\": eval_result.get('eval_loss', 'N/A'),\n",
        "    \"training_runtime_seconds\": train_result.metrics.get('train_runtime', 'N/A'),\n",
        "}\n",
        "\n",
        "config_path = OUTPUT_DIR / \"training_config.json\"\n",
        "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(training_config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"✅ Training config saved to: {config_path}\")\n",
        "\n",
        "# ============================================================\n",
        "# Model Files Summary\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Saved Files Summary:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "print(f\"\\nFinal model directory: {FINAL_MODEL_DIR}\")\n",
        "print(f\"  Contains:\")\n",
        "print(f\"  - pytorch_model.bin (model weights)\")\n",
        "print(f\"  - config.json (model configuration)\")\n",
        "print(f\"  - tokenizer files\")\n",
        "\n",
        "print(f\"\\nCheckpoints directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Contains intermediate checkpoints from training\")\n",
        "\n",
        "print(f\"\\nTraining config: {config_path}\")\n",
        "\n",
        "print(\"\\n✅ All files saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2707e1a",
      "metadata": {
        "id": "e2707e1a"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e5093fcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5093fcd",
        "outputId": "f2e5c058-85b1-4e9e-c095-30341a5d432b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EVALUATION ON HOSPITAL_1\n",
            "============================================================\n",
            "\n",
            "Loading fine-tuned model for evaluation...\n",
            "------------------------------------------------------------\n",
            "✅ Model ready for evaluation\n",
            "\n",
            "✅ SQL utilities defined\n",
            "\n",
            "------------------------------------------------------------\n",
            "Connecting to database...\n",
            "------------------------------------------------------------\n",
            "✅ Connected to: hospital_1.sqlite\n",
            "\n",
            "============================================================\n",
            "RUNNING EVALUATION\n",
            "============================================================\n",
            "\n",
            "Evaluating on 100 examples from hospital_1...\n",
            "------------------------------------------------------------\n",
            "[10/100] EM=0.200 EX=0.600 Valid=0.700\n",
            "[20/100] EM=0.250 EX=0.700 Valid=0.750\n",
            "[30/100] EM=0.367 EX=0.667 Valid=0.767\n",
            "[40/100] EM=0.450 EX=0.725 Valid=0.825\n",
            "[50/100] EM=0.440 EX=0.740 Valid=0.860\n",
            "[60/100] EM=0.383 EX=0.683 Valid=0.850\n",
            "[70/100] EM=0.414 EX=0.671 Valid=0.843\n",
            "[80/100] EM=0.438 EX=0.662 Valid=0.838\n",
            "[90/100] EM=0.389 EX=0.611 Valid=0.767\n",
            "[100/100] EM=0.370 EX=0.610 Valid=0.770\n",
            "\n",
            "------------------------------------------------------------\n",
            "Saving results...\n",
            "------------------------------------------------------------\n",
            "✅ Results saved to: results_hospital_1_finetuned_flant5.csv\n",
            "\n",
            "============================================================\n",
            "EVALUATION SUMMARY\n",
            "============================================================\n",
            "\n",
            "Model: Fine-tuned Flan-T5\n",
            "Base Model: juierror/flan-t5-text2sql-with-schema-v2\n",
            "Test Dataset: hospital_1\n",
            "Examples: 100\n",
            "\n",
            "Metrics:\n",
            "  Exact Match (EM):        37.000% (37/100)\n",
            "  Execution Accuracy (EX): 61.000% (61/100)\n",
            "  Valid-SQL rate:          77.000% (77/100)\n",
            "\n",
            "Performance:\n",
            "  Median generation time:  1331.0 ms\n",
            "\n",
            "Baseline Comparison (Flan-T5 before fine-tuning):\n",
            "  Baseline EM:  15.0%\n",
            "  Baseline EX:  28.0%\n",
            "  Baseline Valid: 38.0%\n",
            "\n",
            "Improvement:\n",
            "  EM improvement:    +22.0 percentage points\n",
            "  EX improvement:    +33.0 percentage points\n",
            "  Valid improvement: +39.0 percentage points\n",
            "\n",
            "Results saved to: results_hospital_1_finetuned_flant5.csv\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"EVALUATION ON HOSPITAL_1\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Load Fine-tuned Model (if needed)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nLoading fine-tuned model for evaluation...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# If you're continuing from training, the model is already loaded\n",
        "# If you're starting fresh, uncomment these lines:\n",
        "# eval_model = AutoModelForSeq2SeqLM.from_pretrained(str(FINAL_MODEL_DIR))\n",
        "# eval_tokenizer = AutoTokenizer.from_pretrained(str(FINAL_MODEL_DIR))\n",
        "# eval_model = eval_model.to(DEVICE)\n",
        "\n",
        "# For now, we'll use the trainer's model\n",
        "eval_model = trainer.model\n",
        "eval_tokenizer = tokenizer\n",
        "\n",
        "print(f\"✅ Model ready for evaluation\")\n",
        "\n",
        "# ============================================================\n",
        "# SQL Utilities\n",
        "# ============================================================\n",
        "\n",
        "def canonical_sql(sql_text):\n",
        "    \"\"\"Normalize SQL to canonical form using sqlglot.\"\"\"\n",
        "    if not sql_text:\n",
        "        return None\n",
        "    try:\n",
        "        ast = parse_one(sql_text, read=\"sqlite\")\n",
        "        return ast.sql(dialect=\"sqlite\", pretty=False)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def try_execute(conn, sql_text):\n",
        "    \"\"\"Execute SQL query and return result set.\"\"\"\n",
        "    try:\n",
        "        cur = conn.execute(sql_text)\n",
        "        rows = cur.fetchall()\n",
        "\n",
        "        # Normalize floats\n",
        "        normalized = []\n",
        "        for row in rows:\n",
        "            norm_row = []\n",
        "            for val in row:\n",
        "                if isinstance(val, float):\n",
        "                    norm_row.append(round(val, 6))\n",
        "                else:\n",
        "                    norm_row.append(val)\n",
        "            normalized.append(tuple(norm_row))\n",
        "\n",
        "        return set(normalized), None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "\n",
        "def extract_sql(text):\n",
        "    \"\"\"Extract SQL from model output.\"\"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove markdown code blocks if present\n",
        "    if \"```\" in text:\n",
        "        parts = text.split(\"```\")\n",
        "        for part in parts:\n",
        "            if \"select\" in part.lower() or \"SELECT\" in part:\n",
        "                text = part.strip()\n",
        "                if text.lower().startswith(\"sql\"):\n",
        "                    text = text[3:].strip()\n",
        "                break\n",
        "\n",
        "    # Remove common prefixes\n",
        "    for prefix in [\"sql:\", \"answer:\", \"query:\"]:\n",
        "        if text.lower().startswith(prefix):\n",
        "            text = text[len(prefix):].strip()\n",
        "\n",
        "    # Ensure semicolon\n",
        "    if \";\" in text:\n",
        "        text = text.split(\";\", 1)[0] + \";\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "print(\"\\n✅ SQL utilities defined\")\n",
        "\n",
        "# ============================================================\n",
        "# Connect to Database\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Connecting to database...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "conn = sqlite3.connect(str(SQLITE_DB))\n",
        "conn.execute(\"PRAGMA foreign_keys=ON\")\n",
        "\n",
        "print(f\"✅ Connected to: {SQLITE_DB}\")\n",
        "\n",
        "# ============================================================\n",
        "# Evaluation Loop\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RUNNING EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = []\n",
        "n_examples = len(test_data)\n",
        "\n",
        "em_count = 0\n",
        "ex_count = 0\n",
        "valid_count = 0\n",
        "latencies = []\n",
        "\n",
        "print(f\"\\nEvaluating on {n_examples} examples from hospital_1...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "eval_model.eval()\n",
        "\n",
        "for i, example in enumerate(test_data, 1):\n",
        "    question = example['question']\n",
        "    gold_sql = example['gold_query']\n",
        "    schema = example['schema_serialized']\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = PROMPT_TEMPLATE.format(question=question, schema=schema)\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = eval_tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Generate SQL\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = eval_model.generate(\n",
        "            **inputs,\n",
        "            max_length=GEN_MAX_LENGTH,\n",
        "            num_beams=GEN_NUM_BEAMS,\n",
        "            temperature=GEN_TEMPERATURE if GEN_TEMPERATURE > 0 else 1.0,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    gen_time_ms = (time.time() - start_time) * 1000.0\n",
        "    latencies.append(gen_time_ms)\n",
        "\n",
        "    # Decode\n",
        "    pred_sql_raw = eval_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    pred_sql_raw = extract_sql(pred_sql_raw)\n",
        "\n",
        "    # Normalize\n",
        "    pred_sql_norm = canonical_sql(pred_sql_raw)\n",
        "    gold_sql_norm = canonical_sql(gold_sql)\n",
        "\n",
        "    # ============================================================\n",
        "    # Compute Metrics\n",
        "    # ============================================================\n",
        "\n",
        "    # Exact Match (EM)\n",
        "    em = int(\n",
        "        pred_sql_norm is not None and\n",
        "        gold_sql_norm is not None and\n",
        "        pred_sql_norm == gold_sql_norm\n",
        "    )\n",
        "\n",
        "    # Execution Accuracy (EX) and Valid SQL\n",
        "    valid = 0\n",
        "    ex_ok = 0\n",
        "    error = None\n",
        "\n",
        "    if pred_sql_norm is not None:\n",
        "        # Try to execute predicted SQL\n",
        "        pred_rows, error = try_execute(conn, pred_sql_norm)\n",
        "\n",
        "        if pred_rows is not None:\n",
        "            valid = 1  # SQL is valid\n",
        "\n",
        "            # Execute gold SQL\n",
        "            gold_rows, gold_error = try_execute(conn, gold_sql_norm or gold_sql)\n",
        "\n",
        "            if gold_rows is not None:\n",
        "                # Compare result sets\n",
        "                ex_ok = int(pred_rows == gold_rows)\n",
        "            else:\n",
        "                error = f\"Gold SQL failed: {gold_error}\"\n",
        "    else:\n",
        "        error = \"ParseError: Could not parse predicted SQL\"\n",
        "\n",
        "    # Update counters\n",
        "    em_count += em\n",
        "    ex_count += ex_ok\n",
        "    valid_count += valid\n",
        "\n",
        "    # Store result\n",
        "    results.append({\n",
        "        \"id\": example.get(\"id\", f\"test_{i}\"),\n",
        "        \"question\": question,\n",
        "        \"gold_sql\": gold_sql,\n",
        "        \"pred_sql_raw\": pred_sql_raw,\n",
        "        \"pred_sql_norm\": pred_sql_norm or \"\",\n",
        "        \"em\": em,\n",
        "        \"ex\": ex_ok,\n",
        "        \"valid_sql\": valid,\n",
        "        \"latency_ms\": round(gen_time_ms, 2),\n",
        "        \"error\": error or \"\"\n",
        "    })\n",
        "\n",
        "    # Progress update\n",
        "    if i % 10 == 0 or i == n_examples:\n",
        "        print(f\"[{i}/{n_examples}] EM={em_count/i:.3f} EX={ex_count/i:.3f} Valid={valid_count/i:.3f}\")\n",
        "\n",
        "# ============================================================\n",
        "# Save Results\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Saving results...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "with open(RESULTS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    if results:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(results[0].keys()))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "print(f\"✅ Results saved to: {RESULTS_CSV}\")\n",
        "\n",
        "# ============================================================\n",
        "# Summary Statistics\n",
        "# ============================================================\n",
        "\n",
        "em_rate = em_count / n_examples\n",
        "ex_rate = ex_count / n_examples\n",
        "valid_rate = valid_count / n_examples\n",
        "median_latency = sorted(latencies)[len(latencies) // 2] if latencies else 0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nModel: Fine-tuned Flan-T5\")\n",
        "print(f\"Base Model: {BASE_MODEL_NAME}\")\n",
        "print(f\"Test Dataset: hospital_1\")\n",
        "print(f\"Examples: {n_examples}\")\n",
        "print(f\"\\nMetrics:\")\n",
        "print(f\"  Exact Match (EM):        {em_rate:.3%} ({em_count}/{n_examples})\")\n",
        "print(f\"  Execution Accuracy (EX): {ex_rate:.3%} ({ex_count}/{n_examples})\")\n",
        "print(f\"  Valid-SQL rate:          {valid_rate:.3%} ({valid_count}/{n_examples})\")\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Median generation time:  {median_latency:.1f} ms\")\n",
        "print(f\"\\nBaseline Comparison (Flan-T5 before fine-tuning):\")\n",
        "print(f\"  Baseline EM:  15.0%\")\n",
        "print(f\"  Baseline EX:  28.0%\")\n",
        "print(f\"  Baseline Valid: 38.0%\")\n",
        "print(f\"\\nImprovement:\")\n",
        "print(f\"  EM improvement:    {(em_rate - 0.15) * 100:+.1f} percentage points\")\n",
        "print(f\"  EX improvement:    {(ex_rate - 0.28) * 100:+.1f} percentage points\")\n",
        "print(f\"  Valid improvement: {(valid_rate - 0.38) * 100:+.1f} percentage points\")\n",
        "print(f\"\\nResults saved to: {RESULTS_CSV}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Close database connection\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "641f454e",
      "metadata": {
        "id": "641f454e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2534a96dcae140dd83b97df389a761fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53b23a74a7ac47b898ae93252d43815d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97037c5e0e8c4faea5321b78b21d3eea",
            "max": 8559,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb968e830f9b4a3f84264ab295f95165",
            "value": 8559
          }
        },
        "557bef4c130740f1a2cc3f4c6a380735": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd89270fa6ee4b679560ea69ea8c95d3",
              "IPY_MODEL_aeec6f163e6d4a139ff437a7d95fb167",
              "IPY_MODEL_a7829f872d21477aad2d8537c9a0fcf9"
            ],
            "layout": "IPY_MODEL_8edf1d737bf64e138de788f809061e93"
          }
        },
        "5e68d3b6df28400ca58300460e8bba3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62c27373906d46a9a8c194f92503e249": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "670407ed473843449aa9412a0eda735b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "695c5514abf94214abb6b9b25f0155b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79d68ea720ba4e42b9f5b01713f202c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82261ff138dd46b4908533f9313cbffa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8edf1d737bf64e138de788f809061e93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97037c5e0e8c4faea5321b78b21d3eea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2f3c9a7999840a88f5ce2a4a8a6d908": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7829f872d21477aad2d8537c9a0fcf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79d68ea720ba4e42b9f5b01713f202c1",
            "placeholder": "​",
            "style": "IPY_MODEL_bfa456ba358147448af76541857957e8",
            "value": " 1034/1034 [00:00&lt;00:00, 4884.64 examples/s]"
          }
        },
        "ad97b0de4c304edaa5167a47c1c31611": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aeec6f163e6d4a139ff437a7d95fb167": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e68d3b6df28400ca58300460e8bba3d",
            "max": 1034,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_695c5514abf94214abb6b9b25f0155b4",
            "value": 1034
          }
        },
        "b9cca6eebfe34b8eb997ac54156b803f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2f3c9a7999840a88f5ce2a4a8a6d908",
            "placeholder": "​",
            "style": "IPY_MODEL_2534a96dcae140dd83b97df389a761fe",
            "value": " 8559/8559 [00:02&lt;00:00, 2882.40 examples/s]"
          }
        },
        "bfa456ba358147448af76541857957e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd89270fa6ee4b679560ea69ea8c95d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82261ff138dd46b4908533f9313cbffa",
            "placeholder": "​",
            "style": "IPY_MODEL_ad97b0de4c304edaa5167a47c1c31611",
            "value": "Tokenizing validation data: 100%"
          }
        },
        "d6d525ca5690426d8b9967b7f2c9f91e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee282ad93f39416795ba549f82447629": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5c77c29849b47c1841abb5bb9fec068",
              "IPY_MODEL_53b23a74a7ac47b898ae93252d43815d",
              "IPY_MODEL_b9cca6eebfe34b8eb997ac54156b803f"
            ],
            "layout": "IPY_MODEL_62c27373906d46a9a8c194f92503e249"
          }
        },
        "f5c77c29849b47c1841abb5bb9fec068": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_670407ed473843449aa9412a0eda735b",
            "placeholder": "​",
            "style": "IPY_MODEL_d6d525ca5690426d8b9967b7f2c9f91e",
            "value": "Tokenizing training data: 100%"
          }
        },
        "fb968e830f9b4a3f84264ab295f95165": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
