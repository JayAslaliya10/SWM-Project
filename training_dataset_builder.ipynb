{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddcd0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Excluded 100 examples from 'hospital_1'\n",
      "‚úÖ train (excluding hospital_1): 8559  ‚Üí train_text2sql.jsonl\n",
      "‚úÖ val (excluding hospital_1):   1034    ‚Üí val_text2sql.jsonl\n",
      "‚úÖ test (ONLY hospital_1):       100   ‚Üí test_hospital_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "# rebuild_train_val_from_spider.py (notebook cell)\n",
    "import json, sqlite3\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "ROOT = Path(\"spider_data\")\n",
    "TABLES_JSON     = ROOT / \"tables.json\"\n",
    "TRAIN_SPIDER    = ROOT / \"train_spider.json\"\n",
    "TRAIN_OTHERS    = ROOT / \"train_others.json\"\n",
    "DEV_JSON        = ROOT / \"dev.json\"\n",
    "DB_ROOT         = ROOT / \"database\"\n",
    "\n",
    "OUT_TRAIN = Path(\"train_text2sql.jsonl\")\n",
    "OUT_VAL   = Path(\"val_text2sql.jsonl\")\n",
    "OUT_TEST  = Path(\"test_hospital_1.jsonl\")  # NEW: separate test file\n",
    "\n",
    "def load_json(p): return json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# ---- schema render from tables.json (robust) ----\n",
    "def serialize_schema_from_tables(tables_by_db, db_id: str) -> str:\n",
    "    entry = tables_by_db[db_id]\n",
    "    tables = entry[\"table_names_original\"]\n",
    "    cols = entry[\"column_names_original\"]     # list of [table_idx, col_name]\n",
    "    col_table = [c[0] for c in cols]\n",
    "    col_name  = [c[1] for c in cols]\n",
    "    pk = set(entry.get(\"primary_keys\", []))\n",
    "    fks = entry.get(\"foreign_keys\", [])\n",
    "    \n",
    "    # gather columns by table\n",
    "    cols_by_table = defaultdict(list)\n",
    "    for idx, (ti, name) in enumerate(zip(col_table, col_name)):\n",
    "        if ti == -1:  # * or root\n",
    "            continue\n",
    "        cols_by_table[ti].append((idx, name))\n",
    "    \n",
    "    lines = [f\"Database: {db_id}\", \"Tables:\"]\n",
    "    for ti, tname in enumerate(tables):\n",
    "        col_str = \", \".join(\n",
    "            f\"{name}{'*' if idx in pk else ''}\"\n",
    "            for idx, name in cols_by_table.get(ti, [])\n",
    "        )\n",
    "        lines.append(f\"- {tname}({col_str})\")\n",
    "    \n",
    "    for src, tgt in fks:\n",
    "        ti_src = col_table[src]; ti_tgt = col_table[tgt]\n",
    "        if ti_src == -1 or ti_tgt == -1: \n",
    "            continue\n",
    "        lines.append(f\"FK {tables[ti_src]}.{col_name[src]} -> {tables[ti_tgt]}.{col_name[tgt]}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def ensure_semicolon(sql: str) -> str:\n",
    "    sql = (sql or \"\").strip()\n",
    "    return sql if sql.endswith(\";\") else sql + \";\"\n",
    "\n",
    "# MODIFIED: Added exclude_db parameter\n",
    "def pick(rows, exclude_db=None):\n",
    "    out = []\n",
    "    excluded_count = 0\n",
    "    for r in rows:\n",
    "        q = r.get(\"question\")\n",
    "        s = r.get(\"query\") or r.get(\"sql\")\n",
    "        dbid = r.get(\"db_id\")\n",
    "        \n",
    "        # Skip excluded database\n",
    "        if exclude_db and dbid == exclude_db:\n",
    "            excluded_count += 1\n",
    "            continue\n",
    "            \n",
    "        if q and s and dbid:\n",
    "            out.append({\"question\": q.strip(), \"gold_query\": ensure_semicolon(s.strip()), \"db_id\": dbid})\n",
    "    \n",
    "    if excluded_count > 0:\n",
    "        print(f\"[info] Excluded {excluded_count} examples from '{exclude_db}'\")\n",
    "    \n",
    "    return out\n",
    "\n",
    "# NEW: Function to pick ONLY hospital_1\n",
    "def pick_only_hospital(rows):\n",
    "    out = []\n",
    "    for r in rows:\n",
    "        q = r.get(\"question\")\n",
    "        s = r.get(\"query\") or r.get(\"sql\")\n",
    "        dbid = r.get(\"db_id\")\n",
    "        \n",
    "        # Only keep hospital_1\n",
    "        if dbid == \"hospital_1\" and q and s:\n",
    "            out.append({\"question\": q.strip(), \"gold_query\": ensure_semicolon(s.strip()), \"db_id\": dbid})\n",
    "    \n",
    "    return out\n",
    "\n",
    "tables = load_json(TABLES_JSON)\n",
    "tables_by_db = {e[\"db_id\"]: e for e in tables}\n",
    "\n",
    "# Build train/val WITHOUT hospital_1\n",
    "train = pick(load_json(TRAIN_SPIDER), exclude_db=\"hospital_1\") + pick(load_json(TRAIN_OTHERS), exclude_db=\"hospital_1\")\n",
    "val   = pick(load_json(DEV_JSON), exclude_db=\"hospital_1\")\n",
    "\n",
    "# Build test set with ONLY hospital_1\n",
    "test  = pick_only_hospital(load_json(TRAIN_SPIDER)) + \\\n",
    "        pick_only_hospital(load_json(TRAIN_OTHERS)) + \\\n",
    "        pick_only_hospital(load_json(DEV_JSON))\n",
    "\n",
    "# attach schema text + optional sqlite path if present\n",
    "def attach_schema(rows):\n",
    "    out = []\n",
    "    miss = 0\n",
    "    for i, r in enumerate(rows, 1):\n",
    "        dbid = r[\"db_id\"]\n",
    "        if dbid not in tables_by_db:\n",
    "            miss += 1\n",
    "            continue\n",
    "        schema_text = serialize_schema_from_tables(tables_by_db, dbid)\n",
    "        # sqlite is not required for training; we add if exists for later EX\n",
    "        sqlite_path = DB_ROOT / dbid / f\"{dbid}.sqlite\"\n",
    "        out.append({\n",
    "            \"id\": f\"{dbid}_{i}\",\n",
    "            \"dataset\": dbid,\n",
    "            \"db_id\": dbid,\n",
    "            \"sqlite_path\": str(sqlite_path) if sqlite_path.exists() else \"\",\n",
    "            \"schema_serialized\": schema_text,\n",
    "            \"question\": r[\"question\"],\n",
    "            \"gold_query\": r[\"gold_query\"],\n",
    "        })\n",
    "    if miss:\n",
    "        print(f\"[warn] skipped {miss} rows with missing tables.json entries\")\n",
    "    return out\n",
    "\n",
    "train_rows = attach_schema(train)\n",
    "val_rows   = attach_schema(val)\n",
    "test_rows  = attach_schema(test)\n",
    "\n",
    "def dump_jsonl(path, rows):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "dump_jsonl(OUT_TRAIN, train_rows)\n",
    "dump_jsonl(OUT_VAL,   val_rows)\n",
    "dump_jsonl(OUT_TEST,  test_rows)\n",
    "\n",
    "print(f\"‚úÖ train (excluding hospital_1): {len(train_rows)}  ‚Üí {OUT_TRAIN}\")\n",
    "print(f\"‚úÖ val (excluding hospital_1):   {len(val_rows)}    ‚Üí {OUT_VAL}\")\n",
    "print(f\"‚úÖ test (ONLY hospital_1):       {len(test_rows)}   ‚Üí {OUT_TEST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f66b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf755d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPIDER DATASET - Examples per Database\n",
      "============================================================\n",
      "\n",
      "üìÅ Found: train_spider.json\n",
      "\n",
      "‚ùå Not found: train.json\n",
      "\n",
      "üìÅ Found: dev.json\n",
      "\n",
      "============================================================\n",
      "COMBINED TOTALS (sorted by count)\n",
      "============================================================\n",
      "\n",
      "Rank   Database                       Count     \n",
      "------------------------------------------------------------\n",
      "1      college_2                      170       \n",
      "2      college_1                      164       \n",
      "3      hr_1                           124       \n",
      "4      world_1                        120       \n",
      "5      store_1                        112       \n",
      "6      soccer_2                       106       \n",
      "7      bike_1                         104       \n",
      "8      music_1                        100       \n",
      "9      hospital_1                     100       \n",
      "10     music_2                        100       \n",
      "11     dorm_1                         100       \n",
      "12     allergy_1                      98        \n",
      "13     movie_1                        98        \n",
      "14     flight_1                       96        \n",
      "15     driving_school                 93        \n",
      "16     car_1                          92        \n",
      "17     cre_Doc_Tracking_DB            90        \n",
      "18     department_store               88        \n",
      "19     customers_and_addresses        88        \n",
      "20     activity_1                     88        \n",
      "21     network_2                      86        \n",
      "22     products_gen_characteristics   86        \n",
      "23     game_1                         86        \n",
      "24     chinook_1                      84        \n",
      "25     cre_Theme_park                 84        \n",
      "26     cre_Docs_and_Epenses           84        \n",
      "27     cre_Doc_Template_Mgt           84        \n",
      "28     customers_and_invoices         82        \n",
      "29     sakila_1                       82        \n",
      "30     baseball_1                     82        \n",
      "31     e_learning                     82        \n",
      "32     cre_Drama_Workshop_Groups      82        \n",
      "33     wine_1                         82        \n",
      "34     flight_4                       82        \n",
      "35     dog_kennels                    82        \n",
      "36     customers_card_transactions    80        \n",
      "37     apartment_rentals              80        \n",
      "38     formula_1                      80        \n",
      "39     loan_1                         80        \n",
      "40     manufactory_1                  80        \n",
      "41     flight_2                       80        \n",
      "42     tracking_grants_for_research   78        \n",
      "43     student_transcripts_tracking   78        \n",
      "44     inn_1                          74        \n",
      "45     college_3                      74        \n",
      "46     voter_2                        72        \n",
      "47     csu_1                          70        \n",
      "48     club_1                         70        \n",
      "49     election                       68        \n",
      "50     student_1                      68        \n",
      "51     icfp_1                         66        \n",
      "52     wta_1                          62        \n",
      "53     tvshow                         62        \n",
      "54     music_4                        60        \n",
      "55     tracking_orders                60        \n",
      "56     match_season                   58        \n",
      "57     network_1                      56        \n",
      "58     student_assessment             53        \n",
      "59     sports_competition             52        \n",
      "60     small_bank_1                   50        \n",
      "61     insurance_policies             48        \n",
      "62     film_rank                      48        \n",
      "63     tracking_software_problems     48        \n",
      "64     ship_1                         48        \n",
      "65     scientist_1                    48        \n",
      "66     university_basketball          46        \n",
      "67     gas_company                    46        \n",
      "68     aircraft                       46        \n",
      "69     customer_complaints            46        \n",
      "70     concert_singer                 45        \n",
      "71     medicine_enzyme_interaction    44        \n",
      "72     party_people                   44        \n",
      "73     storm_record                   44        \n",
      "74     document_management            44        \n",
      "75     store_product                  44        \n",
      "76     shop_membership                44        \n",
      "77     product_catalog                42        \n",
      "78     race_track                     42        \n",
      "79     insurance_fnol                 42        \n",
      "80     pets_1                         42        \n",
      "81     tracking_share_transactions    41        \n",
      "82     farm                           40        \n",
      "83     musical                        40        \n",
      "84     climbing                       40        \n",
      "85     insurance_and_eClaims          40        \n",
      "86     theme_gallery                  40        \n",
      "87     gymnast                        40        \n",
      "88     wrestler                       40        \n",
      "89     entrepreneur                   40        \n",
      "90     candidate_poll                 40        \n",
      "91     county_public_safety           40        \n",
      "92     behavior_monitoring            40        \n",
      "93     program_share                  40        \n",
      "94     company_office                 40        \n",
      "95     school_player                  40        \n",
      "96     device                         40        \n",
      "97     city_record                    40        \n",
      "98     e_government                   40        \n",
      "99     poker_player                   40        \n",
      "100    orchestra                      40        \n",
      "101    culture_company                38        \n",
      "102    employee_hire_evaluation       38        \n",
      "103    cre_Doc_Control_Systems        33        \n",
      "104    assets_maintenance             31        \n",
      "105    epinions_1                     30        \n",
      "106    cinema                         30        \n",
      "107    machine_repair                 30        \n",
      "108    party_host                     30        \n",
      "109    ship_mission                   30        \n",
      "110    swimming                       30        \n",
      "111    workshop_paper                 30        \n",
      "112    course_teach                   30        \n",
      "113    singer                         30        \n",
      "114    phone_1                        29        \n",
      "115    twitter_1                      27        \n",
      "116    school_finance                 26        \n",
      "117    body_builder                   24        \n",
      "118    train_station                  23        \n",
      "119    restaurant_1                   22        \n",
      "120    entertainment_awards           22        \n",
      "121    book_2                         21        \n",
      "122    perpetrator                    21        \n",
      "123    railway                        21        \n",
      "124    election_representative        20        \n",
      "125    wedding                        20        \n",
      "126    protein_institute              20        \n",
      "127    game_injury                    19        \n",
      "128    flight_company                 19        \n",
      "129    journal_committee              18        \n",
      "130    coffee_shop                    18        \n",
      "131    browser_web                    18        \n",
      "132    products_for_hire              18        \n",
      "133    news_report                    18        \n",
      "134    school_bus                     18        \n",
      "135    museum_visit                   18        \n",
      "136    riding_club                    17        \n",
      "137    mountain_photos                17        \n",
      "138    local_govt_and_lot             17        \n",
      "139    roller_coaster                 17        \n",
      "140    architecture                   17        \n",
      "141    department_management          16        \n",
      "142    customer_deliveries            16        \n",
      "143    station_weather                16        \n",
      "144    manufacturer                   16        \n",
      "145    company_employee               16        \n",
      "146    battle_death                   16        \n",
      "147    performance_attendance         15        \n",
      "148    debate                         15        \n",
      "149    phone_market                   15        \n",
      "150    pilot_record                   15        \n",
      "151    local_govt_in_alabama          15        \n",
      "152    decoration_competition         15        \n",
      "153    solvency_ii                    15        \n",
      "154    customers_campaigns_ecommerce  15        \n",
      "155    customers_and_products_contacts 15        \n",
      "156    voter_1                        15        \n",
      "157    soccer_1                       14        \n",
      "158    local_govt_mdm                 14        \n",
      "159    company_1                      7         \n",
      "160    real_estate_properties         4         \n",
      "\n",
      "============================================================\n",
      "üéØ TOP 3 DATABASES FOR FINE-TUNING:\n",
      "============================================================\n",
      "1. college_2: 170 examples\n",
      "2. college_1: 164 examples\n",
      "3. hr_1: 124 examples\n",
      "\n",
      "============================================================\n",
      "üìä TOP 10 DATABASES:\n",
      "============================================================\n",
      " 1. college_2                 -  170 examples\n",
      " 2. college_1                 -  164 examples\n",
      " 3. hr_1                      -  124 examples\n",
      " 4. world_1                   -  120 examples\n",
      " 5. store_1                   -  112 examples\n",
      " 6. soccer_2                  -  106 examples\n",
      " 7. bike_1                    -  104 examples\n",
      " 8. music_1                   -  100 examples\n",
      " 9. hospital_1                -  100 examples\n",
      "10. music_2                   -  100 examples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "# Path to your spider data - adjust if needed\n",
    "spider_path = \"spider_data\"\n",
    "\n",
    "def count_examples_per_database(json_file):\n",
    "    \"\"\"Count examples for each database in a Spider JSON file\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract database names from each example\n",
    "    db_names = [example['db_id'] for example in data]\n",
    "    \n",
    "    # Count occurrences\n",
    "    return Counter(db_names)\n",
    "\n",
    "def main():\n",
    "    # Check both train and dev files\n",
    "    files_to_check = ['train_spider.json', 'train.json', 'dev.json']\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"SPIDER DATASET - Examples per Database\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_counts = Counter()\n",
    "    \n",
    "    for filename in files_to_check:\n",
    "        filepath = os.path.join(spider_path, filename)\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"\\nüìÅ Found: {filename}\")\n",
    "            counts = count_examples_per_database(filepath)\n",
    "            all_counts.update(counts)\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Not found: {filename}\")\n",
    "    \n",
    "    if all_counts:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"COMBINED TOTALS (sorted by count)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Sort by count (descending)\n",
    "        sorted_dbs = sorted(all_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\n{'Rank':<6} {'Database':<30} {'Count':<10}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for rank, (db_name, count) in enumerate(sorted_dbs, 1):\n",
    "            print(f\"{rank:<6} {db_name:<30} {count:<10}\")\n",
    "        \n",
    "        # Show top 3 recommendations\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üéØ TOP 3 DATABASES FOR FINE-TUNING:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, (db_name, count) in enumerate(sorted_dbs[:3], 1):\n",
    "            print(f\"{i}. {db_name}: {count} examples\")\n",
    "        \n",
    "        # Also show top 10 for quick reference\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìä TOP 10 DATABASES:\")\n",
    "        print(\"=\" * 60)\n",
    "        for i, (db_name, count) in enumerate(sorted_dbs[:10], 1):\n",
    "            print(f\"{i:2d}. {db_name:25s} - {count:4d} examples\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No data files found. Please check your file paths.\")\n",
    "        print(f\"   Looking in: {spider_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d8abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6e89f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7000 examples...\n",
      "\n",
      "============================================================\n",
      "‚úÖ Dataset created!\n",
      "Total examples: 406\n",
      "\n",
      "Breakdown:\n",
      "  hr_1: 124 examples\n",
      "  store_1: 112 examples\n",
      "  college_2: 170 examples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "def get_schema_serialized(db_path, db_id):\n",
    "    \"\"\"Extract schema information from SQLite database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    schema_parts = [f\"Database: {db_id}\", \"Tables:\"]\n",
    "    \n",
    "    table_schemas = []\n",
    "    for table in tables:\n",
    "        cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "        columns = cursor.fetchall()\n",
    "        \n",
    "        col_names = []\n",
    "        for col in columns:\n",
    "            col_name = col[1]\n",
    "            is_pk = col[5] == 1\n",
    "            if is_pk:\n",
    "                col_names.append(f\"{col_name}*\")\n",
    "            else:\n",
    "                col_names.append(col_name)\n",
    "        \n",
    "        table_schema = f\"- {table}({', '.join(col_names)})\"\n",
    "        table_schemas.append(table_schema)\n",
    "    \n",
    "    schema_parts.extend(table_schemas)\n",
    "    \n",
    "    fk_lines = []\n",
    "    for table in tables:\n",
    "        cursor.execute(f\"PRAGMA foreign_key_list({table})\")\n",
    "        fks = cursor.fetchall()\n",
    "        \n",
    "        for fk in fks:\n",
    "            from_table = table\n",
    "            from_col = fk[3]\n",
    "            to_table = fk[2]\n",
    "            to_col = fk[4]\n",
    "            fk_line = f\"FK {from_table}.{from_col} -> {to_table}.{to_col}\"\n",
    "            fk_lines.append(fk_line)\n",
    "    \n",
    "    if fk_lines:\n",
    "        schema_parts.extend(fk_lines)\n",
    "    \n",
    "    conn.close()\n",
    "    return \"\\n\".join(schema_parts)\n",
    "\n",
    "# Load Spider data\n",
    "with open('spider_data/train_spider.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Include all 3 datasets\n",
    "datasets_to_use = ['hr_1', 'store_1', 'college_2']\n",
    "combined_dataset = []\n",
    "counter = {db: 1 for db in datasets_to_use}\n",
    "\n",
    "print(f\"Processing {len(train_data)} examples...\")\n",
    "\n",
    "for example in train_data:\n",
    "    db_id = example['db_id']\n",
    "    \n",
    "    if db_id not in datasets_to_use:\n",
    "        continue\n",
    "    \n",
    "    sqlite_path = f\"spider_data/database/{db_id}/{db_id}.sqlite\"\n",
    "    \n",
    "    if not os.path.exists(sqlite_path):\n",
    "        print(f\"‚ö†Ô∏è  Skipping {db_id}: database not found\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        schema_serialized = get_schema_serialized(sqlite_path, db_id)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error with {db_id}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    entry = {\n",
    "        \"id\": f\"{db_id}_{counter[db_id]}\",\n",
    "        \"dataset\": db_id,\n",
    "        \"db_id\": db_id,\n",
    "        \"sqlite_path\": sqlite_path,\n",
    "        \"schema_serialized\": schema_serialized,\n",
    "        \"question\": example['question'],\n",
    "        \"gold_query\": example['query']\n",
    "    }\n",
    "    \n",
    "    combined_dataset.append(entry)\n",
    "    counter[db_id] += 1\n",
    "\n",
    "# Save\n",
    "with open('Data_for_demo_v2.jsonl', 'w') as f:\n",
    "    for entry in combined_dataset:\n",
    "        f.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Dataset created!\")\n",
    "print(f\"Total examples: {len(combined_dataset)}\")\n",
    "print(\"\\nBreakdown:\")\n",
    "for db_id in datasets_to_use:\n",
    "    count = counter[db_id] - 1\n",
    "    print(f\"  {db_id}: {count} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da3f275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for world_1 database...\n",
      "Looking for: spider_data/database/world_1/world_1.sqlite\n",
      "‚úÖ world_1.sqlite EXISTS\n",
      "Tables in world_1: ['city', 'sqlite_sequence', 'country', 'countrylanguage']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CHECK: Verify world_1 database exists\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "\n",
    "# Check if world_1 database exists\n",
    "world_1_path = \"spider_data/database/world_1/world_1.sqlite\"\n",
    "\n",
    "print(\"Checking for world_1 database...\")\n",
    "print(f\"Looking for: {world_1_path}\")\n",
    "\n",
    "if os.path.exists(world_1_path):\n",
    "    print(\"‚úÖ world_1.sqlite EXISTS\")\n",
    "    \n",
    "    # Check if it has tables\n",
    "    import sqlite3\n",
    "    conn = sqlite3.connect(world_1_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Tables in world_1: {tables}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå world_1.sqlite NOT FOUND\")\n",
    "    print(\"\\nLet's check what databases DO exist:\")\n",
    "    \n",
    "    db_folder = \"spider_data/database\"\n",
    "    if os.path.exists(db_folder):\n",
    "        databases = [d for d in os.listdir(db_folder) if os.path.isdir(os.path.join(db_folder, d))]\n",
    "        print(f\"\\nAvailable databases ({len(databases)}):\")\n",
    "        for db in sorted(databases)[:20]:  # Show first 20\n",
    "            print(f\"  - {db}\")\n",
    "    else:\n",
    "        print(\"‚ùå spider_data/database folder not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40e40d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples in train_spider.json: 7000\n",
      "world_1 examples found: 0\n",
      "\n",
      "‚ùå world_1 examples NOT FOUND in train_spider.json\n",
      "\n",
      "Let me check what db_ids ARE in the data:\n",
      "\n",
      "Top 20 databases by count:\n",
      "  college_2: 170\n",
      "  college_1: 164\n",
      "  hr_1: 124\n",
      "  store_1: 112\n",
      "  soccer_2: 106\n",
      "  bike_1: 104\n",
      "  music_1: 100\n",
      "  hospital_1: 100\n",
      "  music_2: 100\n",
      "  dorm_1: 100\n",
      "  allergy_1: 98\n",
      "  movie_1: 98\n",
      "  flight_1: 96\n",
      "  driving_school: 93\n",
      "  cre_Doc_Tracking_DB: 90\n",
      "  department_store: 88\n",
      "  customers_and_addresses: 88\n",
      "  activity_1: 88\n",
      "  network_2: 86\n",
      "  products_gen_characteristics: 86\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CHECK: Find world_1 examples in Spider data\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "\n",
    "# Load Spider training data\n",
    "with open('spider_data/train_spider.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Count world_1 examples\n",
    "world_1_examples = [ex for ex in train_data if ex['db_id'] == 'world_1']\n",
    "\n",
    "print(f\"Total examples in train_spider.json: {len(train_data)}\")\n",
    "print(f\"world_1 examples found: {len(world_1_examples)}\")\n",
    "\n",
    "if len(world_1_examples) > 0:\n",
    "    print(\"\\n‚úÖ world_1 examples EXIST in training data\")\n",
    "    print(\"\\nSample world_1 example:\")\n",
    "    print(f\"  Question: {world_1_examples[0]['question']}\")\n",
    "    print(f\"  Query: {world_1_examples[0]['query']}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå world_1 examples NOT FOUND in train_spider.json\")\n",
    "    print(\"\\nLet me check what db_ids ARE in the data:\")\n",
    "    \n",
    "    from collections import Counter\n",
    "    db_counts = Counter([ex['db_id'] for ex in train_data])\n",
    "    \n",
    "    print(f\"\\nTop 20 databases by count:\")\n",
    "    for db_id, count in db_counts.most_common(20):\n",
    "        print(f\"  {db_id}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
