{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cba658c2",
      "metadata": {
        "id": "cba658c2"
      },
      "source": [
        "## Importing Necessary Libraries and Setting up Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7cfe5fc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cfe5fc8",
        "outputId": "ecfd5f67-aa06-4e5e-810c-e1a6a53b7394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "ENVIRONMENT CHECK\n",
            "============================================================\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "\n",
            "FILE PATHS:\n",
            "------------------------------------------------------------\n",
            "Training data:    train_text2sql.jsonl\n",
            "Validation data:  val_text2sql.jsonl\n",
            "Test data:        test_hospital_1.jsonl\n",
            "SQLite DB:        hospital_1.sqlite\n",
            "Save directory:   saved_model\n",
            "Results CSV:      results_hospital_1_bilstm.csv\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE VERIFICATION:\n",
            "------------------------------------------------------------\n",
            "Training data       : ✓ EXISTS\n",
            "Validation data     : ✓ EXISTS\n",
            "Test data           : ✓ EXISTS\n",
            "SQLite DB           : ✓ EXISTS\n",
            "------------------------------------------------------------\n",
            "\n",
            "✅ All required files found!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "import time\n",
        "import sqlite3\n",
        "import re\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import sqlglot\n",
        "from sqlglot import parse_one\n",
        "\n",
        "# In Colab use:\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Check device availability\n",
        "print(\"=\" * 60)\n",
        "print(\"ENVIRONMENT CHECK\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     DEVICE = torch.device(\"cuda\")\n",
        "#     print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "# elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "#     DEVICE = torch.device(\"mps\")\n",
        "#     print(\"Using Apple Silicon MPS\")\n",
        "# else:\n",
        "#     DEVICE = torch.device(\"cpu\")\n",
        "#     print(\"Using CPU\")\n",
        "\n",
        "# print(f\"Device selected: {DEVICE}\")\n",
        "# print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# File Paths\n",
        "# ============================================================\n",
        "\n",
        "# Data paths\n",
        "TRAIN_JSONL = Path(\"train_text2sql.jsonl\")\n",
        "VAL_JSONL = Path(\"val_text2sql.jsonl\")\n",
        "TEST_JSONL = Path(\"test_hospital_1.jsonl\")\n",
        "\n",
        "# # Database path\n",
        "# SQLITE_DB = Path(\"spider_data/database/hospital_1/hospital_1.sqlite\")\n",
        "\n",
        "# Database path for colab\n",
        "SQLITE_DB = Path(\"hospital_1.sqlite\")\n",
        "\n",
        "# Save directory for model checkpoints\n",
        "SAVE_DIR = Path(\"saved_model\")\n",
        "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Results\n",
        "RESULTS_CSV = Path(\"results_hospital_1_bilstm.csv\")\n",
        "\n",
        "print(\"\\nFILE PATHS:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Training data:    {TRAIN_JSONL}\")\n",
        "print(f\"Validation data:  {VAL_JSONL}\")\n",
        "print(f\"Test data:        {TEST_JSONL}\")\n",
        "print(f\"SQLite DB:        {SQLITE_DB}\")\n",
        "print(f\"Save directory:   {SAVE_DIR}\")\n",
        "print(f\"Results CSV:      {RESULTS_CSV}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Verify files exist\n",
        "print(\"\\nFILE VERIFICATION:\")\n",
        "print(\"-\" * 60)\n",
        "files_to_check = {\n",
        "    \"Training data\": TRAIN_JSONL,\n",
        "    \"Validation data\": VAL_JSONL,\n",
        "    \"Test data\": TEST_JSONL,\n",
        "    \"SQLite DB\": SQLITE_DB,\n",
        "}\n",
        "\n",
        "all_exist = True\n",
        "for name, path in files_to_check.items():\n",
        "    exists = path.exists()\n",
        "    status = \"✓ EXISTS\" if exists else \"✗ MISSING\"\n",
        "    print(f\"{name:20s}: {status}\")\n",
        "    if not exists:\n",
        "        all_exist = False\n",
        "\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if all_exist:\n",
        "    print(\"\\n✅ All required files found!\")\n",
        "else:\n",
        "    print(\"\\n⚠️  WARNING: Some files are missing. Please upload them before proceeding.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4eb3b25",
      "metadata": {
        "id": "f4eb3b25"
      },
      "source": [
        "## Setting up the Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "628469f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "628469f1",
        "outputId": "cd682198-8d65-42a0-ffc3-0459964d09e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CONFIGURATION PARAMETERS\n",
            "============================================================\n",
            "\n",
            "Training Hyperparameters:\n",
            "------------------------------------------------------------\n",
            "Epochs:          15\n",
            "Batch size:      32\n",
            "Learning rate:   0.0001\n",
            "Gradient clip:   0.5\n",
            "Random seed:     42\n",
            "\n",
            "Model Architecture:\n",
            "------------------------------------------------------------\n",
            "Embedding dim:   512\n",
            "Hidden dim:      512\n",
            "Encoder layers:  2\n",
            "Dropout rate:    0.2\n",
            "\n",
            "Generation Parameters:\n",
            "------------------------------------------------------------\n",
            "Max decode length:     128\n",
            "Preview decode length: 64\n",
            "\n",
            "Data Processing:\n",
            "------------------------------------------------------------\n",
            "Schema conditioning: True\n",
            "\n",
            "✅ Random seeds set for reproducibility\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"CONFIGURATION PARAMETERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Training Hyperparameters\n",
        "# ============================================================\n",
        "\n",
        "EPOCHS = 15\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "GRAD_CLIP = 0.5\n",
        "SEED = 42\n",
        "\n",
        "print(\"\\nTraining Hyperparameters:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Epochs:          {EPOCHS}\")\n",
        "print(f\"Batch size:      {BATCH_SIZE}\")\n",
        "print(f\"Learning rate:   {LEARNING_RATE}\")\n",
        "print(f\"Gradient clip:   {GRAD_CLIP}\")\n",
        "print(f\"Random seed:     {SEED}\")\n",
        "\n",
        "# ============================================================\n",
        "# Model Architecture Parameters\n",
        "# ============================================================\n",
        "\n",
        "EMB_DIM = 512       # Embedding dimension\n",
        "HID_DIM = 512       # Hidden dimension\n",
        "NUM_LAYERS = 2      # Number of LSTM layers in encoder\n",
        "DROPOUT = 0.2       # Dropout rate\n",
        "\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Embedding dim:   {EMB_DIM}\")\n",
        "print(f\"Hidden dim:      {HID_DIM}\")\n",
        "print(f\"Encoder layers:  {NUM_LAYERS}\")\n",
        "print(f\"Dropout rate:    {DROPOUT}\")\n",
        "\n",
        "# ============================================================\n",
        "# Generation Parameters\n",
        "# ============================================================\n",
        "\n",
        "MAX_DECODE_LEN = 128        # Maximum SQL tokens to generate\n",
        "MAX_DECODE_PREVIEW = 64     # Preview length during training\n",
        "\n",
        "print(\"\\nGeneration Parameters:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Max decode length:     {MAX_DECODE_LEN}\")\n",
        "print(f\"Preview decode length: {MAX_DECODE_PREVIEW}\")\n",
        "\n",
        "# ============================================================\n",
        "# Data Processing\n",
        "# ============================================================\n",
        "\n",
        "SCHEMA_CONDITION = True     # Prepend schema to question\n",
        "\n",
        "print(\"\\nData Processing:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Schema conditioning: {SCHEMA_CONDITION}\")\n",
        "\n",
        "# ============================================================\n",
        "# Set Random Seeds for Reproducibility\n",
        "# ============================================================\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    # MPS doesn't need special seeding\n",
        "    pass\n",
        "\n",
        "print(\"\\n✅ Random seeds set for reproducibility\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6af1003",
      "metadata": {
        "id": "b6af1003"
      },
      "source": [
        "## Tokenizer and Vocab Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "85376cd6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85376cd6",
        "outputId": "cb21efc1-3ea0-4d91-c06b-e0fb74747c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TOKENIZER AND VOCABULARY\n",
            "============================================================\n",
            "\n",
            "Testing Tokenizer:\n",
            "------------------------------------------------------------\n",
            "Added: SELECT * FROM Department WHERE DepartmentID = 1 ;\n",
            "Added: SELECT Name FROM Physician WHERE EmployeeID >= 100 ;\n",
            "\n",
            "Vocabulary size: 18\n",
            "Sample tokens: ['<pad>', '<unk>', '<bos>', '<eos>', 'SELECT', '*', 'FROM', 'Department', 'WHERE', 'DepartmentID', '=', '1', ';', 'Name', 'Physician', 'EmployeeID', '>=', '100']\n",
            "\n",
            "Encoding test:\n",
            "  Original: SELECT Name FROM Department ;\n",
            "  Encoded:  [2, 4, 13, 6, 7, 12, 3]\n",
            "  Decoded:  <bos> SELECT Name FROM Department ; <eos>\n",
            "\n",
            "Special token IDs:\n",
            "  PAD: 0\n",
            "  UNK: 1\n",
            "  BOS: 2\n",
            "  EOS: 3\n",
            "\n",
            "✅ Tokenizer working correctly!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TOKENIZER AND VOCABULARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "class VocabTok:\n",
        "    \"\"\"\n",
        "    Simple vocabulary and tokenizer for SQL generation.\n",
        "    Uses basic regex-based tokenization to split identifiers, numbers, and operators.\n",
        "    \"\"\"\n",
        "\n",
        "    # Special tokens\n",
        "    PAD = \"<pad>\"\n",
        "    UNK = \"<unk>\"\n",
        "    BOS = \"<bos>\"\n",
        "    EOS = \"<eos>\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize vocabulary with special tokens.\"\"\"\n",
        "        self.stoi = {\n",
        "            self.PAD: 0,\n",
        "            self.UNK: 1,\n",
        "            self.BOS: 2,\n",
        "            self.EOS: 3\n",
        "        }\n",
        "        self.itos = [self.PAD, self.UNK, self.BOS, self.EOS]\n",
        "\n",
        "    def add_token(self, tok):\n",
        "        \"\"\"Add a single token to vocabulary if not present.\"\"\"\n",
        "        if tok not in self.stoi:\n",
        "            self.stoi[tok] = len(self.itos)\n",
        "            self.itos.append(tok)\n",
        "\n",
        "    def add_sentence(self, s):\n",
        "        \"\"\"Tokenize a sentence and add all tokens to vocabulary.\"\"\"\n",
        "        for t in self._basic_tokenize(s):\n",
        "            self.add_token(t)\n",
        "\n",
        "    def encode(self, s, add_bos=False, add_eos=False):\n",
        "        \"\"\"\n",
        "        Convert a string to token IDs.\n",
        "\n",
        "        Args:\n",
        "            s: Input string\n",
        "            add_bos: Whether to prepend BOS token\n",
        "            add_eos: Whether to append EOS token\n",
        "\n",
        "        Returns:\n",
        "            List of token IDs\n",
        "        \"\"\"\n",
        "        ids = []\n",
        "        if add_bos:\n",
        "            ids.append(self.stoi[self.BOS])\n",
        "\n",
        "        ids += [self.stoi.get(t, self.stoi[self.UNK]) for t in self._basic_tokenize(s)]\n",
        "\n",
        "        if add_eos:\n",
        "            ids.append(self.stoi[self.EOS])\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        \"\"\"\n",
        "        Convert token IDs back to string.\n",
        "\n",
        "        Args:\n",
        "            ids: List of token IDs\n",
        "\n",
        "        Returns:\n",
        "            Decoded string\n",
        "        \"\"\"\n",
        "        return \" \".join(self.itos[i] for i in ids if i < len(self.itos))\n",
        "\n",
        "    def to_json(self):\n",
        "        \"\"\"Serialize vocabulary to JSON-compatible dict.\"\"\"\n",
        "        return {\"itos\": self.itos}\n",
        "\n",
        "    @classmethod\n",
        "    def from_json(cls, obj):\n",
        "        \"\"\"Load vocabulary from JSON object.\"\"\"\n",
        "        v = cls()\n",
        "        v.itos = obj[\"itos\"]\n",
        "        v.stoi = {t: i for i, t in enumerate(v.itos)}\n",
        "        return v\n",
        "\n",
        "    @staticmethod\n",
        "    def _basic_tokenize(s):\n",
        "        \"\"\"\n",
        "        Basic regex tokenization that splits:\n",
        "        - Identifiers: [A-Za-z_][A-Za-z_0-9]*\n",
        "        - Numbers: [0-9]+\n",
        "        - Multi-char operators: ==, !=, >=, <=\n",
        "        - Single characters: everything else\n",
        "\n",
        "        Preserves case for SQL keywords and identifiers.\n",
        "        \"\"\"\n",
        "        return re.findall(r\"[A-Za-z_][A-Za-z_0-9]*|[0-9]+|==|!=|>=|<=|[^\\s]\", s)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return vocabulary size.\"\"\"\n",
        "        return len(self.itos)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Test the Tokenizer\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nTesting Tokenizer:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Create a test tokenizer\n",
        "test_tok = VocabTok()\n",
        "\n",
        "# Add some test sentences\n",
        "test_sentences = [\n",
        "    \"SELECT * FROM Department WHERE DepartmentID = 1 ;\",\n",
        "    \"SELECT Name FROM Physician WHERE EmployeeID >= 100 ;\"\n",
        "]\n",
        "\n",
        "for sent in test_sentences:\n",
        "    test_tok.add_sentence(sent)\n",
        "    print(f\"Added: {sent}\")\n",
        "\n",
        "print(f\"\\nVocabulary size: {len(test_tok)}\")\n",
        "print(f\"Sample tokens: {test_tok.itos[:20]}\")\n",
        "\n",
        "# Test encoding\n",
        "test_text = \"SELECT Name FROM Department ;\"\n",
        "encoded = test_tok.encode(test_text, add_bos=True, add_eos=True)\n",
        "decoded = test_tok.decode(encoded)\n",
        "\n",
        "print(f\"\\nEncoding test:\")\n",
        "print(f\"  Original: {test_text}\")\n",
        "print(f\"  Encoded:  {encoded}\")\n",
        "print(f\"  Decoded:  {decoded}\")\n",
        "\n",
        "# Test special tokens\n",
        "print(f\"\\nSpecial token IDs:\")\n",
        "print(f\"  PAD: {test_tok.stoi[VocabTok.PAD]}\")\n",
        "print(f\"  UNK: {test_tok.stoi[VocabTok.UNK]}\")\n",
        "print(f\"  BOS: {test_tok.stoi[VocabTok.BOS]}\")\n",
        "print(f\"  EOS: {test_tok.stoi[VocabTok.EOS]}\")\n",
        "\n",
        "print(\"\\n✅ Tokenizer working correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c041257",
      "metadata": {
        "id": "6c041257"
      },
      "source": [
        "## Data Loading & Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "27e98e6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27e98e6a",
        "outputId": "95945261-ab85-4c62-bbac-fc3d1ae75260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DATA LOADING & DATASET CLASS\n",
            "============================================================\n",
            "\n",
            "Loading datasets...\n",
            "------------------------------------------------------------\n",
            "Training examples:   8559\n",
            "Validation examples: 1034\n",
            "Test examples:       100\n",
            "\n",
            "Building vocabulary...\n",
            "------------------------------------------------------------\n",
            "Vocabulary size: 7468\n",
            "Sample vocab (first 30 tokens):\n",
            "  ['<pad>', '<unk>', '<bos>', '<eos>', 'Schema', ':', 'Database', 'department_management', 'Tables', '-', 'department', '(', 'Department_ID', '*', ',', 'Name', 'Creation', 'Ranking', 'Budget_in_Billions', 'Num_Employees', ')', 'head', 'head_ID', 'name', 'born_state', 'age', 'management', 'department_ID', 'temporary_acting', 'FK']\n",
            "\n",
            "Creating PyTorch datasets...\n",
            "------------------------------------------------------------\n",
            "Train dataset size: 8559\n",
            "Val dataset size:   1034\n",
            "Test dataset size:  100\n",
            "\n",
            "Creating DataLoaders...\n",
            "------------------------------------------------------------\n",
            "Train batches: 268\n",
            "Val batches:   33\n",
            "Test batches:  100\n",
            "\n",
            "Testing data loading...\n",
            "------------------------------------------------------------\n",
            "Source batch shape: torch.Size([32, 984])\n",
            "Target batch shape: torch.Size([32, 70])\n",
            "Number of schemas:  32\n",
            "\n",
            "First example in batch:\n",
            "  Source (decoded): Schema : Database : match_season Tables : - country ( Country_id * , Country_name , Capital , Official_native_language ) - team ( Team_id * , Name ) - match_season ( Season * , Player , Position , Cou...\n",
            "  Target (decoded): <bos> SELECT College FROM match_season WHERE POSITION = \" Midfielder \" INTERSECT SELECT College FROM match_season WHERE POSITION = \" Defender \" ; <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "\n",
            "✅ Data loading working correctly!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"DATA LOADING & DATASET CLASS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Utility Function to Load JSONL Files\n",
        "# ============================================================\n",
        "\n",
        "def load_jsonl(path):\n",
        "    \"\"\"\n",
        "    Load JSONL file and return list of dictionaries.\n",
        "\n",
        "    Args:\n",
        "        path: Path to JSONL file\n",
        "\n",
        "    Returns:\n",
        "        List of dictionaries (one per line)\n",
        "    \"\"\"\n",
        "    if not path or not path.exists():\n",
        "        return []\n",
        "\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                rows.append(json.loads(line))\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# PyTorch Dataset Class\n",
        "# ============================================================\n",
        "\n",
        "class T2SQLDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for Text-to-SQL training.\n",
        "\n",
        "    Each example contains:\n",
        "    - Source: Question (optionally with schema)\n",
        "    - Target: Gold SQL query\n",
        "    - Schema: Database schema text (for masking)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, rows, tok, schema_condition=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            rows: List of data dictionaries\n",
        "            tok: VocabTok tokenizer instance\n",
        "            schema_condition: Whether to prepend schema to question\n",
        "        \"\"\"\n",
        "        self.rows = rows\n",
        "        self.tok = tok\n",
        "        self.schema_condition = schema_condition\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single example.\n",
        "\n",
        "        Returns:\n",
        "            src: Tensor of source token IDs\n",
        "            tgt: Tensor of target token IDs (with BOS/EOS)\n",
        "            schema: Schema text string (for vocab masking)\n",
        "        \"\"\"\n",
        "        r = self.rows[idx]\n",
        "\n",
        "        # Build source text (question + optional schema)\n",
        "        if self.schema_condition:\n",
        "            src_text = f\"Schema:\\n{r['schema_serialized']}\\nQuestion:\\n{r['question']}\"\n",
        "        else:\n",
        "            src_text = r['question']\n",
        "\n",
        "        # Target SQL\n",
        "        tgt_text = r['gold_query']\n",
        "\n",
        "        # Encode to token IDs\n",
        "        src = torch.tensor(self.tok.encode(src_text), dtype=torch.long)\n",
        "        tgt = torch.tensor(self.tok.encode(tgt_text, add_bos=True, add_eos=True), dtype=torch.long)\n",
        "\n",
        "        return src, tgt, r['schema_serialized']\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Collate Function for Batching\n",
        "# ============================================================\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function to pad sequences in a batch.\n",
        "\n",
        "    Args:\n",
        "        batch: List of (src, tgt, schema) tuples\n",
        "\n",
        "    Returns:\n",
        "        srcs: Padded source tensor [B, max_src_len]\n",
        "        tgts: Padded target tensor [B, max_tgt_len]\n",
        "        schemas: List of schema strings\n",
        "    \"\"\"\n",
        "    srcs, tgts, schemas = zip(*batch)\n",
        "\n",
        "    # Pad sequences with PAD token (id=0)\n",
        "    srcs = pad_sequence(srcs, batch_first=True, padding_value=0)\n",
        "    tgts = pad_sequence(tgts, batch_first=True, padding_value=0)\n",
        "\n",
        "    return srcs, tgts, list(schemas)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Build Vocabulary from Data\n",
        "# ============================================================\n",
        "\n",
        "def build_vocab(train_rows, val_rows, test_rows):\n",
        "    \"\"\"\n",
        "    Build vocabulary from training, validation, and test data.\n",
        "\n",
        "    Args:\n",
        "        train_rows: Training examples\n",
        "        val_rows: Validation examples\n",
        "        test_rows: Test examples (to ensure schema tokens are in vocab)\n",
        "\n",
        "    Returns:\n",
        "        VocabTok instance with complete vocabulary\n",
        "    \"\"\"\n",
        "    tok = VocabTok()\n",
        "\n",
        "    # Add tokens from train + val (schema + question + gold SQL)\n",
        "    for r in (train_rows + val_rows):\n",
        "        tok.add_sentence(f\"Schema:\\n{r['schema_serialized']}\\nQuestion:\\n{r['question']}\")\n",
        "        tok.add_sentence(r[\"gold_query\"])\n",
        "\n",
        "    # Add tokens from test schema (both original and lowercase for robustness)\n",
        "    for r in test_rows:\n",
        "        schema = r[\"schema_serialized\"]\n",
        "        tok.add_sentence(schema)\n",
        "        tok.add_sentence(schema.lower())\n",
        "\n",
        "    return tok\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Load Data and Build Vocabulary\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nLoading datasets...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "train_rows = load_jsonl(TRAIN_JSONL)\n",
        "val_rows = load_jsonl(VAL_JSONL)\n",
        "test_rows = load_jsonl(TEST_JSONL)\n",
        "\n",
        "print(f\"Training examples:   {len(train_rows)}\")\n",
        "print(f\"Validation examples: {len(val_rows)}\")\n",
        "print(f\"Test examples:       {len(test_rows)}\")\n",
        "\n",
        "print(\"\\nBuilding vocabulary...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "vocab = build_vocab(train_rows, val_rows, test_rows)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Sample vocab (first 30 tokens):\")\n",
        "print(f\"  {vocab.itos[:30]}\")\n",
        "\n",
        "# ============================================================\n",
        "# Create Datasets\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nCreating PyTorch datasets...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "train_dataset = T2SQLDataset(train_rows, vocab, schema_condition=SCHEMA_CONDITION)\n",
        "val_dataset = T2SQLDataset(val_rows, vocab, schema_condition=SCHEMA_CONDITION)\n",
        "test_dataset = T2SQLDataset(test_rows, vocab, schema_condition=SCHEMA_CONDITION)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Val dataset size:   {len(val_dataset)}\")\n",
        "print(f\"Test dataset size:  {len(test_dataset)}\")\n",
        "\n",
        "# ============================================================\n",
        "# Create DataLoaders\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nCreating DataLoaders...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,  # Batch size 1 for evaluation\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches:   {len(val_loader)}\")\n",
        "print(f\"Test batches:  {len(test_loader)}\")\n",
        "\n",
        "# ============================================================\n",
        "# Test Data Loading\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nTesting data loading...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Get one batch\n",
        "src_batch, tgt_batch, schema_batch = next(iter(train_loader))\n",
        "\n",
        "print(f\"Source batch shape: {src_batch.shape}\")\n",
        "print(f\"Target batch shape: {tgt_batch.shape}\")\n",
        "print(f\"Number of schemas:  {len(schema_batch)}\")\n",
        "\n",
        "# Decode first example\n",
        "print(f\"\\nFirst example in batch:\")\n",
        "print(f\"  Source (decoded): {vocab.decode(src_batch[0].tolist())[:200]}...\")\n",
        "print(f\"  Target (decoded): {vocab.decode(tgt_batch[0].tolist())}\")\n",
        "\n",
        "print(\"\\n✅ Data loading working correctly!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "lyqgXfeO-nFe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyqgXfeO-nFe",
        "outputId": "ebd51493-df1a-4885-b626-d33893c632fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "VALIDATION CHECK\n",
            "============================================================\n",
            "Source shape: torch.Size([32, 517])\n",
            "Target shape: torch.Size([32, 69])\n",
            "Source sample (first 50 tokens): [4, 5, 6, 5, 2844, 8, 5, 9, 2845, 11, 2846, 13, 14, 2847, 14, 1095, 14, 2318, 14, 158, 20, 9, 2848, 11, 2847, 13, 14, 158, 14, 2849, 20, 9, 2850, 11, 158, 13, 14, 2847, 14, 2851, 20, 9, 2852, 11, 2847, 13, 14, 2853, 14, 158]\n",
            "Target sample (first 20 tokens): [2, 44, 2862, 46, 2861, 47, 191, 112, 2866, 80, 191, 1255, 2867, 48, 3, 0, 0, 0, 0, 0]\n",
            "Max source value: 6742\n",
            "Max target value: 6742\n",
            "Vocab size: 7468\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Diagnostic check\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"VALIDATION CHECK\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Quick sanity check on data\n",
        "sample_batch = next(iter(train_loader))\n",
        "src, tgt, schemas = sample_batch\n",
        "print(f\"Source shape: {src.shape}\")\n",
        "print(f\"Target shape: {tgt.shape}\")\n",
        "print(f\"Source sample (first 50 tokens): {src[0][:50].tolist()}\")\n",
        "print(f\"Target sample (first 20 tokens): {tgt[0][:20].tolist()}\")\n",
        "print(f\"Max source value: {src.max().item()}\")\n",
        "print(f\"Max target value: {tgt.max().item()}\")\n",
        "print(f\"Vocab size: {len(vocab)}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9c54cb3",
      "metadata": {
        "id": "c9c54cb3"
      },
      "source": [
        "## Schema-Aware Vocabulary Masking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3762687c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3762687c",
        "outputId": "4b973750-6721-450c-a1e0-681b05038907"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SCHEMA-AWARE VOCABULARY MASKING\n",
            "============================================================\n",
            "\n",
            "SQL Keywords defined:\n",
            "  Total keywords: 40\n",
            "  Sample: ['having', 'right', 'exists', 'order', 'like', 'or', 'intersect', 'between', 'case', 'in']\n",
            "\n",
            "SQL Tokens defined:\n",
            "  Total tokens: 17\n",
            "  All tokens: {'=', '.', '%', ';', '<=', '<', '>=', '!=', ',', '(', '>', '-', '/', '||', ')', '*', '+'}\n",
            "\n",
            "------------------------------------------------------------\n",
            "Testing Schema Masking:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Test Example:\n",
            "  Schema:\n",
            "Database: hospital_1\n",
            "Tables:\n",
            "- Physician(EmployeeID*, Name, Position, SSN)\n",
            "- Department(DepartmentID*, Name, Head)\n",
            "- Affiliated_With(Physician*, Department, PrimaryAffiliation)\n",
            "- Procedures(Code*, Nam...\n",
            "  Question: Which department has the largest number of employees?\n",
            "  Gold SQL: SELECT name FROM department GROUP BY departmentID ORDER BY count(departmentID) DESC LIMIT 1;\n",
            "\n",
            "Extracted Identifiers (109 total):\n",
            "  ['Address', 'Affiliated_With', 'Appointment', 'AppointmentID', 'AssistingNurse', 'Block', 'BlockCode', 'BlockFloor', 'Brand', 'CertificationDate', 'CertificationExpires', 'Code', 'Cost', 'Date', 'DateUndergoes', 'Department', 'DepartmentID', 'Description', 'Dose', 'EmployeeID']...\n",
            "\n",
            "Vocabulary Mask:\n",
            "  Total vocab size: 7468\n",
            "  Allowed tokens:   220\n",
            "  Blocked tokens:   7248\n",
            "  Allowed ratio:    2.95%\n",
            "\n",
            "Sample allowed tokens (first 30):\n",
            "  ['<unk>', '<bos>', '<eos>', '-', 'department', '(', '*', ',', 'Name', ')', 'head', 'name', 'FK', '.', '>', 'SELECT', 'count', 'FROM', 'WHERE', ';', 'and', 'by', 'ORDER', 'BY', 'max', 'min', 'is', 'between', 'avg', 'BETWEEN']\n",
            "\n",
            "Sample blocked tokens (first 20):\n",
            "  ['<pad>', 'Schema', ':', 'Database', 'department_management', 'Tables', 'Department_ID', 'Creation', 'Ranking', 'Budget_in_Billions', 'Num_Employees', 'head_ID', 'born_state', 'age', 'management', 'department_ID', 'temporary_acting', 'Question', 'How', 'many']\n",
            "\n",
            "✅ Schema masking working correctly!\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"SCHEMA-AWARE VOCABULARY MASKING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# SQL Keywords and Tokens\n",
        "# ============================================================\n",
        "\n",
        "# Standard SQL keywords (lowercase)\n",
        "SQL_KEYWORDS = {\n",
        "    \"select\", \"from\", \"where\", \"group\", \"by\", \"order\", \"limit\", \"having\", \"distinct\", \"as\",\n",
        "    \"join\", \"inner\", \"left\", \"right\", \"on\", \"and\", \"or\", \"not\", \"in\", \"between\", \"like\",\n",
        "    \"count\", \"sum\", \"avg\", \"min\", \"max\", \"asc\", \"desc\", \"union\", \"intersect\", \"except\",\n",
        "    \"case\", \"when\", \"then\", \"end\", \"exists\", \"all\", \"any\", \"is\", \"null\"\n",
        "}\n",
        "\n",
        "# SQL operators and punctuation\n",
        "SQL_TOKENS = {\n",
        "    \"*\", \"(\", \")\", \",\", \".\", \"=\", \"<\", \">\", \"<=\", \">=\", \"!=\", \";\", \"+\", \"-\", \"/\", \"%\", \"||\"\n",
        "}\n",
        "\n",
        "print(\"\\nSQL Keywords defined:\")\n",
        "print(f\"  Total keywords: {len(SQL_KEYWORDS)}\")\n",
        "print(f\"  Sample: {list(SQL_KEYWORDS)[:10]}\")\n",
        "\n",
        "print(\"\\nSQL Tokens defined:\")\n",
        "print(f\"  Total tokens: {len(SQL_TOKENS)}\")\n",
        "print(f\"  All tokens: {SQL_TOKENS}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Extract Identifiers from Schema\n",
        "# ============================================================\n",
        "\n",
        "def extract_identifiers_from_schema(schema_text):\n",
        "    \"\"\"\n",
        "    Extract table and column names from schema text.\n",
        "\n",
        "    Args:\n",
        "        schema_text: Schema string (e.g., from schema_serialized field)\n",
        "\n",
        "    Returns:\n",
        "        Set of identifiers (both original case and lowercase)\n",
        "    \"\"\"\n",
        "    names = set()\n",
        "\n",
        "    for line in schema_text.splitlines():\n",
        "        # Find all identifiers (alphanumeric + underscore)\n",
        "        for tok in re.findall(r\"[A-Za-z_][A-Za-z_0-9]*\", line):\n",
        "            # Skip common schema metadata words\n",
        "            if tok.lower() in {\"database\", \"tables\", \"foreign\", \"keys\"}:\n",
        "                continue\n",
        "\n",
        "            # Add both original case and lowercase\n",
        "            names.add(tok)\n",
        "            names.add(tok.lower())\n",
        "\n",
        "    return names\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Build Vocabulary Mask for Example\n",
        "# ============================================================\n",
        "\n",
        "def build_vocab_mask_for_example(tok, schema_text):\n",
        "    \"\"\"\n",
        "    Create a boolean mask indicating which vocabulary tokens are allowed\n",
        "    for a given schema.\n",
        "\n",
        "    Allowed tokens:\n",
        "    - SQL keywords (both cases)\n",
        "    - SQL operators/punctuation\n",
        "    - Identifiers from schema (original + lowercase)\n",
        "    - Digits 0-9\n",
        "    - Special tokens (BOS, EOS, UNK)\n",
        "    - PAD is NOT allowed (should never be predicted)\n",
        "\n",
        "    Args:\n",
        "        tok: VocabTok instance\n",
        "        schema_text: Schema string\n",
        "\n",
        "    Returns:\n",
        "        Boolean tensor [V] where True = allowed\n",
        "    \"\"\"\n",
        "    allowed = set(SQL_TOKENS)\n",
        "\n",
        "    # Add SQL keywords in both original and uppercase\n",
        "    for word in SQL_KEYWORDS:\n",
        "        allowed.add(word)\n",
        "        allowed.add(word.upper())\n",
        "\n",
        "    # Add identifiers from schema\n",
        "    allowed.update(extract_identifiers_from_schema(schema_text))\n",
        "\n",
        "    # Add digits\n",
        "    allowed.update(list(\"0123456789\"))\n",
        "\n",
        "    # Add special tokens (except PAD)\n",
        "    allowed.update([VocabTok.BOS, VocabTok.EOS, VocabTok.UNK])\n",
        "\n",
        "    # Create mask\n",
        "    mask = torch.zeros(len(tok.itos), dtype=torch.bool)\n",
        "\n",
        "    for i, token in enumerate(tok.itos):\n",
        "        if token == VocabTok.PAD:\n",
        "            mask[i] = False  # Never predict PAD\n",
        "        elif token in allowed or token.lower() in allowed:\n",
        "            mask[i] = True\n",
        "        else:\n",
        "            mask[i] = False\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Expand Mask with Gold Tokens (for Teacher Forcing)\n",
        "# ============================================================\n",
        "\n",
        "def expand_mask_with_gold(vocab_mask_batch, target_batch, pad_idx=0):\n",
        "    \"\"\"\n",
        "    Expand vocabulary mask to include all gold target tokens.\n",
        "    This ensures teacher forcing doesn't fail during training.\n",
        "\n",
        "    Args:\n",
        "        vocab_mask_batch: [B, V] boolean mask\n",
        "        target_batch: [B, T] target token IDs\n",
        "        pad_idx: Padding token ID\n",
        "\n",
        "    Returns:\n",
        "        Updated vocab_mask_batch with gold tokens allowed\n",
        "    \"\"\"\n",
        "    B, T = target_batch.size()\n",
        "\n",
        "    for b in range(B):\n",
        "        # Get unique non-padding gold tokens for this example\n",
        "        gold_ids = target_batch[b][target_batch[b] != pad_idx].unique()\n",
        "\n",
        "        # Allow all gold tokens\n",
        "        vocab_mask_batch[b, gold_ids] = True\n",
        "\n",
        "    return vocab_mask_batch\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Test Schema Masking\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Testing Schema Masking:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Get first test example\n",
        "test_example = test_rows[0]\n",
        "test_schema = test_example['schema_serialized']\n",
        "test_question = test_example['question']\n",
        "test_gold = test_example['gold_query']\n",
        "\n",
        "print(f\"\\nTest Example:\")\n",
        "print(f\"  Schema:\\n{test_schema[:200]}...\")\n",
        "print(f\"  Question: {test_question}\")\n",
        "print(f\"  Gold SQL: {test_gold}\")\n",
        "\n",
        "# Extract identifiers\n",
        "identifiers = extract_identifiers_from_schema(test_schema)\n",
        "print(f\"\\nExtracted Identifiers ({len(identifiers)} total):\")\n",
        "print(f\"  {sorted(list(identifiers))[:20]}...\")\n",
        "\n",
        "# Build mask\n",
        "mask = build_vocab_mask_for_example(vocab, test_schema)\n",
        "allowed_count = mask.sum().item()\n",
        "total_count = len(mask)\n",
        "\n",
        "print(f\"\\nVocabulary Mask:\")\n",
        "print(f\"  Total vocab size: {total_count}\")\n",
        "print(f\"  Allowed tokens:   {allowed_count}\")\n",
        "print(f\"  Blocked tokens:   {total_count - allowed_count}\")\n",
        "print(f\"  Allowed ratio:    {allowed_count / total_count:.2%}\")\n",
        "\n",
        "# Show some allowed tokens\n",
        "allowed_tokens = [vocab.itos[i] for i in range(len(vocab.itos)) if mask[i]]\n",
        "print(f\"\\nSample allowed tokens (first 30):\")\n",
        "print(f\"  {allowed_tokens[:30]}\")\n",
        "\n",
        "# Show some blocked tokens\n",
        "blocked_tokens = [vocab.itos[i] for i in range(len(vocab.itos)) if not mask[i]]\n",
        "print(f\"\\nSample blocked tokens (first 20):\")\n",
        "print(f\"  {blocked_tokens[:20]}\")\n",
        "\n",
        "print(\"\\n✅ Schema masking working correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6145f0de",
      "metadata": {
        "id": "6145f0de"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28026963",
      "metadata": {
        "id": "28026963"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "412535c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "412535c7",
        "outputId": "7fd8cb45-35d5-4fc9-c490-cf4344e54a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "6.1: Luong Attention Mechanism\n",
            "------------------------------------------------------------\n",
            "✅ LuongAttention defined\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n6.1: Luong Attention Mechanism\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "class LuongAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Luong (multiplicative) attention mechanism.\n",
        "    Computes attention weights over encoder outputs using decoder hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hid_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hid_dim: Hidden dimension size\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(hid_dim, hid_dim, bias=False)\n",
        "\n",
        "    def forward(self, dec_h, enc_out, mask=None):\n",
        "        \"\"\"\n",
        "        Compute attention context vector.\n",
        "\n",
        "        Args:\n",
        "            dec_h: Decoder hidden state [B, H]\n",
        "            enc_out: Encoder outputs [B, T, H]\n",
        "            mask: Attention mask [B, T] (True = attend, False = ignore)\n",
        "\n",
        "        Returns:\n",
        "            ctx: Context vector [B, H]\n",
        "            attn: Attention weights [B, T]\n",
        "        \"\"\"\n",
        "        # Compute attention scores: score[i] = enc_out[i] · W(dec_h)\n",
        "        # [B, T, H] @ [B, H, 1] -> [B, T, 1] -> [B, T]\n",
        "        score = torch.bmm(enc_out, self.W(dec_h).unsqueeze(2)).squeeze(2)\n",
        "\n",
        "        # Apply mask (set padded positions to -inf)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(~mask, -100)  # CHANGED: -1e9 -> -1e4\n",
        "\n",
        "        # Softmax to get attention weights\n",
        "        attn = torch.softmax(score, dim=1)  # [B, T]\n",
        "\n",
        "        # Compute context vector as weighted sum of encoder outputs\n",
        "        # [B, 1, T] @ [B, T, H] -> [B, 1, H] -> [B, H]\n",
        "        ctx = torch.bmm(attn.unsqueeze(1), enc_out).squeeze(1)\n",
        "\n",
        "        return ctx, attn\n",
        "\n",
        "print(\"✅ LuongAttention defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3f8962",
      "metadata": {
        "id": "7e3f8962"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7c1c46fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c1c46fc",
        "outputId": "df236ad6-cfda-4fc8-c060-b3a9bb038b96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "6.2: Bidirectional LSTM Encoder\n",
            "------------------------------------------------------------\n",
            "✅ Encoder defined\n",
            "   - Input: [B, T] token IDs\n",
            "   - Output: [B, T, 512] encoder outputs\n",
            "   - Hidden: [4, B, 256] per direction\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n6.2: Bidirectional LSTM Encoder\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM encoder.\n",
        "    Processes input sequence and returns hidden states.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, emb_dim=512, hid_dim=512,\n",
        "                 num_layers=2, pad_idx=0, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: Size of vocabulary\n",
        "            emb_dim: Embedding dimension\n",
        "            hid_dim: Hidden dimension (will be split for bidirectional)\n",
        "            num_layers: Number of LSTM layers\n",
        "            pad_idx: Padding token index\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # Bidirectional LSTM: each direction has hid_dim//2 units\n",
        "        self.rnn = nn.LSTM(\n",
        "            emb_dim,\n",
        "            hid_dim // 2,  # Each direction gets half the hidden dim\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        \"\"\"\n",
        "        Encode source sequence.\n",
        "\n",
        "        Args:\n",
        "            src: Source token IDs [B, T]\n",
        "\n",
        "        Returns:\n",
        "            out: Encoder outputs [B, T, H]\n",
        "            (h, c): Final hidden and cell states\n",
        "                    h: [2*num_layers, B, H//2]\n",
        "                    c: [2*num_layers, B, H//2]\n",
        "        \"\"\"\n",
        "        # Embed and apply dropout\n",
        "        x = self.dropout(self.emb(src))  # [B, T, E]\n",
        "\n",
        "        # Pass through BiLSTM\n",
        "        out, (h, c) = self.rnn(x)\n",
        "\n",
        "        return out, (h, c)\n",
        "\n",
        "print(\"✅ Encoder defined\")\n",
        "print(f\"   - Input: [B, T] token IDs\")\n",
        "print(f\"   - Output: [B, T, {HID_DIM}] encoder outputs\")\n",
        "print(f\"   - Hidden: [{2*NUM_LAYERS}, B, {HID_DIM//2}] per direction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61068b19",
      "metadata": {
        "id": "61068b19"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1f79c1e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f79c1e9",
        "outputId": "c01a6d98-bf62-487b-ffab-34b99834a4ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "6.3: LSTM Decoder with Attention\n",
            "------------------------------------------------------------\n",
            "✅ Decoder defined\n",
            "   - Forward: Teacher forcing for training\n",
            "   - Step: Greedy decoding for inference\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n6.3: LSTM Decoder with Attention\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM decoder with Luong attention.\n",
        "    Generates output sequence one token at a time.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, emb_dim=512, hid_dim=512,\n",
        "                 pad_idx=0, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: Size of vocabulary\n",
        "            emb_dim: Embedding dimension\n",
        "            hid_dim: Hidden dimension\n",
        "            pad_idx: Padding token index\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "\n",
        "        # LSTM input: concatenation of embedding + context vector\n",
        "        self.rnn = nn.LSTM(\n",
        "            emb_dim + hid_dim,  # Input: [embedding, context]\n",
        "            hid_dim,\n",
        "            num_layers=1,  # Single layer decoder\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.attn = LuongAttention(hid_dim)\n",
        "\n",
        "        # Output projection: [decoder_out, context] -> vocab logits\n",
        "        self.out = nn.Linear(hid_dim + hid_dim, vocab_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, trg, enc_out, hidden, enc_mask=None, vocab_mask=None):\n",
        "        \"\"\"\n",
        "        Teacher-forced forward pass (training).\n",
        "\n",
        "        Args:\n",
        "            trg: Target sequence [B, T] (includes BOS)\n",
        "            enc_out: Encoder outputs [B, T_src, H]\n",
        "            hidden: Initial decoder hidden state (h, c)\n",
        "            enc_mask: Encoder padding mask [B, T_src]\n",
        "            vocab_mask: Allowed vocabulary mask [B, V]\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits [B, T-1, V] (predicts trg[:,1:])\n",
        "        \"\"\"\n",
        "        B, T = trg.size()\n",
        "\n",
        "        # Embed target tokens\n",
        "        x = self.dropout(self.emb(trg))  # [B, T, E]\n",
        "\n",
        "        h, c = hidden\n",
        "        logits = []\n",
        "\n",
        "        # Decode one step at a time (teacher forcing)\n",
        "        for t in range(T - 1):  # Predict next token, so stop at T-1\n",
        "            # Compute attention using current hidden state\n",
        "            dec_h = h[-1]  # [B, H]\n",
        "            ctx, _ = self.attn(dec_h, enc_out, mask=enc_mask)  # [B, H]\n",
        "\n",
        "            # LSTM input: [current_embedding, context_vector]\n",
        "            rnn_in = torch.cat([x[:, t:t+1, :], ctx.unsqueeze(1)], dim=-1)  # [B, 1, E+H]\n",
        "\n",
        "            # LSTM step\n",
        "            o, (h, c) = self.rnn(rnn_in, (h, c))  # o: [B, 1, H]\n",
        "\n",
        "            # Compute output logits\n",
        "            logit = self.out(torch.cat([o.squeeze(1), ctx], dim=-1))  # [B, V]\n",
        "\n",
        "            # Apply vocabulary mask if provided\n",
        "            if vocab_mask is not None:\n",
        "                logit = logit.masked_fill(~vocab_mask, -100)  # CHANGED: -1e9 -> -1e4\n",
        "\n",
        "            logits.append(logit.unsqueeze(1))\n",
        "\n",
        "        return torch.cat(logits, dim=1)  # [B, T-1, V]\n",
        "\n",
        "    def step(self, y_prev, enc_out, hidden, enc_mask=None, vocab_mask=None):\n",
        "        \"\"\"\n",
        "        Single decoding step (inference).\n",
        "\n",
        "        Args:\n",
        "            y_prev: Previous token [B]\n",
        "            enc_out: Encoder outputs [B, T_src, H]\n",
        "            hidden: Previous hidden state (h, c)\n",
        "            enc_mask: Encoder padding mask [B, T_src]\n",
        "            vocab_mask: Allowed vocabulary mask [B, V]\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits [B, V]\n",
        "            next_id: Predicted token [B]\n",
        "            hidden: Updated hidden state (h, c)\n",
        "        \"\"\"\n",
        "        # Embed previous token\n",
        "        emb = self.dropout(self.emb(y_prev).unsqueeze(1))  # [B, 1, E]\n",
        "\n",
        "        # Compute attention\n",
        "        dec_h = hidden[0][-1]  # [B, H]\n",
        "        ctx, _ = self.attn(dec_h, enc_out, mask=enc_mask)  # [B, H]\n",
        "\n",
        "        # LSTM input\n",
        "        rnn_in = torch.cat([emb, ctx.unsqueeze(1)], dim=-1)  # [B, 1, E+H]\n",
        "\n",
        "        # LSTM step\n",
        "        o, hidden = self.rnn(rnn_in, hidden)  # [B, 1, H]\n",
        "\n",
        "        # Output logits\n",
        "        logits = self.out(torch.cat([o.squeeze(1), ctx], dim=-1))  # [B, V]\n",
        "\n",
        "        # Apply vocabulary mask\n",
        "        if vocab_mask is not None:\n",
        "            logits = logits.masked_fill(~vocab_mask, -100)  # CHANGED: -1e9 -> -1e4\n",
        "\n",
        "        # Greedy selection\n",
        "        next_id = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        return logits, next_id, hidden\n",
        "\n",
        "print(\"✅ Decoder defined\")\n",
        "print(f\"   - Forward: Teacher forcing for training\")\n",
        "print(f\"   - Step: Greedy decoding for inference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e54122b",
      "metadata": {
        "id": "4e54122b"
      },
      "source": [
        "### Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8d0dd40d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d0dd40d",
        "outputId": "3a56aea1-0945-48e6-b021-6d81efbe19ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "6.4: BiLSTM Seq2Seq Model\n",
            "------------------------------------------------------------\n",
            "✅ BiLSTMSeq2SQL defined\n",
            "   - Vocab size will be: 7468\n",
            "   - Embedding dim: 512\n",
            "   - Hidden dim: 512\n",
            "   - Encoder layers: 2\n",
            "\n",
            "------------------------------------------------------------\n",
            "Instantiating Model...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Model Statistics:\n",
            "  Total parameters:      22,393,132\n",
            "  Trainable parameters:  22,393,132\n",
            "  Model size (approx):   85.42 MB\n",
            "\n",
            "✅ Model architecture complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n6.4: BiLSTM Seq2Seq Model\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "class BiLSTMSeq2SQL(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Seq2Seq model for text-to-SQL generation.\n",
        "    Combines encoder, decoder, and handles state initialization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, pad_idx=0, emb_dim=512,\n",
        "                 hid_dim=512, num_layers=2, dropout=0.2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: Size of vocabulary\n",
        "            pad_idx: Padding token index\n",
        "            emb_dim: Embedding dimension\n",
        "            hid_dim: Hidden dimension\n",
        "            num_layers: Number of encoder LSTM layers\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            vocab_size, emb_dim, hid_dim,\n",
        "            num_layers=num_layers,\n",
        "            pad_idx=pad_idx,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            vocab_size, emb_dim, hid_dim,\n",
        "            pad_idx=pad_idx,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.pad_idx = pad_idx\n",
        "        self.hid_dim = hid_dim\n",
        "\n",
        "        # Projection layers to initialize decoder state from encoder\n",
        "        self.dec_init_h = nn.Linear(hid_dim, hid_dim)\n",
        "        self.dec_init_c = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "    def _init_decoder_state(self, enc_h, enc_c):\n",
        "        \"\"\"\n",
        "        Initialize decoder hidden state from encoder final state.\n",
        "        Combines forward and backward directions.\n",
        "\n",
        "        Args:\n",
        "            enc_h: Encoder hidden state [2*num_layers, B, H//2]\n",
        "            enc_c: Encoder cell state [2*num_layers, B, H//2]\n",
        "\n",
        "        Returns:\n",
        "            (h0, c0): Decoder initial state, each [1, B, H]\n",
        "        \"\"\"\n",
        "        # Take last layer, concat forward and backward\n",
        "        fwd_h = enc_h[-2]  # [B, H//2]\n",
        "        bwd_h = enc_h[-1]  # [B, H//2]\n",
        "        fwd_c = enc_c[-2]\n",
        "        bwd_c = enc_c[-1]\n",
        "\n",
        "        # Concatenate and project\n",
        "        h0 = torch.tanh(self.dec_init_h(torch.cat([fwd_h, bwd_h], dim=-1)))  # [B, H]\n",
        "        c0 = torch.tanh(self.dec_init_c(torch.cat([fwd_c, bwd_c], dim=-1)))  # [B, H]\n",
        "\n",
        "        return h0.unsqueeze(0), c0.unsqueeze(0)  # [1, B, H]\n",
        "\n",
        "    def forward(self, src, trg, vocab_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass (training with teacher forcing).\n",
        "\n",
        "        Args:\n",
        "            src: Source sequence [B, T_src]\n",
        "            trg: Target sequence [B, T_trg] (includes BOS)\n",
        "            vocab_mask: Vocabulary mask [B, V] or [V]\n",
        "\n",
        "        Returns:\n",
        "            logits: Output logits [B, T_trg-1, V]\n",
        "        \"\"\"\n",
        "        # Encode\n",
        "        enc_out, (h, c) = self.encoder(src)\n",
        "\n",
        "        # Create encoder padding mask for attention\n",
        "        enc_mask = (src != self.pad_idx)\n",
        "\n",
        "        # Initialize decoder state\n",
        "        h0, c0 = self._init_decoder_state(h, c)\n",
        "\n",
        "        # Expand vocab mask to batch size if needed\n",
        "        if vocab_mask is not None:\n",
        "            if vocab_mask.dim() == 1:\n",
        "                vocab_mask = vocab_mask.unsqueeze(0).expand(src.size(0), -1)\n",
        "            vocab_mask = vocab_mask.to(src.device)\n",
        "\n",
        "        # Decode\n",
        "        logits = self.decoder(trg, enc_out, (h0, c0),\n",
        "                             enc_mask=enc_mask, vocab_mask=vocab_mask)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def greedy_decode(self, src, bos_id, eos_id, max_len=128, vocab_mask=None):\n",
        "        \"\"\"\n",
        "        Greedy decoding for inference.\n",
        "\n",
        "        Args:\n",
        "            src: Source sequence [B, T_src]\n",
        "            bos_id: BOS token ID\n",
        "            eos_id: EOS token ID\n",
        "            max_len: Maximum generation length\n",
        "            vocab_mask: Vocabulary mask [B, V] or [V]\n",
        "\n",
        "        Returns:\n",
        "            Generated token IDs [B, T_gen]\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        # Encode\n",
        "        enc_out, (h, c) = self.encoder(src)\n",
        "        enc_mask = (src != self.pad_idx)\n",
        "\n",
        "        # Initialize decoder state\n",
        "        h0, c0 = self._init_decoder_state(h, c)\n",
        "\n",
        "        # Expand vocab mask\n",
        "        if vocab_mask is not None:\n",
        "            if vocab_mask.dim() == 1:\n",
        "                vocab_mask = vocab_mask.unsqueeze(0).expand(src.size(0), -1)\n",
        "            vocab_mask = vocab_mask.to(src.device)\n",
        "\n",
        "        # Start with BOS token\n",
        "        B = src.size(0)\n",
        "        y = torch.full((B,), bos_id, dtype=torch.long, device=src.device)\n",
        "\n",
        "        out_ids = []\n",
        "        hidden = (h0, c0)\n",
        "\n",
        "        # Generate tokens one by one\n",
        "        for _ in range(max_len):\n",
        "            _, y, hidden = self.decoder.step(\n",
        "                y, enc_out, hidden,\n",
        "                enc_mask=enc_mask,\n",
        "                vocab_mask=vocab_mask\n",
        "            )\n",
        "            out_ids.append(y.clone())\n",
        "\n",
        "            # Stop if all sequences generated EOS\n",
        "            if torch.all(y == eos_id):\n",
        "                break\n",
        "\n",
        "        return torch.stack(out_ids, dim=1) if out_ids else \\\n",
        "               torch.zeros(B, 0, dtype=torch.long, device=src.device)\n",
        "\n",
        "print(\"✅ BiLSTMSeq2SQL defined\")\n",
        "print(f\"   - Vocab size will be: {len(vocab)}\")\n",
        "print(f\"   - Embedding dim: {EMB_DIM}\")\n",
        "print(f\"   - Hidden dim: {HID_DIM}\")\n",
        "print(f\"   - Encoder layers: {NUM_LAYERS}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Instantiate Model\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Instantiating Model...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "model = BiLSTMSeq2SQL(\n",
        "    vocab_size=len(vocab),\n",
        "    pad_idx=0,\n",
        "    emb_dim=EMB_DIM,\n",
        "    hid_dim=HID_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT\n",
        ")\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  Total parameters:      {total_params:,}\")\n",
        "print(f\"  Trainable parameters:  {trainable_params:,}\")\n",
        "print(f\"  Model size (approx):   {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "print(\"\\n✅ Model architecture complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VGV3y0T5_SmX",
      "metadata": {
        "id": "VGV3y0T5_SmX"
      },
      "source": [
        "### Pretraining Diagnostic Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "m2dI8yIF_VHZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2dI8yIF_VHZ",
        "outputId": "941f4a72-7cd3-4140-d6dc-7951f49de658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "PRE-TRAINING DIAGNOSTIC\n",
            "============================================================\n",
            "Validation loader batches: 33\n",
            "Validation dataset size: 1034\n",
            "Val source shape: torch.Size([32, 113])\n",
            "Val target shape: torch.Size([32, 47])\n",
            "Val source max: 6762\n",
            "Val target max: 6764\n",
            "Val source has NaN: False\n",
            "Val target has NaN: False\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PRE-TRAINING DIAGNOSTIC\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check if validation loader is working\n",
        "print(f\"Validation loader batches: {len(val_loader)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "# Try one validation batch\n",
        "val_src, val_tgt, val_schemas = next(iter(val_loader))\n",
        "print(f\"Val source shape: {val_src.shape}\")\n",
        "print(f\"Val target shape: {val_tgt.shape}\")\n",
        "print(f\"Val source max: {val_src.max().item()}\")\n",
        "print(f\"Val target max: {val_tgt.max().item()}\")\n",
        "\n",
        "# Check for NaN or extreme values\n",
        "print(f\"Val source has NaN: {torch.isnan(val_src).any()}\")\n",
        "print(f\"Val target has NaN: {torch.isnan(val_tgt).any()}\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea2f2c22",
      "metadata": {
        "id": "ea2f2c22"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2cd5d209",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cd5d209",
        "outputId": "47376891-913b-40f9-b420-71324505edce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TRAINING THE MODEL\n",
            "============================================================\n",
            "\n",
            "Setting up training components...\n",
            "------------------------------------------------------------\n",
            "✅ Loss function: CrossEntropyLoss (ignore_index=0)\n",
            "✅ Optimizer: AdamW (lr=0.0001)\n",
            "✅ Scheduler: CosineAnnealingLR\n",
            "✅ BOS token ID: 2\n",
            "✅ EOS token ID: 3\n",
            "\n",
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n",
            "  First batch loss: 5.0786\n",
            "  First batch loss: 11.2567\n",
            "\n",
            "[Epoch 01/15]\n",
            "  Train Loss: 2.2665\n",
            "  Val Loss:   19.2808\n",
            "  LR:         9.89e-05\n",
            "  Time:       6.44 min\n",
            "  Preview SQL: SELECT count ( * ) FROM stadium ; <eos>\n",
            "  ✅ Saved checkpoint (best val loss: 19.2808)\n",
            "  First batch loss: 1.2082\n",
            "  First batch loss: 11.0046\n",
            "\n",
            "[Epoch 02/15]\n",
            "  Train Loss: 1.0243\n",
            "  Val Loss:   19.1156\n",
            "  LR:         9.57e-05\n",
            "  Time:       6.42 min\n",
            "  Preview SQL: SELECT count ( * ) FROM stadium ; <eos>\n",
            "  ✅ Saved checkpoint (best val loss: 19.1156)\n",
            "  First batch loss: 0.7941\n",
            "  First batch loss: 10.9505\n",
            "\n",
            "[Epoch 03/15]\n",
            "  Train Loss: 0.8213\n",
            "  Val Loss:   19.1011\n",
            "  LR:         9.05e-05\n",
            "  Time:       6.43 min\n",
            "  Preview SQL: SELECT song_name FROM stadium ORDER BY Age DESC LIMIT 1 ; <eos>\n",
            "  ✅ Saved checkpoint (best val loss: 19.1011)\n",
            "  First batch loss: 0.9622\n",
            "  First batch loss: 10.9471\n",
            "\n",
            "[Epoch 04/15]\n",
            "  Train Loss: 0.7248\n",
            "  Val Loss:   19.1067\n",
            "  LR:         8.35e-05\n",
            "  Time:       6.48 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE YEAR = ( SELECT MAX ( age ) FROM stadium ) ; <eos>\n",
            "  First batch loss: 0.7170\n",
            "  First batch loss: 10.9521\n",
            "\n",
            "[Epoch 05/15]\n",
            "  Train Loss: 0.6618\n",
            "  Val Loss:   19.1203\n",
            "  LR:         7.50e-05\n",
            "  Time:       6.54 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE capacity > ( SELECT avg ( capacity ) FROM stadium ) ; <eos>\n",
            "  First batch loss: 0.6641\n",
            "  First batch loss: 10.9427\n",
            "\n",
            "[Epoch 06/15]\n",
            "  Train Loss: 0.6153\n",
            "  Val Loss:   19.1427\n",
            "  LR:         6.55e-05\n",
            "  Time:       6.53 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE YEAR = ( SELECT max ( capacity ) FROM stadium ) ; <eos>\n",
            "  First batch loss: 0.5740\n",
            "  First batch loss: 11.0204\n",
            "\n",
            "[Epoch 07/15]\n",
            "  Train Loss: 0.5804\n",
            "  Val Loss:   19.1423\n",
            "  LR:         5.52e-05\n",
            "  Time:       6.55 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE capacity > ( SELECT avg ( capacity ) FROM stadium ) ; <eos>\n",
            "  First batch loss: 0.5387\n",
            "  First batch loss: 10.9879\n",
            "\n",
            "[Epoch 08/15]\n",
            "  Train Loss: 0.5512\n",
            "  Val Loss:   19.1526\n",
            "  LR:         4.48e-05\n",
            "  Time:       6.49 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE capacity > ( SELECT avg ( capacity ) FROM stadium ) ; <eos>\n",
            "  First batch loss: 0.5101\n",
            "  First batch loss: 11.0197\n",
            "\n",
            "[Epoch 09/15]\n",
            "  Train Loss: 0.5266\n",
            "  Val Loss:   19.1435\n",
            "  LR:         3.45e-05\n",
            "  Time:       6.47 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE YEAR = ( SELECT max ( capacity ) FROM stadium ) ; <eos>\n",
            "  First batch loss: 0.6041\n",
            "  First batch loss: 11.0303\n",
            "\n",
            "[Epoch 10/15]\n",
            "  Train Loss: 0.5063\n",
            "  Val Loss:   19.1520\n",
            "  LR:         2.50e-05\n",
            "  Time:       6.51 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE capacity > ( SELECT avg ( capacity ) FROM stadium ) ; <eos>\n",
            "  First batch loss: 0.4246\n",
            "  First batch loss: 10.9969\n",
            "\n",
            "[Epoch 11/15]\n",
            "  Train Loss: 0.4920\n",
            "  Val Loss:   19.1503\n",
            "  LR:         1.65e-05\n",
            "  Time:       6.47 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE capacity > ( SELECT avg ( capacity ) FROM stadium ) ; <eos>\n",
            "  First batch loss: 0.4159\n",
            "  First batch loss: 11.0025\n",
            "\n",
            "[Epoch 12/15]\n",
            "  Train Loss: 0.4818\n",
            "  Val Loss:   19.1574\n",
            "  LR:         9.55e-06\n",
            "  Time:       6.47 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE capacity > ( SELECT avg ( capacity ) FROM stadium ) ; <eos>\n",
            "  First batch loss: 0.4712\n",
            "  First batch loss: 10.9946\n",
            "\n",
            "[Epoch 13/15]\n",
            "  Train Loss: 0.4739\n",
            "  Val Loss:   19.1639\n",
            "  LR:         4.32e-06\n",
            "  Time:       6.44 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE capacity > ( SELECT avg ( capacity ) FROM stadium ) ; <eos>\n",
            "  First batch loss: 0.5591\n",
            "  First batch loss: 11.0015\n",
            "\n",
            "[Epoch 14/15]\n",
            "  Train Loss: 0.4697\n",
            "  Val Loss:   19.1657\n",
            "  LR:         1.09e-06\n",
            "  Time:       6.43 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE capacity > ( SELECT avg ( capacity ) FROM stadium ) ; <eos>\n",
            "  First batch loss: 0.4966\n",
            "  First batch loss: 11.0012\n",
            "\n",
            "[Epoch 15/15]\n",
            "  Train Loss: 0.4665\n",
            "  Val Loss:   19.1648\n",
            "  LR:         0.00e+00\n",
            "  Time:       6.42 min\n",
            "  Preview SQL: SELECT song_name FROM stadium WHERE capacity > ( SELECT avg ( capacity ) FROM stadium ) ; <eos>\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Best validation loss: 19.1011\n",
            "Model saved to: saved_model/bilstm_model.pt\n",
            "Vocab saved to: saved_model/bilstm_vocab.json\n",
            "Training history saved to: saved_model/training_history.json\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"TRAINING THE MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Setup Training Components\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nSetting up training components...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Loss function (ignore padding)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler (cosine annealing)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=EPOCHS\n",
        ")\n",
        "\n",
        "# Special token IDs\n",
        "BOS_ID = vocab.stoi[VocabTok.BOS]\n",
        "EOS_ID = vocab.stoi[VocabTok.EOS]\n",
        "\n",
        "print(f\"✅ Loss function: CrossEntropyLoss (ignore_index=0)\")\n",
        "print(f\"✅ Optimizer: AdamW (lr={LEARNING_RATE})\")\n",
        "print(f\"✅ Scheduler: CosineAnnealingLR\")\n",
        "print(f\"✅ BOS token ID: {BOS_ID}\")\n",
        "print(f\"✅ EOS token ID: {EOS_ID}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Training Function\n",
        "# ============================================================\n",
        "\n",
        "def run_epoch(data_loader, train_mode=True):\n",
        "    \"\"\"\n",
        "    Run one epoch of training or validation.\n",
        "\n",
        "    Args:\n",
        "        data_loader: DataLoader to iterate over\n",
        "        train_mode: If True, update weights. If False, just evaluate.\n",
        "\n",
        "    Returns:\n",
        "        Average loss for the epoch\n",
        "    \"\"\"\n",
        "    model.train(train_mode)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for batch_idx, (src, tgt, schemas) in enumerate(data_loader):\n",
        "        # Move to device\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        # Build vocabulary mask for each example in batch\n",
        "        vocab_masks = []\n",
        "        for schema in schemas:\n",
        "            mask = build_vocab_mask_for_example(vocab, schema)\n",
        "            vocab_masks.append(mask)\n",
        "\n",
        "        vocab_mask_batch = torch.stack(vocab_masks, dim=0).to(DEVICE)  # [B, V]\n",
        "\n",
        "        # During training, expand mask to include gold tokens (for teacher forcing)\n",
        "        if train_mode:\n",
        "            vocab_mask_batch = expand_mask_with_gold(vocab_mask_batch, tgt, pad_idx=0)\n",
        "\n",
        "        # Forward pass\n",
        "        # Model returns logits aligned with tgt[:,1:] (predict next token)\n",
        "        logits = model(src, tgt, vocab_mask=vocab_mask_batch)  # [B, T-1, V]\n",
        "\n",
        "        # Compute loss\n",
        "        # Target: tgt[:,1:] (everything after BOS)\n",
        "        gold = tgt[:, 1:].contiguous().view(-1)  # [B*(T-1)]\n",
        "        loss = criterion(logits.view(-1, len(vocab)), gold)\n",
        "\n",
        "        # Check for NaN loss\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(f\"WARNING: NaN/Inf loss detected at batch {batch_idx}\")\n",
        "            continue\n",
        "\n",
        "        # Backward pass and optimization (only in train mode)\n",
        "        if train_mode:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Print progress for very first batch\n",
        "        if batch_idx == 0 and num_batches == 1:\n",
        "            print(f\"  First batch loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / max(1, num_batches)\n",
        "\n",
        "    # Sanity check\n",
        "    if avg_loss > 1000:\n",
        "        print(f\"  WARNING: Unusually high average loss: {avg_loss:.2f}\")\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "# ============================================================\n",
        "# Training Loop\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "training_history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'learning_rates': []\n",
        "}\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    # Train\n",
        "    train_loss = run_epoch(train_loader, train_mode=True)\n",
        "\n",
        "    # Validate\n",
        "    val_loss = run_epoch(val_loader, train_mode=False)\n",
        "\n",
        "    # Step scheduler\n",
        "    scheduler.step()\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    # Record history\n",
        "    training_history['train_loss'].append(train_loss)\n",
        "    training_history['val_loss'].append(val_loss)\n",
        "    training_history['learning_rates'].append(current_lr)\n",
        "\n",
        "    epoch_time = (time.time() - epoch_start_time) / 60\n",
        "\n",
        "    # ============================================================\n",
        "    # Preview Generation\n",
        "    # ============================================================\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get one batch from validation set for preview\n",
        "        preview_src, preview_tgt, preview_schemas = next(iter(val_loader))\n",
        "        preview_src = preview_src[:1].to(DEVICE)  # Take first example\n",
        "        preview_schema = preview_schemas[0]\n",
        "\n",
        "        # Build vocab mask\n",
        "        preview_mask = build_vocab_mask_for_example(vocab, preview_schema)\n",
        "        preview_mask = preview_mask.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        # Generate\n",
        "        generated_ids = model.greedy_decode(\n",
        "            preview_src,\n",
        "            BOS_ID,\n",
        "            EOS_ID,\n",
        "            max_len=MAX_DECODE_PREVIEW,\n",
        "            vocab_mask=preview_mask\n",
        "        )\n",
        "\n",
        "        # Decode\n",
        "        if generated_ids.numel() > 0:\n",
        "            pred_sql = vocab.decode(generated_ids[0].tolist())\n",
        "        else:\n",
        "            pred_sql = \"<empty>\"\n",
        "\n",
        "    # ============================================================\n",
        "    # Logging\n",
        "    # ============================================================\n",
        "\n",
        "    print(f\"\\n[Epoch {epoch:02d}/{EPOCHS}]\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
        "    print(f\"  LR:         {current_lr:.2e}\")\n",
        "    print(f\"  Time:       {epoch_time:.2f} min\")\n",
        "    print(f\"  Preview SQL: {pred_sql}\")\n",
        "\n",
        "    # ============================================================\n",
        "    # Save Best Model\n",
        "    # ============================================================\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "\n",
        "        # Save model checkpoint\n",
        "        checkpoint = {\n",
        "            'model': model.state_dict(),\n",
        "            'vocab': vocab.to_json(),\n",
        "            'epoch': epoch,\n",
        "            'val_loss': val_loss,\n",
        "            'train_loss': train_loss,\n",
        "            'config': {\n",
        "                'vocab_size': len(vocab),\n",
        "                'emb_dim': EMB_DIM,\n",
        "                'hid_dim': HID_DIM,\n",
        "                'num_layers': NUM_LAYERS,\n",
        "                'dropout': DROPOUT\n",
        "            }\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, SAVE_DIR / \"bilstm_model.pt\")\n",
        "\n",
        "        # Also save vocab separately for easy loading\n",
        "        with open(SAVE_DIR / \"bilstm_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(vocab.to_json(), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"  ✅ Saved checkpoint (best val loss: {best_val_loss:.4f})\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Training Complete\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nBest validation loss: {best_val_loss:.4f}\")\n",
        "print(f\"Model saved to: {SAVE_DIR / 'bilstm_model.pt'}\")\n",
        "print(f\"Vocab saved to: {SAVE_DIR / 'bilstm_vocab.json'}\")\n",
        "\n",
        "# Save training history\n",
        "history_path = SAVE_DIR / \"training_history.json\"\n",
        "with open(history_path, \"w\") as f:\n",
        "    json.dump(training_history, f, indent=2)\n",
        "\n",
        "print(f\"Training history saved to: {history_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08950994",
      "metadata": {
        "id": "08950994"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9190f36f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9190f36f",
        "outputId": "5936a63e-d2f2-46a6-daf7-6a228df5484f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EVALUATION ON HOSPITAL_1 TEST SET\n",
            "============================================================\n",
            "\n",
            "Setting up evaluation utilities...\n",
            "------------------------------------------------------------\n",
            "✅ SQL utilities defined\n",
            "\n",
            "Loading trained model...\n",
            "------------------------------------------------------------\n",
            "✅ Loaded checkpoint from epoch 3\n",
            "   Train loss: 0.8213\n",
            "   Val loss:   19.1011\n",
            "✅ Model loaded and ready for evaluation\n",
            "\n",
            "Connecting to database...\n",
            "------------------------------------------------------------\n",
            "✅ Connected to: hospital_1.sqlite\n",
            "\n",
            "============================================================\n",
            "RUNNING EVALUATION\n",
            "============================================================\n",
            "\n",
            "Evaluating on 100 examples from hospital_1...\n",
            "------------------------------------------------------------\n",
            "[10/100] EM=0.000 EX=0.000 Valid=0.000\n",
            "[20/100] EM=0.000 EX=0.000 Valid=0.000\n",
            "[30/100] EM=0.000 EX=0.000 Valid=0.000\n",
            "[40/100] EM=0.000 EX=0.000 Valid=0.000\n",
            "[50/100] EM=0.000 EX=0.000 Valid=0.000\n",
            "[60/100] EM=0.000 EX=0.000 Valid=0.000\n",
            "[70/100] EM=0.000 EX=0.000 Valid=0.000\n",
            "[80/100] EM=0.000 EX=0.000 Valid=0.000\n",
            "[90/100] EM=0.000 EX=0.000 Valid=0.000\n",
            "[100/100] EM=0.000 EX=0.000 Valid=0.000\n",
            "\n",
            "Saving results...\n",
            "------------------------------------------------------------\n",
            "✅ Results saved to: results_hospital_1_bilstm.csv\n",
            "\n",
            "============================================================\n",
            "EVALUATION SUMMARY\n",
            "============================================================\n",
            "\n",
            "Model: BiLSTM Encoder-Decoder\n",
            "Test Dataset: hospital_1\n",
            "Examples: 100\n",
            "\n",
            "Metrics:\n",
            "  Exact Match (EM):        0.000%\n",
            "  Execution Accuracy (EX): 0.000%\n",
            "  Valid-SQL rate:          0.000%\n",
            "\n",
            "Performance:\n",
            "  Median generation time:  23.9 ms\n",
            "\n",
            "Results saved to: results_hospital_1_bilstm.csv\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Section 8 Complete!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Section 8: Evaluation on hospital_1 Test Set\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATION ON HOSPITAL_1 TEST SET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# SQL Utilities\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nSetting up evaluation utilities...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "def canonical_sql(sql_text):\n",
        "    \"\"\"\n",
        "    Normalize SQL to canonical form using sqlglot.\n",
        "\n",
        "    Args:\n",
        "        sql_text: SQL query string\n",
        "\n",
        "    Returns:\n",
        "        Normalized SQL string, or None if parsing fails\n",
        "    \"\"\"\n",
        "    if not sql_text:\n",
        "        return None\n",
        "    try:\n",
        "        ast = parse_one(sql_text, read=\"sqlite\")\n",
        "        return ast.sql(dialect=\"sqlite\", pretty=False)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def try_execute(conn, sql_text):\n",
        "    \"\"\"\n",
        "    Execute SQL query and return result set.\n",
        "\n",
        "    Args:\n",
        "        conn: SQLite connection\n",
        "        sql_text: SQL query string\n",
        "\n",
        "    Returns:\n",
        "        (result_set, error):\n",
        "            - result_set: Set of tuples (rows), or None if error\n",
        "            - error: Error message string, or None if success\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cur = conn.execute(sql_text)\n",
        "        rows = cur.fetchall()\n",
        "\n",
        "        # Normalize floats to 6 decimal places\n",
        "        normalized = []\n",
        "        for row in rows:\n",
        "            norm_row = []\n",
        "            for val in row:\n",
        "                if isinstance(val, float):\n",
        "                    norm_row.append(round(val, 6))\n",
        "                else:\n",
        "                    norm_row.append(val)\n",
        "            normalized.append(tuple(norm_row))\n",
        "\n",
        "        return set(normalized), None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "\n",
        "def extract_sql_from_tokens(tok, token_ids):\n",
        "    \"\"\"\n",
        "    Convert token IDs back to SQL string.\n",
        "\n",
        "    Args:\n",
        "        tok: VocabTok instance\n",
        "        token_ids: List of token IDs\n",
        "\n",
        "    Returns:\n",
        "        SQL string\n",
        "    \"\"\"\n",
        "    # Decode tokens\n",
        "    text = \" \".join(tok.itos[i] for i in token_ids if i < len(tok.itos))\n",
        "    text = text.strip()\n",
        "\n",
        "    # Clean up special tokens\n",
        "    text = text.replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"<pad>\", \"\")\n",
        "    text = text.strip()\n",
        "\n",
        "    # Ensure semicolon at end\n",
        "    if \";\" in text:\n",
        "        text = text.split(\";\", 1)[0] + \";\"\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "print(\"✅ SQL utilities defined\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Load Trained Model\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nLoading trained model...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint_path = SAVE_DIR / \"bilstm_model.pt\"\n",
        "vocab_path = SAVE_DIR / \"bilstm_vocab.json\"\n",
        "\n",
        "if not checkpoint_path.exists():\n",
        "    print(f\"❌ ERROR: Model checkpoint not found at {checkpoint_path}\")\n",
        "    print(\"Please complete training first!\")\n",
        "else:\n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "\n",
        "    # Extract config\n",
        "    config = checkpoint.get('config', {\n",
        "        'vocab_size': len(vocab),\n",
        "        'emb_dim': EMB_DIM,\n",
        "        'hid_dim': HID_DIM,\n",
        "        'num_layers': NUM_LAYERS,\n",
        "        'dropout': DROPOUT\n",
        "    })\n",
        "\n",
        "    print(f\"✅ Loaded checkpoint from epoch {checkpoint.get('epoch', '?')}\")\n",
        "    print(f\"   Train loss: {checkpoint.get('train_loss', 0):.4f}\")\n",
        "    print(f\"   Val loss:   {checkpoint.get('val_loss', 0):.4f}\")\n",
        "\n",
        "    # Recreate model with saved config\n",
        "    eval_model = BiLSTMSeq2SQL(\n",
        "        vocab_size=config['vocab_size'],\n",
        "        pad_idx=0,\n",
        "        emb_dim=config['emb_dim'],\n",
        "        hid_dim=config['hid_dim'],\n",
        "        num_layers=config['num_layers'],\n",
        "        dropout=config['dropout']\n",
        "    )\n",
        "\n",
        "    # Load weights\n",
        "    eval_model.load_state_dict(checkpoint['model'])\n",
        "    eval_model = eval_model.to(DEVICE)\n",
        "    eval_model.eval()\n",
        "\n",
        "    print(f\"✅ Model loaded and ready for evaluation\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Connect to hospital_1 Database\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nConnecting to database...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "conn = sqlite3.connect(str(SQLITE_DB))\n",
        "conn.execute(\"PRAGMA foreign_keys=ON\")\n",
        "\n",
        "print(f\"✅ Connected to: {SQLITE_DB}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Evaluation Loop\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RUNNING EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = []\n",
        "n_examples = len(test_dataset)\n",
        "\n",
        "em_count = 0\n",
        "ex_count = 0\n",
        "valid_count = 0\n",
        "latencies = []\n",
        "\n",
        "print(f\"\\nEvaluating on {n_examples} examples from hospital_1...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for i, (src, tgt, schemas) in enumerate(test_loader, 1):  # CHANGED: schema -> schemas\n",
        "    # Move to device\n",
        "    src = src.to(DEVICE)\n",
        "\n",
        "    # Get example data\n",
        "    test_ex = test_rows[i - 1]\n",
        "    question = test_ex['question']\n",
        "    gold_sql = test_ex['gold_query']\n",
        "\n",
        "    # Extract schema string from list (ADDED)\n",
        "    schema = schemas[0] if isinstance(schemas, (list, tuple)) else schemas\n",
        "\n",
        "    # Build vocabulary mask for this example\n",
        "    vocab_mask = build_vocab_mask_for_example(vocab, schema)\n",
        "    vocab_mask = vocab_mask.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # Generate SQL\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = eval_model.greedy_decode(\n",
        "            src,\n",
        "            BOS_ID,\n",
        "            EOS_ID,\n",
        "            max_len=MAX_DECODE_LEN,\n",
        "            vocab_mask=vocab_mask\n",
        "        )\n",
        "\n",
        "    gen_time_ms = (time.time() - start_time) * 1000.0\n",
        "    latencies.append(gen_time_ms)\n",
        "\n",
        "    # Extract SQL from generated tokens\n",
        "    if generated_ids.numel() > 0:\n",
        "        pred_ids = generated_ids[0].tolist()\n",
        "        pred_sql_raw = extract_sql_from_tokens(vocab, pred_ids)\n",
        "    else:\n",
        "        pred_sql_raw = \"\"\n",
        "\n",
        "    # Normalize both predicted and gold SQL\n",
        "    pred_sql_norm = canonical_sql(pred_sql_raw)\n",
        "    gold_sql_norm = canonical_sql(gold_sql)\n",
        "\n",
        "    # ============================================================\n",
        "    # Compute Metrics\n",
        "    # ============================================================\n",
        "\n",
        "    # Exact Match (EM)\n",
        "    em = int(\n",
        "        pred_sql_norm is not None and\n",
        "        gold_sql_norm is not None and\n",
        "        pred_sql_norm == gold_sql_norm\n",
        "    )\n",
        "\n",
        "    # Execution Accuracy (EX) and Valid SQL\n",
        "    valid = 0\n",
        "    ex_ok = 0\n",
        "    error = None\n",
        "\n",
        "    if pred_sql_norm is not None:\n",
        "        # Try to execute predicted SQL\n",
        "        pred_rows, error = try_execute(conn, pred_sql_norm)\n",
        "\n",
        "        if pred_rows is not None:\n",
        "            valid = 1  # SQL is valid (executed without error)\n",
        "\n",
        "            # Execute gold SQL\n",
        "            gold_rows, gold_error = try_execute(conn, gold_sql_norm or gold_sql)\n",
        "\n",
        "            if gold_rows is not None:\n",
        "                # Compare result sets\n",
        "                ex_ok = int(pred_rows == gold_rows)\n",
        "            else:\n",
        "                error = f\"Gold SQL failed: {gold_error}\"\n",
        "    else:\n",
        "        error = \"ParseError: Could not parse predicted SQL\"\n",
        "\n",
        "    # Update counters\n",
        "    em_count += em\n",
        "    ex_count += ex_ok\n",
        "    valid_count += valid\n",
        "\n",
        "    # Store result\n",
        "    results.append({\n",
        "        \"id\": test_ex[\"id\"],\n",
        "        \"question\": question,\n",
        "        \"gold_sql\": gold_sql,\n",
        "        \"pred_sql_raw\": pred_sql_raw,\n",
        "        \"pred_sql_norm\": pred_sql_norm or \"\",\n",
        "        \"em\": em,\n",
        "        \"ex\": ex_ok,\n",
        "        \"valid_sql\": valid,\n",
        "        \"latency_ms\": round(gen_time_ms, 2),\n",
        "        \"error\": error or \"\"\n",
        "    })\n",
        "\n",
        "    # Progress update\n",
        "    if i % 10 == 0 or i == n_examples:\n",
        "        print(f\"[{i}/{n_examples}] EM={em_count/i:.3f} EX={ex_count/i:.3f} Valid={valid_count/i:.3f}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Save Results\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nSaving results...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "with open(RESULTS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    if results:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(results[0].keys()))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "print(f\"✅ Results saved to: {RESULTS_CSV}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Summary Statistics\n",
        "# ============================================================\n",
        "\n",
        "em_rate = em_count / n_examples\n",
        "ex_rate = ex_count / n_examples\n",
        "valid_rate = valid_count / n_examples\n",
        "median_latency = sorted(latencies)[len(latencies) // 2] if latencies else 0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nModel: BiLSTM Encoder-Decoder\")\n",
        "print(f\"Test Dataset: hospital_1\")\n",
        "print(f\"Examples: {n_examples}\")\n",
        "print(f\"\\nMetrics:\")\n",
        "print(f\"  Exact Match (EM):        {em_rate:.3%}\")\n",
        "print(f\"  Execution Accuracy (EX): {ex_rate:.3%}\")\n",
        "print(f\"  Valid-SQL rate:          {valid_rate:.3%}\")\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Median generation time:  {median_latency:.1f} ms\")\n",
        "print(f\"\\nResults saved to: {RESULTS_CSV}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Close database connection\n",
        "conn.close()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Section 8 Complete!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340bc52f",
      "metadata": {
        "id": "340bc52f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
