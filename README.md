
 
# Text2SQL – Natural Language to SQL Converter

    This project enables users to convert natural language queries into executable SQL commands using deep learning and transformer-based models.
    It includes both a backend (Flask-based) and a frontend (Streamlit interface) for smooth interaction.


---
## Overview  

This system enables users to enter natural language questions — e.g., _“List all patients admitted in 2023”_ — and automatically generates SQL queries that can be executed on structured databases.

## Repository Structure  

```bash
SWM-Project/
│
├── CODE/
│   ├── frontend.py                    
│   ├── patch_sqlnet_py3.py              
│   ├── training_dataset_builder.ipynb   
│   ├── testing_visualizations.ipynb     
│   ├── testing_data_builder_for_visualization.ipynb
│   └── Model Testing/                   
│        ├── Arctic_Baseline_Testing.ipynb
│        ├── Enc_Dec_BiLSTM.ipynb
│        ├── Demo_FlanT5.ipynb
│        ├── FlanT5_Fine_Tuned_main.ipynb
│        ├── Flan_T5_Fine_Tuned_Using_Prompting_Strategies
│        └── FlanT5_Baseline_Testing_on_Hospital_1.ipynb
│
├── DATA/
│   ├── train_text2sql.jsonl
│   ├── val_text2sql.jsonl
│   ├── test_hospital_1.jsonl
│   ├── eval_hospital_1.jsonl
│   ├── Data_for_demo.jsonl
│   └── spider_dataset_link.txt
│
├── EVALUATION/
│   ├── results_hospital_1_flan.csv
│   ├── test_visualizations/
│   └── evaluation_results/
│
├── requirements.txt
├── env.txt
└── README.md
```



## The pipeline supports:

* Dataset loading & schema extraction
* Normalization and prompt creation
* BiLSTM baseline and Flan-T5 transformer models
* Interactive demo (Streamlit frontend)
* Evaluation metrics & visualizations



## Getting Started

Follow these steps to clone the repository, set up your environment, and run the Streamlit application.

### 1. Clone the Repository

```bash
git clone AtharvaBOT7/SWM-Project
cd SWM-Project-main
```

---

### 2. Create a Virtual Environment

Create a Python virtual environment (Python 3.10 is recommended):

```bash
python3.10 -m venv venv
```

Activate the environment:

**For macOS/Linux:**

```bash
source venv/bin/activate
```

**For Windows (PowerShell):**

```bash
venv\Scripts\activate
```

---

### 3. Verify Python Version

Check that the correct Python version from your virtual environment is active:

```bash
which python
```

It should point to the path inside your `venv/` directory.

---

### 4. Install Dependencies

Install all required packages using `requirements.txt`:

```bash
pip install -r requirements.txt
```

---

### 5. Run the Application

Start the Streamlit frontend:

```bash
streamlit run frontend.py
```

This launches a browser UI where you can:

1. Select / configure the target database.

2. Type a natural-language question.
3. View the generated SQL query

---
### 6. Code Structure Details
#### Data & Preprocessing

* Spider dataset (Yale LILY Lab) used as the primary benchmark.
* DATA/train_text2sql.jsonl, val_text2sql.jsonl, test_hospital_1.jsonl built via CODE/training_dataset_builder.ipynb.
* Schema extraction from SQLite databases and Spider’s tables.json.
* Normalization of questions and SQL, plus schema-aware prompt construction.

#### Models
* Enc_Dec_BiLSTM.ipynb - Classical encoder–decoder baseline with attention, used as a lower-bound reference.
* FlanT5_Fine_Tuned_main.ipynb - Fine-tunes Flan-T5 on the prepared Spider-style Text2SQL dataset, excluding hospital_1 for cross-schema testing.
* Arctic_Baseline_Testing.ipynb - Experiments with Arctic as a state-of-the-art comparison model.
* FlanT5_Baseline_Testing_on_Hospital_1.ipynb - Evaluates base vs fine-tuned Flan-T5 on the held-out clinical schema.

#### Evaluation
All evaluation artifacts live under EVALUATION:
  * results_hospital_1_flan.csv – per-example results on hospital_1.
  * evaluation_results/ – EM / EX / Valid metrics.
  * test_visualizations/ – bar plots for each metric across databases.
  * These are generated by CODE/testing_visualizations.ipynb

---
### 7. Evaluation Summary
Key metrics used:
 * Exact Match (EM): % of predictions whose SQL string exactly matches the gold query.
 * Execution Accuracy (EX): % where predicted and gold SQL return identical result sets.
 * Valid SQL (% Valid): % of generated SQL queries that execute without syntax/runtime errors.

---
### 8.Tech Stack
* **Core**: Python, PyTorch, HuggingFace Transformers<br>
* **Data**: Spider dataset, SQLite<br>
* **Frontend**: Streamlit (CODE/frontend.py)<br>
* **Visualization**: Matplotlib, Pandas, Jupyter Notebooks

---

<p align="center"><strong>Contributors</strong></p>
<p align="center">Khushi Agrawal | Jay Vishalbhai Aslaliya | Atharva Chundurwar | Vaibhavi Kundle | Crehan Santhumayor</p>
