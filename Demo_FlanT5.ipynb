{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e099d9bc318b4c06b54c283da395272a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a6e3d6eec3d24615b5bf799b6d3c8597",
              "IPY_MODEL_02a322002889480abf4012ea702fd6c7",
              "IPY_MODEL_98407ce1ecf94269a6b23e00e37fd213"
            ],
            "layout": "IPY_MODEL_f822923ce3a149cfae3b82e24f6f2be4"
          }
        },
        "a6e3d6eec3d24615b5bf799b6d3c8597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35461a19418d4593a12db19dd29eb3e7",
            "placeholder": "​",
            "style": "IPY_MODEL_435bc83c076d4365ab0f9f409440bdd3",
            "value": "Tokenizing training data: 100%"
          }
        },
        "02a322002889480abf4012ea702fd6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf53c32c8f1c4761ae9da8744a7925a3",
            "max": 365,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43a8f4befb164d9989281c0ec47900b9",
            "value": 365
          }
        },
        "98407ce1ecf94269a6b23e00e37fd213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87cdebe119bb4ec6a606d2646f768e55",
            "placeholder": "​",
            "style": "IPY_MODEL_613a45c9058e4559b28070cd023c92a1",
            "value": " 365/365 [00:00&lt;00:00, 2845.14 examples/s]"
          }
        },
        "f822923ce3a149cfae3b82e24f6f2be4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35461a19418d4593a12db19dd29eb3e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "435bc83c076d4365ab0f9f409440bdd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf53c32c8f1c4761ae9da8744a7925a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a8f4befb164d9989281c0ec47900b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "87cdebe119bb4ec6a606d2646f768e55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "613a45c9058e4559b28070cd023c92a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0ccaf6464494f86bede8b84c514c704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de41e857036d411a94428c15b44b6d6b",
              "IPY_MODEL_1c2ddb7957964d1c84b91b4430efd642",
              "IPY_MODEL_244dcad2b16d408495293ea9f3c80372"
            ],
            "layout": "IPY_MODEL_782ee92288ef44f09e7d167244102a03"
          }
        },
        "de41e857036d411a94428c15b44b6d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97531e13c8374038bb22479aa3007557",
            "placeholder": "​",
            "style": "IPY_MODEL_d5b21c3fa9384ff4842be906621ea4d5",
            "value": "Tokenizing validation data: 100%"
          }
        },
        "1c2ddb7957964d1c84b91b4430efd642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_752efd7101a34aa7ba08879ce98df93a",
            "max": 41,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dbe17ed049f0494c973a304de0679139",
            "value": 41
          }
        },
        "244dcad2b16d408495293ea9f3c80372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_946e3b6011b44b7685a5e36cf77d227d",
            "placeholder": "​",
            "style": "IPY_MODEL_7826ce315fab4fb385b7dc659e4525f1",
            "value": " 41/41 [00:00&lt;00:00, 1495.39 examples/s]"
          }
        },
        "782ee92288ef44f09e7d167244102a03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97531e13c8374038bb22479aa3007557": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5b21c3fa9384ff4842be906621ea4d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "752efd7101a34aa7ba08879ce98df93a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe17ed049f0494c973a304de0679139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "946e3b6011b44b7685a5e36cf77d227d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7826ce315fab4fb385b7dc659e4525f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SETUP AND IMPORTS"
      ],
      "metadata": {
        "id": "ncvkirFZ3Qrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "import time\n",
        "import sqlite3\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from datasets import Dataset\n",
        "import sqlglot\n",
        "from sqlglot import parse_one\n",
        "\n",
        "# Check environment\n",
        "print(\"=\" * 60)\n",
        "print(\"ENVIRONMENT CHECK\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {__import__('transformers').__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "    print(\"WARNING: No GPU detected! Training will be very slow.\")\n",
        "\n",
        "print(f\"Device selected: {DEVICE}\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXNrkddY3Tiw",
        "outputId": "cb1eebf9-fb5c-4912-a0df-acb061ac883f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ENVIRONMENT CHECK\n",
            "============================================================\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Transformers version: 4.57.1\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA A100-SXM4-80GB\n",
            "GPU Memory: 79.32 GB\n",
            "Device selected: cuda\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FILE PATHS & CONFIGURATION"
      ],
      "metadata": {
        "id": "0gxoKN0m60FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "TRAIN_JSONL = Path(\"Data_for_demo.jsonl\")  # The file we created earlier\n",
        "\n",
        "# For now, we'll create a validation split from the training data\n",
        "# We don't need separate test data since this is just for demo fine-tuning\n",
        "\n",
        "# Output directories\n",
        "OUTPUT_DIR = Path(\"finetuned_demo_model\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"\\nFILE PATHS:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Training data:      {TRAIN_JSONL}\")\n",
        "print(f\"Output directory:   {OUTPUT_DIR}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Verify file exists\n",
        "print(\"\\nFILE VERIFICATION:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "if TRAIN_JSONL.exists():\n",
        "    print(f\"Training data: ✅ EXISTS\")\n",
        "\n",
        "    # Count examples\n",
        "    with open(TRAIN_JSONL, 'r') as f:\n",
        "        num_examples = sum(1 for line in f)\n",
        "    print(f\"Total examples: {num_examples}\")\n",
        "else:\n",
        "    print(f\"Training data: ❌ MISSING\")\n",
        "    print(\"\\n⚠️ WARNING: Please upload train_combined_demo.jsonl before proceeding!\")\n",
        "\n",
        "print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHZwmOGN3XKm",
        "outputId": "3319d8aa-761e-4cb2-863c-7c10fec2ca44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FILE PATHS:\n",
            "------------------------------------------------------------\n",
            "Training data:      Data_for_demo.jsonl\n",
            "Output directory:   finetuned_demo_model\n",
            "------------------------------------------------------------\n",
            "\n",
            "FILE VERIFICATION:\n",
            "------------------------------------------------------------\n",
            "Training data: ✅ EXISTS\n",
            "Total examples: 406\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL CONFIGURATION"
      ],
      "metadata": {
        "id": "ka39Arg968cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"CONFIGURATION PARAMETERS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Model Selection\n",
        "# ============================================================\n",
        "\n",
        "# Using BASE Flan-T5 for full parameter fine-tuning\n",
        "BASE_MODEL_NAME = \"juierror/flan-t5-text2sql-with-schema-v2\"  # ~250M parameters\n",
        "\n",
        "print(\"\\nModel Selection:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Base model: {BASE_MODEL_NAME}\")\n",
        "print(f\"Fine-tuning: Full parameter (all weights trainable)\")\n",
        "\n",
        "# ============================================================\n",
        "# Training Hyperparameters\n",
        "# ============================================================\n",
        "\n",
        "NUM_EPOCHS = 10  # More epochs since we have focused data\n",
        "BATCH_SIZE = 8   # Adjust based on GPU (use 4 if you get OOM)\n",
        "LEARNING_RATE = 3e-5  # Slightly higher for full fine-tuning\n",
        "WARMUP_STEPS = 100\n",
        "WEIGHT_DECAY = 0.01\n",
        "LOGGING_STEPS = 25\n",
        "SAVE_STEPS = 200\n",
        "EVAL_STEPS = 200\n",
        "\n",
        "print(\"\\nTraining Hyperparameters:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Epochs:              {NUM_EPOCHS}\")\n",
        "print(f\"Batch size:          {BATCH_SIZE}\")\n",
        "print(f\"Learning rate:       {LEARNING_RATE}\")\n",
        "print(f\"Warmup steps:        {WARMUP_STEPS}\")\n",
        "print(f\"Weight decay:        {WEIGHT_DECAY}\")\n",
        "print(f\"Logging steps:       {LOGGING_STEPS}\")\n",
        "print(f\"Save steps:          {SAVE_STEPS}\")\n",
        "print(f\"Eval steps:          {EVAL_STEPS}\")\n",
        "\n",
        "# ============================================================\n",
        "# Generation Parameters\n",
        "# ============================================================\n",
        "\n",
        "MAX_INPUT_LENGTH = 512   # Max tokens for input (question + schema)\n",
        "MAX_TARGET_LENGTH = 256  # Max tokens for output (SQL)\n",
        "\n",
        "GEN_MAX_LENGTH = 256\n",
        "GEN_NUM_BEAMS = 4\n",
        "GEN_TEMPERATURE = 0.0  # Greedy decoding\n",
        "\n",
        "print(\"\\nGeneration Parameters:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Max input length:    {MAX_INPUT_LENGTH}\")\n",
        "print(f\"Max target length:   {MAX_TARGET_LENGTH}\")\n",
        "print(f\"Generation max len:  {GEN_MAX_LENGTH}\")\n",
        "print(f\"Num beams:           {GEN_NUM_BEAMS}\")\n",
        "print(f\"Temperature:         {GEN_TEMPERATURE}\")\n",
        "\n",
        "# ============================================================\n",
        "# Prompt Template\n",
        "# ============================================================\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"Question: {question}\n",
        "\n",
        "Schema:\n",
        "{schema}\n",
        "\n",
        "SQL:\"\"\"\n",
        "\n",
        "print(\"\\nPrompt Template:\")\n",
        "print(\"-\" * 60)\n",
        "print(PROMPT_TEMPLATE.format(\n",
        "    question=\"<question here>\",\n",
        "    schema=\"<schema here>\"\n",
        "))\n",
        "\n",
        "# ============================================================\n",
        "# Other Settings\n",
        "# ============================================================\n",
        "\n",
        "SEED = 42\n",
        "FP16 = True if DEVICE == \"cuda\" else False\n",
        "\n",
        "print(\"\\nOther Settings:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Random seed:         {SEED}\")\n",
        "print(f\"FP16 (mixed prec):   {FP16}\")\n",
        "\n",
        "# Set random seed\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"\\n✅ Configuration complete!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "XgXRxUhd3XPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "125347c7-acfd-4854-9b4f-8dfd6ed22d35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CONFIGURATION PARAMETERS\n",
            "============================================================\n",
            "\n",
            "Model Selection:\n",
            "------------------------------------------------------------\n",
            "Base model: juierror/flan-t5-text2sql-with-schema-v2\n",
            "Fine-tuning: Full parameter (all weights trainable)\n",
            "\n",
            "Training Hyperparameters:\n",
            "------------------------------------------------------------\n",
            "Epochs:              10\n",
            "Batch size:          8\n",
            "Learning rate:       3e-05\n",
            "Warmup steps:        100\n",
            "Weight decay:        0.01\n",
            "Logging steps:       25\n",
            "Save steps:          200\n",
            "Eval steps:          200\n",
            "\n",
            "Generation Parameters:\n",
            "------------------------------------------------------------\n",
            "Max input length:    512\n",
            "Max target length:   256\n",
            "Generation max len:  256\n",
            "Num beams:           4\n",
            "Temperature:         0.0\n",
            "\n",
            "Prompt Template:\n",
            "------------------------------------------------------------\n",
            "Question: <question here>\n",
            "\n",
            "Schema:\n",
            "<schema here>\n",
            "\n",
            "SQL:\n",
            "\n",
            "Other Settings:\n",
            "------------------------------------------------------------\n",
            "Random seed:         42\n",
            "FP16 (mixed prec):   True\n",
            "\n",
            "✅ Configuration complete!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOADING MODEL & TOKENIZER"
      ],
      "metadata": {
        "id": "oRGqijFv8WLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 4: LOADING MODEL & TOKENIZER\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"LOADING MODEL & TOKENIZER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Load Tokenizer\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nLoading tokenizer...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
        "\n",
        "print(f\"✅ Tokenizer loaded: {BASE_MODEL_NAME}\")\n",
        "print(f\"   Vocab size: {len(tokenizer)}\")\n",
        "print(f\"   Model max length: {tokenizer.model_max_length}\")\n",
        "\n",
        "# Test tokenization\n",
        "test_text = \"SELECT * FROM Department WHERE DepartmentID = 1;\"\n",
        "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"\\nTokenization test:\")\n",
        "print(f\"   Input: {test_text}\")\n",
        "print(f\"   Token IDs shape: {test_tokens['input_ids'].shape}\")\n",
        "print(f\"   Number of tokens: {test_tokens['input_ids'].shape[1]}\")\n",
        "\n",
        "# ============================================================\n",
        "# Load Model\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Loading model (this may take a minute)...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_NAME)\n",
        "\n",
        "# Move to device\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "print(f\"✅ Model loaded: {BASE_MODEL_NAME}\")\n",
        "print(f\"   Device: {DEVICE}\")\n",
        "\n",
        "# ============================================================\n",
        "# Model Statistics\n",
        "# ============================================================\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"   Total parameters:      {total_params:,}\")\n",
        "print(f\"   Trainable parameters:  {trainable_params:,}\")\n",
        "print(f\"   Trainable %:           {100 * trainable_params / total_params:.2f}%\")\n",
        "print(f\"   Model size (approx):   {total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# ============================================================\n",
        "# Test Generation (before our fine-tuning)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Testing generation (before fine-tuning on our 3 datasets)...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "test_prompt = PROMPT_TEMPLATE.format(\n",
        "    question=\"How many employees work in each department?\",\n",
        "    schema=\"\"\"Database: hr_1\n",
        "Tables:\n",
        "- employees(employee_id*, first_name, last_name, department_id)\n",
        "- departments(department_id*, department_name)\n",
        "FK employees.department_id -> departments.department_id\"\"\"\n",
        ")\n",
        "\n",
        "print(f\"Test prompt:\\n{test_prompt}\\n\")\n",
        "\n",
        "# Tokenize and generate\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=GEN_MAX_LENGTH,\n",
        "        num_beams=GEN_NUM_BEAMS,\n",
        "        temperature=GEN_TEMPERATURE if GEN_TEMPERATURE > 0 else 1.0,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Generated SQL (baseline, before our fine-tuning):\")\n",
        "print(f\"   {generated_sql}\")\n",
        "\n",
        "print(\"\\n✅ Model is ready for fine-tuning on our 3 datasets!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "99J1euBD3XVG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb88431-0efb-4bfe-b46f-4afff55dfc36"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "LOADING MODEL & TOKENIZER\n",
            "============================================================\n",
            "\n",
            "Loading tokenizer...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tokenizer loaded: juierror/flan-t5-text2sql-with-schema-v2\n",
            "   Vocab size: 32101\n",
            "   Model max length: 512\n",
            "\n",
            "Tokenization test:\n",
            "   Input: SELECT * FROM Department WHERE DepartmentID = 1;\n",
            "   Token IDs shape: torch.Size([1, 15])\n",
            "   Number of tokens: 15\n",
            "\n",
            "------------------------------------------------------------\n",
            "Loading model (this may take a minute)...\n",
            "------------------------------------------------------------\n",
            "✅ Model loaded: juierror/flan-t5-text2sql-with-schema-v2\n",
            "   Device: cuda\n",
            "\n",
            "Model Statistics:\n",
            "------------------------------------------------------------\n",
            "   Total parameters:      247,536,384\n",
            "   Trainable parameters:  247,536,384\n",
            "   Trainable %:           100.00%\n",
            "   Model size (approx):   944.28 MB\n",
            "\n",
            "------------------------------------------------------------\n",
            "Testing generation (before fine-tuning on our 3 datasets)...\n",
            "------------------------------------------------------------\n",
            "Test prompt:\n",
            "Question: How many employees work in each department?\n",
            "\n",
            "Schema:\n",
            "Database: hr_1\n",
            "Tables:\n",
            "- employees(employee_id*, first_name, last_name, department_id)\n",
            "- departments(department_id*, department_name)\n",
            "FK employees.department_id -> departments.department_id\n",
            "\n",
            "SQL:\n",
            "\n",
            "Generated SQL (baseline, before our fine-tuning):\n",
            "   SELECT T1.department_id, count(*) FROM employees AS T1 JOIN departments AS T2 ON T1.department_id = T2.department_id GROUP BY T1.department_id\n",
            "\n",
            "✅ Model is ready for fine-tuning on our 3 datasets!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADING AND PREPROCESSING"
      ],
      "metadata": {
        "id": "XODPsa-O820I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 5: DATA LOADING AND PREPROCESSING\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA LOADING AND PREPROCESSING\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Load JSONL File\n",
        "# ============================================================\n",
        "\n",
        "def load_jsonl(path):\n",
        "    \"\"\"Load JSONL file and return list of dictionaries.\"\"\"\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "print(\"\\nLoading dataset...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Load your combined dataset\n",
        "train_data_full = load_jsonl(\"Data_for_demo.jsonl\")\n",
        "\n",
        "print(f\"Total examples loaded: {len(train_data_full)}\")\n",
        "\n",
        "# Show breakdown by database\n",
        "from collections import Counter\n",
        "db_counts = Counter([ex['dataset'] for ex in train_data_full])\n",
        "\n",
        "print(f\"\\nBreakdown by database:\")\n",
        "for db_id, count in sorted(db_counts.items()):\n",
        "    print(f\"  {db_id}: {count} examples\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nSample training example:\")\n",
        "sample = train_data_full[0]\n",
        "print(f\"  Dataset: {sample['dataset']}\")\n",
        "print(f\"  Question: {sample['question']}\")\n",
        "print(f\"  Gold SQL: {sample['gold_query'][:80]}...\")\n",
        "\n",
        "# ============================================================\n",
        "# Create Train/Validation Split\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Creating train/validation split (90/10)...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split 90% train, 10% validation\n",
        "train_data, val_data = train_test_split(\n",
        "    train_data_full,\n",
        "    test_size=0.1,\n",
        "    random_state=SEED,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Training examples:   {len(train_data)}\")\n",
        "print(f\"Validation examples: {len(val_data)}\")\n",
        "\n",
        "# ============================================================\n",
        "# Preprocessing Function\n",
        "# ============================================================\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    \"\"\"\n",
        "    Preprocess examples for fine-tuning.\n",
        "\n",
        "    Args:\n",
        "        examples: Dictionary with lists of questions, schemas, and SQL queries\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with tokenized inputs and labels\n",
        "    \"\"\"\n",
        "    # Build input prompts\n",
        "    inputs = []\n",
        "    for question, schema in zip(examples['question'], examples['schema_serialized']):\n",
        "        prompt = PROMPT_TEMPLATE.format(question=question, schema=schema)\n",
        "        inputs.append(prompt)\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=False  # We'll pad dynamically in the data collator\n",
        "    )\n",
        "\n",
        "    # Tokenize targets (SQL queries)\n",
        "    labels = tokenizer(\n",
        "        text_target=examples['gold_query'],\n",
        "        max_length=MAX_TARGET_LENGTH,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# ============================================================\n",
        "# Convert to HuggingFace Dataset Format\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Converting to HuggingFace Dataset format...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_dict({\n",
        "    'question': [ex['question'] for ex in train_data],\n",
        "    'schema_serialized': [ex['schema_serialized'] for ex in train_data],\n",
        "    'gold_query': [ex['gold_query'] for ex in train_data]\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    'question': [ex['question'] for ex in val_data],\n",
        "    'schema_serialized': [ex['schema_serialized'] for ex in val_data],\n",
        "    'gold_query': [ex['gold_query'] for ex in val_data]\n",
        "})\n",
        "\n",
        "print(f\"✅ Datasets converted\")\n",
        "print(f\"   Train dataset: {len(train_dataset)} examples\")\n",
        "print(f\"   Val dataset:   {len(val_dataset)} examples\")\n",
        "\n",
        "# ============================================================\n",
        "# Tokenize Datasets\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Tokenizing datasets (this may take 1-2 minutes)...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Tokenize with batching for speed\n",
        "tokenized_train = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Tokenizing training data\"\n",
        ")\n",
        "\n",
        "tokenized_val = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Tokenizing validation data\"\n",
        ")\n",
        "\n",
        "print(f\"✅ Tokenization complete\")\n",
        "print(f\"   Tokenized train: {len(tokenized_train)} examples\")\n",
        "print(f\"   Tokenized val:   {len(tokenized_val)} examples\")\n",
        "\n",
        "# Show tokenized example\n",
        "print(f\"\\nTokenized example (first training sample):\")\n",
        "print(f\"   Input IDs length:  {len(tokenized_train[0]['input_ids'])}\")\n",
        "print(f\"   Label IDs length:  {len(tokenized_train[0]['labels'])}\")\n",
        "\n",
        "# ============================================================\n",
        "# Data Collator\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Setting up data collator...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Data collator handles dynamic padding\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    max_length=MAX_INPUT_LENGTH\n",
        ")\n",
        "\n",
        "print(f\"✅ Data collator ready\")\n",
        "print(f\"   Will pad batches dynamically during training\")\n",
        "print(\"\\n✅ Data preprocessing complete!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "A1t_oWLL3XWx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974,
          "referenced_widgets": [
            "e099d9bc318b4c06b54c283da395272a",
            "a6e3d6eec3d24615b5bf799b6d3c8597",
            "02a322002889480abf4012ea702fd6c7",
            "98407ce1ecf94269a6b23e00e37fd213",
            "f822923ce3a149cfae3b82e24f6f2be4",
            "35461a19418d4593a12db19dd29eb3e7",
            "435bc83c076d4365ab0f9f409440bdd3",
            "cf53c32c8f1c4761ae9da8744a7925a3",
            "43a8f4befb164d9989281c0ec47900b9",
            "87cdebe119bb4ec6a606d2646f768e55",
            "613a45c9058e4559b28070cd023c92a1",
            "c0ccaf6464494f86bede8b84c514c704",
            "de41e857036d411a94428c15b44b6d6b",
            "1c2ddb7957964d1c84b91b4430efd642",
            "244dcad2b16d408495293ea9f3c80372",
            "782ee92288ef44f09e7d167244102a03",
            "97531e13c8374038bb22479aa3007557",
            "d5b21c3fa9384ff4842be906621ea4d5",
            "752efd7101a34aa7ba08879ce98df93a",
            "dbe17ed049f0494c973a304de0679139",
            "946e3b6011b44b7685a5e36cf77d227d",
            "7826ce315fab4fb385b7dc659e4525f1"
          ]
        },
        "outputId": "9d37a38f-b00f-4b9a-9d37-a2cc02e3237a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "DATA LOADING AND PREPROCESSING\n",
            "============================================================\n",
            "\n",
            "Loading dataset...\n",
            "------------------------------------------------------------\n",
            "Total examples loaded: 406\n",
            "\n",
            "Breakdown by database:\n",
            "  college_2: 170 examples\n",
            "  hr_1: 124 examples\n",
            "  store_1: 112 examples\n",
            "\n",
            "Sample training example:\n",
            "  Dataset: store_1\n",
            "  Question: A list of the top 5 countries by number of invoices. List country name and number of invoices.\n",
            "  Gold SQL: SELECT billing_country ,  COUNT(*) FROM invoices GROUP BY billing_country ORDER ...\n",
            "\n",
            "------------------------------------------------------------\n",
            "Creating train/validation split (90/10)...\n",
            "------------------------------------------------------------\n",
            "Training examples:   365\n",
            "Validation examples: 41\n",
            "\n",
            "------------------------------------------------------------\n",
            "Converting to HuggingFace Dataset format...\n",
            "------------------------------------------------------------\n",
            "✅ Datasets converted\n",
            "   Train dataset: 365 examples\n",
            "   Val dataset:   41 examples\n",
            "\n",
            "------------------------------------------------------------\n",
            "Tokenizing datasets (this may take 1-2 minutes)...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing training data:   0%|          | 0/365 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e099d9bc318b4c06b54c283da395272a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing validation data:   0%|          | 0/41 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0ccaf6464494f86bede8b84c514c704"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tokenization complete\n",
            "   Tokenized train: 365 examples\n",
            "   Tokenized val:   41 examples\n",
            "\n",
            "Tokenized example (first training sample):\n",
            "   Input IDs length:  426\n",
            "   Label IDs length:  20\n",
            "\n",
            "------------------------------------------------------------\n",
            "Setting up data collator...\n",
            "------------------------------------------------------------\n",
            "✅ Data collator ready\n",
            "   Will pad batches dynamically during training\n",
            "\n",
            "✅ Data preprocessing complete!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING SETUP & FINE-TUNING"
      ],
      "metadata": {
        "id": "w7P1L5fw90OE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"FINE-TUNING SETUP\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Training Arguments\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nSetting up training arguments...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "\n",
        "    # Training hyperparameters\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "\n",
        "    # Evaluation and logging\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "\n",
        "    # Generation settings for evaluation\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=GEN_MAX_LENGTH,\n",
        "    generation_num_beams=GEN_NUM_BEAMS,\n",
        "\n",
        "    # Performance optimizations for Colab GPU\n",
        "    fp16=FP16,\n",
        "    dataloader_num_workers=2,\n",
        "\n",
        "    # Model saving\n",
        "    save_total_limit=3,  # Keep only 3 best checkpoints\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "\n",
        "    # Other settings\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard\n",
        "    seed=SEED,\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "print(f\"✅ Training arguments configured\")\n",
        "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"   Total epochs: {NUM_EPOCHS}\")\n",
        "print(f\"   Steps per epoch: ~{len(tokenized_train) // BATCH_SIZE}\")\n",
        "print(f\"   Total training steps: ~{(len(tokenized_train) // BATCH_SIZE) * NUM_EPOCHS}\")\n",
        "\n",
        "# ============================================================\n",
        "# Initialize Trainer\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Initializing Seq2SeqTrainer...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(f\"✅ Trainer initialized\")\n",
        "print(f\"   Training samples: {len(tokenized_train)}\")\n",
        "print(f\"   Validation samples: {len(tokenized_val)}\")\n",
        "\n",
        "# ============================================================\n",
        "# Start Fine-Tuning\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING FINE-TUNING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nEstimated time:\")\n",
        "print(\"   - On T4 GPU (Colab Free): ~25-35 minutes\")\n",
        "print(\"   - On A100 GPU (Colab Pro): ~8-12 minutes\")\n",
        "print(\"\\nTraining progress will be displayed below...\")\n",
        "print(\"You'll see:\")\n",
        "print(\"   - Loss decreasing over time\")\n",
        "print(\"   - Evaluation metrics every 200 steps\")\n",
        "print(\"   - Checkpoints saved every 200 steps\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# Start training!\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"🎉 TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Print training metrics\n",
        "print(f\"\\nTraining Metrics:\")\n",
        "print(f\"   Total runtime: {train_result.metrics['train_runtime']:.2f} seconds ({train_result.metrics['train_runtime']/60:.1f} minutes)\")\n",
        "print(f\"   Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
        "print(f\"   Steps per second: {train_result.metrics['train_steps_per_second']:.3f}\")\n",
        "print(f\"   Final train loss: {train_result.metrics['train_loss']:.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# Evaluate on Validation Set\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Evaluating on validation set...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "eval_result = trainer.evaluate()\n",
        "\n",
        "print(f\"✅ Evaluation complete\")\n",
        "print(f\"\\nValidation Metrics:\")\n",
        "print(f\"   Eval loss: {eval_result['eval_loss']:.4f}\")\n",
        "print(f\"   Eval runtime: {eval_result['eval_runtime']:.2f} seconds\")\n",
        "print(f\"   Samples per second: {eval_result['eval_samples_per_second']:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✅ Fine-tuning complete! Model is ready for demo.\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "ONgAt5rO3Xaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "873d28c6-09ff-4b52-dc8b-875bb2d6b74c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "FINE-TUNING SETUP\n",
            "============================================================\n",
            "\n",
            "Setting up training arguments...\n",
            "------------------------------------------------------------\n",
            "✅ Training arguments configured\n",
            "   Output directory: finetuned_demo_model\n",
            "   Total epochs: 10\n",
            "   Steps per epoch: ~45\n",
            "   Total training steps: ~450\n",
            "\n",
            "------------------------------------------------------------\n",
            "Initializing Seq2SeqTrainer...\n",
            "------------------------------------------------------------\n",
            "✅ Trainer initialized\n",
            "   Training samples: 365\n",
            "   Validation samples: 41\n",
            "\n",
            "============================================================\n",
            "STARTING FINE-TUNING\n",
            "============================================================\n",
            "\n",
            "Estimated time:\n",
            "   - On T4 GPU (Colab Free): ~25-35 minutes\n",
            "   - On A100 GPU (Colab Pro): ~8-12 minutes\n",
            "\n",
            "Training progress will be displayed below...\n",
            "You'll see:\n",
            "   - Loss decreasing over time\n",
            "   - Evaluation metrics every 200 steps\n",
            "   - Checkpoints saved every 200 steps\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1086830809.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='460' max='460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [460/460 01:43, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🎉 TRAINING COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Training Metrics:\n",
            "   Total runtime: 104.40 seconds (1.7 minutes)\n",
            "   Samples per second: 34.96\n",
            "   Steps per second: 4.406\n",
            "   Final train loss: 0.0000\n",
            "\n",
            "------------------------------------------------------------\n",
            "Evaluating on validation set...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Evaluation complete\n",
            "\n",
            "Validation Metrics:\n",
            "   Eval loss: nan\n",
            "   Eval runtime: 0.49 seconds\n",
            "   Samples per second: 82.93\n",
            "\n",
            "============================================================\n",
            "✅ Fine-tuning complete! Model is ready for demo.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 7: SAVE FINE-TUNED MODEL\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SAVING FINE-TUNED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# Save Model and Tokenizer\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nSaving model and tokenizer...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "FINAL_MODEL_DIR = OUTPUT_DIR / \"final_model\"\n",
        "FINAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "trainer.save_model(str(FINAL_MODEL_DIR))\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(str(FINAL_MODEL_DIR))\n",
        "\n",
        "print(f\"✅ Model saved to: {FINAL_MODEL_DIR}\")\n",
        "print(f\"✅ Tokenizer saved to: {FINAL_MODEL_DIR}\")\n",
        "\n",
        "# ============================================================\n",
        "# Save Training Configuration\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Saving training configuration...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "training_config = {\n",
        "    \"base_model\": BASE_MODEL_NAME,\n",
        "    \"datasets\": [\"college_2\", \"hr_1\", \"store_1\"],\n",
        "    \"num_examples\": len(train_data_full),\n",
        "    \"train_examples\": len(train_data),\n",
        "    \"val_examples\": len(val_data),\n",
        "    \"num_epochs\": NUM_EPOCHS,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"warmup_steps\": WARMUP_STEPS,\n",
        "    \"weight_decay\": WEIGHT_DECAY,\n",
        "    \"max_input_length\": MAX_INPUT_LENGTH,\n",
        "    \"max_target_length\": MAX_TARGET_LENGTH,\n",
        "    \"final_train_loss\": train_result.metrics.get('train_loss', 'N/A'),\n",
        "    \"final_eval_loss\": eval_result.get('eval_loss', 'N/A'),\n",
        "    \"training_runtime_seconds\": train_result.metrics.get('train_runtime', 'N/A'),\n",
        "}\n",
        "\n",
        "config_path = OUTPUT_DIR / \"training_config.json\"\n",
        "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(training_config, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"✅ Training config saved to: {config_path}\")\n",
        "print(\"\\n✅ All files saved successfully!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "xKohlmS13Xca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "074f402f-e65a-4184-ebb8-36d452fdd60d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SAVING FINE-TUNED MODEL\n",
            "============================================================\n",
            "\n",
            "Saving model and tokenizer...\n",
            "------------------------------------------------------------\n",
            "✅ Model saved to: finetuned_demo_model/final_model\n",
            "✅ Tokenizer saved to: finetuned_demo_model/final_model\n",
            "\n",
            "------------------------------------------------------------\n",
            "Saving training configuration...\n",
            "------------------------------------------------------------\n",
            "✅ Training config saved to: finetuned_demo_model/training_config.json\n",
            "\n",
            "✅ All files saved successfully!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING MODEL PERFORMANCE"
      ],
      "metadata": {
        "id": "AcVT9pHR_ZBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 8: TESTING ON DEMO DATASETS (Modified for direct upload)\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TESTING FINE-TUNED MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# SQL Utilities\n",
        "# ============================================================\n",
        "\n",
        "def canonical_sql(sql_text):\n",
        "    \"\"\"Normalize SQL to canonical form using sqlglot.\"\"\"\n",
        "    if not sql_text:\n",
        "        return None\n",
        "    try:\n",
        "        ast = parse_one(sql_text, read=\"sqlite\")\n",
        "        return ast.sql(dialect=\"sqlite\", pretty=False)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def try_execute(conn, sql_text):\n",
        "    \"\"\"Execute SQL query and return result set.\"\"\"\n",
        "    try:\n",
        "        cur = conn.execute(sql_text)\n",
        "        rows = cur.fetchall()\n",
        "\n",
        "        # Normalize floats\n",
        "        normalized = []\n",
        "        for row in rows:\n",
        "            norm_row = []\n",
        "            for val in row:\n",
        "                if isinstance(val, float):\n",
        "                    norm_row.append(round(val, 6))\n",
        "                else:\n",
        "                    norm_row.append(val)\n",
        "            normalized.append(tuple(norm_row))\n",
        "\n",
        "        return set(normalized), None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, str(e)\n",
        "\n",
        "\n",
        "def extract_sql(text):\n",
        "    \"\"\"Extract SQL from model output.\"\"\"\n",
        "    text = text.strip()\n",
        "\n",
        "    # Remove markdown code blocks if present\n",
        "    if \"```\" in text:\n",
        "        parts = text.split(\"```\")\n",
        "        for part in parts:\n",
        "            if \"select\" in part.lower() or \"SELECT\" in part:\n",
        "                text = part.strip()\n",
        "                if text.lower().startswith(\"sql\"):\n",
        "                    text = text[3:].strip()\n",
        "                break\n",
        "\n",
        "    # Remove common prefixes\n",
        "    for prefix in [\"sql:\", \"answer:\", \"query:\"]:\n",
        "        if text.lower().startswith(prefix):\n",
        "            text = text[len(prefix):].strip()\n",
        "\n",
        "    # Ensure semicolon\n",
        "    if \";\" in text:\n",
        "        text = text.split(\";\", 1)[0] + \";\"\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "print(\"\\n✅ SQL utilities defined\")\n",
        "\n",
        "# ============================================================\n",
        "# Load All Test Data (Full Dataset)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Loading test data...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Use the full dataset for testing\n",
        "test_data = train_data_full  # All 406 examples\n",
        "\n",
        "print(f\"Total test examples: {len(test_data)}\")\n",
        "\n",
        "# Breakdown by database\n",
        "db_counts = Counter([ex['dataset'] for ex in test_data])\n",
        "print(f\"\\nTest examples by database:\")\n",
        "for db_id, count in sorted(db_counts.items()):\n",
        "    print(f\"  {db_id}: {count} examples\")\n",
        "\n",
        "# ============================================================\n",
        "# Prepare Database Connections (Modified for direct upload)\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Setting up database connections...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Dictionary to store database connections\n",
        "db_connections = {}\n",
        "\n",
        "# Check for directly uploaded database files (no folder structure)\n",
        "db_files = {\n",
        "    'college_2': 'college_2.sqlite',\n",
        "    'hr_1': 'hr_1.sqlite',\n",
        "    'store_1': 'store_1.sqlite'\n",
        "}\n",
        "\n",
        "# Try to connect to each database\n",
        "for db_id, db_filename in db_files.items():\n",
        "    # Check if file exists in current directory\n",
        "    if os.path.exists(db_filename):\n",
        "        conn = sqlite3.connect(db_filename)\n",
        "        conn.execute(\"PRAGMA foreign_keys=ON\")\n",
        "        db_connections[db_id] = conn\n",
        "        print(f\"✅ Connected to: {db_id} ({db_filename})\")\n",
        "    else:\n",
        "        print(f\"⚠️  Database not found: {db_filename}\")\n",
        "\n",
        "if not db_connections:\n",
        "    print(\"\\n⚠️  WARNING: No databases found!\")\n",
        "    print(\"   The evaluation will only calculate Exact Match (EM) metrics.\")\n",
        "    print(\"   Upload database files to calculate EX and Valid SQL metrics.\")\n",
        "else:\n",
        "    print(f\"\\n✅ Connected to {len(db_connections)}/3 databases\")\n",
        "\n",
        "# ============================================================\n",
        "# Evaluation Loop\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RUNNING EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = []\n",
        "n_examples = len(test_data)\n",
        "\n",
        "# Overall metrics\n",
        "em_count = 0\n",
        "ex_count = 0\n",
        "valid_count = 0\n",
        "latencies = []\n",
        "\n",
        "# Per-database metrics\n",
        "db_metrics = {db: {'em': 0, 'ex': 0, 'valid': 0, 'total': 0} for db in ['college_2', 'hr_1', 'store_1']}\n",
        "\n",
        "print(f\"\\nEvaluating on {n_examples} examples...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "eval_model = trainer.model\n",
        "eval_model.eval()\n",
        "\n",
        "for i, example in enumerate(test_data, 1):\n",
        "    question = example['question']\n",
        "    gold_sql = example['gold_query']\n",
        "    schema = example['schema_serialized']\n",
        "    db_id = example['dataset']\n",
        "\n",
        "    # Build prompt\n",
        "    prompt = PROMPT_TEMPLATE.format(question=question, schema=schema)\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=MAX_INPUT_LENGTH,\n",
        "        truncation=True\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # Generate SQL\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = eval_model.generate(\n",
        "            **inputs,\n",
        "            max_length=GEN_MAX_LENGTH,\n",
        "            num_beams=GEN_NUM_BEAMS,\n",
        "            temperature=GEN_TEMPERATURE if GEN_TEMPERATURE > 0 else 1.0,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    gen_time_ms = (time.time() - start_time) * 1000.0\n",
        "    latencies.append(gen_time_ms)\n",
        "\n",
        "    # Decode\n",
        "    pred_sql_raw = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    pred_sql_raw = extract_sql(pred_sql_raw)\n",
        "\n",
        "    # Normalize\n",
        "    pred_sql_norm = canonical_sql(pred_sql_raw)\n",
        "    gold_sql_norm = canonical_sql(gold_sql)\n",
        "\n",
        "    # ============================================================\n",
        "    # Compute Metrics\n",
        "    # ============================================================\n",
        "\n",
        "    # Exact Match (EM)\n",
        "    em = int(\n",
        "        pred_sql_norm is not None and\n",
        "        gold_sql_norm is not None and\n",
        "        pred_sql_norm == gold_sql_norm\n",
        "    )\n",
        "\n",
        "    # Execution Accuracy (EX) and Valid SQL\n",
        "    valid = 0\n",
        "    ex_ok = 0\n",
        "    error = None\n",
        "\n",
        "    # Get database connection\n",
        "    conn = db_connections.get(db_id)\n",
        "\n",
        "    if conn and pred_sql_norm is not None:\n",
        "        # Try to execute predicted SQL\n",
        "        pred_rows, error = try_execute(conn, pred_sql_norm)\n",
        "\n",
        "        if pred_rows is not None:\n",
        "            valid = 1  # SQL is valid\n",
        "\n",
        "            # Execute gold SQL\n",
        "            gold_rows, gold_error = try_execute(conn, gold_sql_norm or gold_sql)\n",
        "\n",
        "            if gold_rows is not None:\n",
        "                # Compare result sets\n",
        "                ex_ok = int(pred_rows == gold_rows)\n",
        "            else:\n",
        "                error = f\"Gold SQL failed: {gold_error}\"\n",
        "    elif pred_sql_norm is None:\n",
        "        error = \"ParseError: Could not parse predicted SQL\"\n",
        "    else:\n",
        "        error = f\"Database connection not available for {db_id}\"\n",
        "\n",
        "    # Update counters\n",
        "    em_count += em\n",
        "    ex_count += ex_ok\n",
        "    valid_count += valid\n",
        "\n",
        "    # Update per-database metrics\n",
        "    db_metrics[db_id]['total'] += 1\n",
        "    db_metrics[db_id]['em'] += em\n",
        "    db_metrics[db_id]['ex'] += ex_ok\n",
        "    db_metrics[db_id]['valid'] += valid\n",
        "\n",
        "    # Store result\n",
        "    results.append({\n",
        "        \"id\": example.get(\"id\", f\"{db_id}_{i}\"),\n",
        "        \"dataset\": db_id,\n",
        "        \"question\": question,\n",
        "        \"gold_sql\": gold_sql,\n",
        "        \"pred_sql_raw\": pred_sql_raw,\n",
        "        \"pred_sql_norm\": pred_sql_norm or \"\",\n",
        "        \"em\": em,\n",
        "        \"ex\": ex_ok,\n",
        "        \"valid_sql\": valid,\n",
        "        \"latency_ms\": round(gen_time_ms, 2),\n",
        "        \"error\": error or \"\"\n",
        "    })\n",
        "\n",
        "    # Progress update\n",
        "    if i % 50 == 0 or i == n_examples:\n",
        "        print(f\"[{i}/{n_examples}] Overall: EM={em_count/i:.3f} EX={ex_count/i:.3f} Valid={valid_count/i:.3f}\")\n",
        "\n",
        "# ============================================================\n",
        "# Save Results\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"Saving results...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "RESULTS_CSV = OUTPUT_DIR / \"test_results_demo_datasets.csv\"\n",
        "\n",
        "with open(RESULTS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    if results:\n",
        "        writer = csv.DictWriter(f, fieldnames=list(results[0].keys()))\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "print(f\"✅ Results saved to: {RESULTS_CSV}\")\n",
        "\n",
        "# ============================================================\n",
        "# Summary Statistics\n",
        "# ============================================================\n",
        "\n",
        "em_rate = em_count / n_examples\n",
        "ex_rate = ex_count / n_examples\n",
        "valid_rate = valid_count / n_examples\n",
        "median_latency = sorted(latencies)[len(latencies) // 2] if latencies else 0\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nModel: Fine-tuned on 3 Demo Datasets\")\n",
        "print(f\"Base Model: {BASE_MODEL_NAME}\")\n",
        "print(f\"Datasets: college_2, hr_1, store_1\")\n",
        "print(f\"Total Examples: {n_examples}\")\n",
        "\n",
        "print(f\"\\n📊 OVERALL METRICS:\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"  Exact Match (EM):        {em_rate:.1%} ({em_count}/{n_examples})\")\n",
        "print(f\"  Execution Accuracy (EX): {ex_rate:.1%} ({ex_count}/{n_examples})\")\n",
        "print(f\"  Valid-SQL rate:          {valid_rate:.1%} ({valid_count}/{n_examples})\")\n",
        "print(f\"  Median generation time:  {median_latency:.1f} ms\")\n",
        "\n",
        "print(f\"\\n📊 PER-DATABASE BREAKDOWN:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for db_id in sorted(db_metrics.keys()):\n",
        "    metrics = db_metrics[db_id]\n",
        "    total = metrics['total']\n",
        "    if total > 0:\n",
        "        print(f\"\\n{db_id.upper()} ({total} examples):\")\n",
        "        print(f\"  EM:    {metrics['em']/total:.1%} ({metrics['em']}/{total})\")\n",
        "        print(f\"  EX:    {metrics['ex']/total:.1%} ({metrics['ex']}/{total})\")\n",
        "        print(f\"  Valid: {metrics['valid']/total:.1%} ({metrics['valid']}/{total})\")\n",
        "\n",
        "print(f\"\\n💾 Results saved to: {RESULTS_CSV}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Close database connections\n",
        "for conn in db_connections.values():\n",
        "    conn.close()\n",
        "\n",
        "print(\"\\n✅ Testing complete!\")"
      ],
      "metadata": {
        "id": "i9UnOU0S3XeM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "a7d3c59e-f787-46d2-9a8b-d6dcd8311ce7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TESTING FINE-TUNED MODEL\n",
            "============================================================\n",
            "\n",
            "✅ SQL utilities defined\n",
            "\n",
            "------------------------------------------------------------\n",
            "Loading test data...\n",
            "------------------------------------------------------------\n",
            "Total test examples: 406\n",
            "\n",
            "Test examples by database:\n",
            "  college_2: 170 examples\n",
            "  hr_1: 124 examples\n",
            "  store_1: 112 examples\n",
            "\n",
            "------------------------------------------------------------\n",
            "Setting up database connections...\n",
            "------------------------------------------------------------\n",
            "✅ Connected to: college_2 (college_2.sqlite)\n",
            "✅ Connected to: hr_1 (hr_1.sqlite)\n",
            "✅ Connected to: store_1 (store_1.sqlite)\n",
            "\n",
            "✅ Connected to 3/3 databases\n",
            "\n",
            "============================================================\n",
            "RUNNING EVALUATION\n",
            "============================================================\n",
            "\n",
            "Evaluating on 406 examples...\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2111861130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         outputs = eval_model.generate(\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGEN_MAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3263\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_running_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3265\u001b[0;31m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3267\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1764\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1765\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1101\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0;31m# clamp inf values to enable fp16 training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mforwarded_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforwarded_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mhidden_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwi_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_gelu\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QDlTI5Yx3XgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUhHj7S53XkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldXw359L3XmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PRLROrK3Xn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-t8IokFN3Xpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7KNgnC1m3Xrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dHw71YWc3X07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_FvsysD03X3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "woXGtcRB3X5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNAQx_m83X8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OKmQRRQW3X-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w9ZYlZpS3YBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRdSukPh3YEA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}