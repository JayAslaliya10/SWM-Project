{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebfa5c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING TEST DATASETS FOR 11 NEW DATABASES\n",
      "============================================================\n",
      "\n",
      "‚ÑπÔ∏è  Using databases NOT seen during training for fair evaluation\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÇ Loading from: spider_data/train_spider.json\n",
      "‚úÖ Loaded 7000 examples\n",
      "\n",
      "üîç Checking available databases in the data...\n",
      "\n",
      "Databases found in training data:\n",
      "------------------------------------------------------------\n",
      "‚úÖ dorm_1                               100 examples\n",
      "‚úÖ allergy_1                             98 examples\n",
      "‚úÖ movie_1                               98 examples\n",
      "‚úÖ flight_1                              96 examples\n",
      "‚úÖ driving_school                        93 examples\n",
      "‚úÖ cre_Doc_Tracking_DB                   90 examples\n",
      "‚úÖ department_store                      88 examples\n",
      "‚úÖ customers_and_addresses               88 examples\n",
      "‚úÖ activity_1                            88 examples\n",
      "‚úÖ network_2                             86 examples\n",
      "‚úÖ products_gen_characteristics          86 examples\n",
      "\n",
      "============================================================\n",
      "Creating test files...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìÅ Processing dorm_1...\n",
      "   ‚úÖ Created test_dorm_1.jsonl with 50 examples\n",
      "\n",
      "üìÅ Processing allergy_1...\n",
      "   ‚úÖ Created test_allergy_1.jsonl with 50 examples\n",
      "\n",
      "üìÅ Processing movie_1...\n",
      "   ‚úÖ Created test_movie_1.jsonl with 50 examples\n",
      "\n",
      "üìÅ Processing flight_1...\n",
      "   ‚úÖ Created test_flight_1.jsonl with 50 examples\n",
      "\n",
      "üìÅ Processing driving_school...\n",
      "   ‚úÖ Created test_driving_school.jsonl with 50 examples\n",
      "\n",
      "üìÅ Processing cre_Doc_Tracking_DB...\n",
      "   ‚úÖ Created test_cre_Doc_Tracking_DB.jsonl with 50 examples\n",
      "\n",
      "üìÅ Processing department_store...\n",
      "   ‚úÖ Created test_department_store.jsonl with 50 examples\n",
      "\n",
      "üìÅ Processing customers_and_addresses...\n",
      "   ‚úÖ Created test_customers_and_addresses.jsonl with 50 examples\n",
      "\n",
      "üìÅ Processing activity_1...\n",
      "   ‚úÖ Created test_activity_1.jsonl with 50 examples\n",
      "\n",
      "üìÅ Processing network_2...\n",
      "   ‚úÖ Created test_network_2.jsonl with 50 examples\n",
      "\n",
      "üìÅ Processing products_gen_characteristics...\n",
      "   ‚úÖ Created test_products_gen_characteristics.jsonl with 50 examples\n",
      "\n",
      "============================================================\n",
      "‚úÖ TEST DATASETS CREATED\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   Databases processed: 11\n",
      "   Total test examples: 550\n",
      "\n",
      "üìã Dataset Details:\n",
      "--------------------------------------------------------------------------------\n",
      "Database                       Available    Used         Test File                     \n",
      "--------------------------------------------------------------------------------\n",
      "dorm_1                         100          50           test_dorm_1.jsonl             \n",
      "allergy_1                      98           50           test_allergy_1.jsonl          \n",
      "movie_1                        98           50           test_movie_1.jsonl            \n",
      "flight_1                       96           50           test_flight_1.jsonl           \n",
      "driving_school                 93           50           test_driving_school.jsonl     \n",
      "cre_Doc_Tracking_DB            90           50           test_cre_Doc_Tracking_DB.jsonl\n",
      "department_store               88           50           test_department_store.jsonl   \n",
      "customers_and_addresses        88           50           test_customers_and_addresses.jsonl\n",
      "activity_1                     88           50           test_activity_1.jsonl         \n",
      "network_2                      86           50           test_network_2.jsonl          \n",
      "products_gen_characteristics   86           50           test_products_gen_characteristics.jsonl\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üíæ Files saved:\n",
      "   Test files: test_visualizations/\n",
      "   Summary: test_visualizations/evaluation_summary.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Spider data path\n",
    "SPIDER_PATH = \"spider_data\"\n",
    "\n",
    "# 11 NEW databases for evaluation (not used in your fine-tuning)\n",
    "EVALUATION_DATABASES = [\n",
    "    \"dorm_1\",\n",
    "    \"allergy_1\",\n",
    "    \"movie_1\",\n",
    "    \"flight_1\",\n",
    "    \"driving_school\",\n",
    "    \"cre_Doc_Tracking_DB\",\n",
    "    \"department_store\",\n",
    "    \"customers_and_addresses\",\n",
    "    \"activity_1\",\n",
    "    \"network_2\",\n",
    "    \"products_gen_characteristics\"\n",
    "]\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"test_visualizations\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# SCHEMA EXTRACTION\n",
    "# ============================================================\n",
    "\n",
    "def extract_schema_from_db(db_path, db_id):\n",
    "    \"\"\"Extract schema from SQLite database\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get all tables\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "        \n",
    "        if not tables:\n",
    "            return None\n",
    "        \n",
    "        schema_parts = [f\"Database: {db_id}\", \"Tables:\"]\n",
    "        \n",
    "        # Get columns for each table\n",
    "        for table in tables:\n",
    "            cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "            columns = cursor.fetchall()\n",
    "            \n",
    "            col_names = []\n",
    "            for col in columns:\n",
    "                col_name = col[1]\n",
    "                is_pk = col[5] == 1\n",
    "                if is_pk:\n",
    "                    col_names.append(f\"{col_name}*\")\n",
    "                else:\n",
    "                    col_names.append(col_name)\n",
    "            \n",
    "            table_schema = f\"- {table}({', '.join(col_names)})\"\n",
    "            schema_parts.append(table_schema)\n",
    "        \n",
    "        # Get foreign keys\n",
    "        fk_lines = []\n",
    "        for table in tables:\n",
    "            cursor.execute(f\"PRAGMA foreign_key_list({table})\")\n",
    "            fks = cursor.fetchall()\n",
    "            \n",
    "            for fk in fks:\n",
    "                from_col = fk[3]\n",
    "                to_table = fk[2]\n",
    "                to_col = fk[4]\n",
    "                fk_line = f\"FK {table}.{from_col} -> {to_table}.{to_col}\"\n",
    "                fk_lines.append(fk_line)\n",
    "        \n",
    "        if fk_lines:\n",
    "            schema_parts.extend(fk_lines)\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        return \"\\n\".join(schema_parts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting schema for {db_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================\n",
    "# LOAD SPIDER DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CREATING TEST DATASETS FOR 11 NEW DATABASES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚ÑπÔ∏è  Using databases NOT seen during training for fair evaluation\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Try both train_spider.json and train.json\n",
    "train_files = [\n",
    "    Path(SPIDER_PATH) / \"train_spider.json\",\n",
    "    Path(SPIDER_PATH) / \"train.json\"\n",
    "]\n",
    "\n",
    "spider_data = None\n",
    "for train_file in train_files:\n",
    "    if train_file.exists():\n",
    "        print(f\"\\nüìÇ Loading from: {train_file}\")\n",
    "        with open(train_file, 'r') as f:\n",
    "            spider_data = json.load(f)\n",
    "        print(f\"‚úÖ Loaded {len(spider_data)} examples\")\n",
    "        break\n",
    "\n",
    "if spider_data is None:\n",
    "    print(\"‚ùå Error: Could not find train_spider.json or train.json\")\n",
    "    print(\"Please check your spider_data folder\")\n",
    "    exit(1)\n",
    "\n",
    "# Let's first check what databases are available\n",
    "print(\"\\nüîç Checking available databases in the data...\")\n",
    "db_counts = Counter([ex['db_id'] for ex in spider_data])\n",
    "\n",
    "print(\"\\nDatabases found in training data:\")\n",
    "print(\"-\" * 60)\n",
    "for db_id in EVALUATION_DATABASES:\n",
    "    count = db_counts.get(db_id, 0)\n",
    "    status = \"‚úÖ\" if count > 0 else \"‚ùå\"\n",
    "    print(f\"{status} {db_id:<35} {count:>4} examples\")\n",
    "\n",
    "# ============================================================\n",
    "# CREATE TEST FILES FOR EACH DATABASE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Creating test files...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "summary = []\n",
    "\n",
    "for db_id in EVALUATION_DATABASES:\n",
    "    print(f\"\\nüìÅ Processing {db_id}...\")\n",
    "    \n",
    "    # Filter examples for this database\n",
    "    db_examples = [ex for ex in spider_data if ex['db_id'] == db_id]\n",
    "    \n",
    "    if not db_examples:\n",
    "        print(f\"   ‚ö†Ô∏è  No examples found for {db_id} - skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Get database path\n",
    "    db_path = Path(SPIDER_PATH) / \"database\" / db_id / f\"{db_id}.sqlite\"\n",
    "    \n",
    "    if not db_path.exists():\n",
    "        print(f\"   ‚ö†Ô∏è  Database not found: {db_path} - skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Extract schema\n",
    "    schema = extract_schema_from_db(str(db_path), db_id)\n",
    "    \n",
    "    if not schema:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not extract schema - skipping\")\n",
    "        continue\n",
    "    \n",
    "    # Take first 50 examples for testing (or all if less than 50)\n",
    "    test_examples = db_examples[:50]\n",
    "    \n",
    "    # Create test JSONL\n",
    "    test_data = []\n",
    "    for i, example in enumerate(test_examples, 1):\n",
    "        entry = {\n",
    "            \"id\": f\"{db_id}_{i}\",\n",
    "            \"dataset\": db_id,\n",
    "            \"db_id\": db_id,\n",
    "            \"sqlite_path\": f\"spider_data/database/{db_id}/{db_id}.sqlite\",\n",
    "            \"schema_serialized\": schema,\n",
    "            \"question\": example['question'],\n",
    "            \"gold_query\": example['query']\n",
    "        }\n",
    "        test_data.append(entry)\n",
    "    \n",
    "    # Save as JSONL\n",
    "    output_file = OUTPUT_DIR / f\"test_{db_id}.jsonl\"\n",
    "    with open(output_file, 'w') as f:\n",
    "        for entry in test_data:\n",
    "            f.write(json.dumps(entry) + '\\n')\n",
    "    \n",
    "    print(f\"   ‚úÖ Created {output_file.name} with {len(test_data)} examples\")\n",
    "    \n",
    "    summary.append({\n",
    "        \"database\": db_id,\n",
    "        \"total_available\": len(db_examples),\n",
    "        \"examples_used\": len(test_data),\n",
    "        \"test_file\": str(output_file),\n",
    "        \"db_file\": str(db_path)\n",
    "    })\n",
    "\n",
    "# ============================================================\n",
    "# SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ TEST DATASETS CREATED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not summary:\n",
    "    print(\"\\n‚ùå ERROR: No test datasets were created!\")\n",
    "    print(\"\\nPossible issues:\")\n",
    "    print(\"1. Database names might be different in Spider dataset\")\n",
    "    print(\"2. SQLite files might not exist\")\n",
    "    print(\"3. Wrong file path to Spider data\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Databases processed: {len(summary)}\")\n",
    "print(f\"   Total test examples: {sum(s['examples_used'] for s in summary)}\")\n",
    "\n",
    "print(\"\\nüìã Dataset Details:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Database':<30} {'Available':<12} {'Used':<12} {'Test File':<30}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for s in summary:\n",
    "    print(f\"{s['database']:<30} {s['total_available']:<12} {s['examples_used']:<12} {Path(s['test_file']).name:<30}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Save summary as JSON\n",
    "summary_file = OUTPUT_DIR / \"evaluation_summary.json\"\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Files saved:\")\n",
    "print(f\"   Test files: {OUTPUT_DIR}/\")\n",
    "print(f\"   Summary: {summary_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196d730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
